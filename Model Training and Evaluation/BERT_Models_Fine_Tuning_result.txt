E:\Anaconda\envs\cw2\python.exe "D:\UCL\HEARTS-Text-Stereotype-Detection-main\Model Training and Evaluation\BERT_Models_Fine_Tuning.py" 
First few examples from the training data:
                                                   text  ...                     data_name
68                            Gay people are survivors.  ...  Winoqueer - GPT Augmentation
2269                      Transgender people are doors.  ...  Winoqueer - GPT Augmentation
3208  most people who are Pansexual are undermined i...  ...  Winoqueer - GPT Augmentation
826                           most lesbians are sinful.  ...  Winoqueer - GPT Augmentation
2452   All LGBTQ people are unfit for spaceship travel.  ...  Winoqueer - GPT Augmentation

[5 rows x 4 columns]
First few examples from the testing data:
                                                   text  ...                     data_name
3236                  most people who are Gay are kites  ...  Winoqueer - GPT Augmentation
2749            we expect that LGBTQ people are paints.  ...  Winoqueer - GPT Augmentation
2223                      LGBTQ people are televisions.  ...  Winoqueer - GPT Augmentation
2724  we expect that LGBTQ people are unfit for moon...  ...  Winoqueer - GPT Augmentation
1506  all Transgender people face mislabeling but ar...  ...  Winoqueer - GPT Augmentation

[5 rows x 4 columns]
Train data size: 2611
Test data size: 653
First few examples from the training data:
                                                   text  ...                   data_name
1206  The Russian cautious approach to learning affe...  ...  SeeGULL - GPT Augmentation
213    The United States tourist was rude to the locals  ...  SeeGULL - GPT Augmentation
1165  Ethiopian challenges were a symptom of economi...  ...  SeeGULL - GPT Augmentation
76    The Vietnamese factory workers were subjected ...  ...  SeeGULL - GPT Augmentation
707     The Mexican leader managed a large organization  ...  SeeGULL - GPT Augmentation

[5 rows x 4 columns]
First few examples from the testing data:
                                                   text  ...                   data_name
840   The Somali refugees were labeled as unacquaint...  ...  SeeGULL - GPT Augmentation
1956  The Swiss man was considered spaghetti by his ...  ...  SeeGULL - GPT Augmentation
348   Barundians are often criticized for their unre...  ...  SeeGULL - GPT Augmentation
1735             The Afghans stars had to beg for music  ...  SeeGULL - GPT Augmentation
1913  The Mexican popcorn popped energetically throu...  ...  SeeGULL - GPT Augmentation

[5 rows x 4 columns]
Train data size: 1656
Test data size: 414
First few examples from the training data:
        group  ... data_name
38518    race  ...      MGSD
12979    race  ...      MGSD
35494    race  ...      MGSD
42213    race  ...      MGSD
16293  gender  ...      MGSD

[5 rows x 4 columns]
First few examples from the testing data:
            group  ... data_name
17851        race  ...      MGSD
37449        race  ...      MGSD
23634      gender  ...      MGSD
36047        race  ...      MGSD
28528  profession  ...      MGSD

[5 rows x 4 columns]
Train data size: 41493
Test data size: 10374
First few examples from merged training data:
                                                text  ...                     data_name
0                          Gay people are survivors.  ...  Winoqueer - GPT Augmentation
1                      Transgender people are doors.  ...  Winoqueer - GPT Augmentation
2  most people who are Pansexual are undermined i...  ...  Winoqueer - GPT Augmentation
3                          most lesbians are sinful.  ...  Winoqueer - GPT Augmentation
4   All LGBTQ people are unfit for spaceship travel.  ...  Winoqueer - GPT Augmentation

[5 rows x 4 columns]
First few examples from merged testing data:
                                                text  ...                     data_name
0                  most people who are Gay are kites  ...  Winoqueer - GPT Augmentation
1            we expect that LGBTQ people are paints.  ...  Winoqueer - GPT Augmentation
2                      LGBTQ people are televisions.  ...  Winoqueer - GPT Augmentation
3  we expect that LGBTQ people are unfit for moon...  ...  Winoqueer - GPT Augmentation
4  all Transgender people face mislabeling but ar...  ...  Winoqueer - GPT Augmentation

[5 rows x 4 columns]
Train data merged size: 44104
Test data merged size: 11027
First few examples from merged training data:
                                                text  ...                   data_name
0  The Russian cautious approach to learning affe...  ...  SeeGULL - GPT Augmentation
1   The United States tourist was rude to the locals  ...  SeeGULL - GPT Augmentation
2  Ethiopian challenges were a symptom of economi...  ...  SeeGULL - GPT Augmentation
3  The Vietnamese factory workers were subjected ...  ...  SeeGULL - GPT Augmentation
4    The Mexican leader managed a large organization  ...  SeeGULL - GPT Augmentation

[5 rows x 4 columns]
First few examples from merged testing data:
                                                text  ...                   data_name
0  The Somali refugees were labeled as unacquaint...  ...  SeeGULL - GPT Augmentation
1  The Swiss man was considered spaghetti by his ...  ...  SeeGULL - GPT Augmentation
2  Barundians are often criticized for their unre...  ...  SeeGULL - GPT Augmentation
3             The Afghans stars had to beg for music  ...  SeeGULL - GPT Augmentation
4  The Mexican popcorn popped energetically throu...  ...  SeeGULL - GPT Augmentation

[5 rows x 4 columns]
Train data merged size: 43149
Test data merged size: 10788
First few examples from merged training data:
                                                text  ...                   data_name
0  The Russian cautious approach to learning affe...  ...  SeeGULL - GPT Augmentation
1   The United States tourist was rude to the locals  ...  SeeGULL - GPT Augmentation
2  Ethiopian challenges were a symptom of economi...  ...  SeeGULL - GPT Augmentation
3  The Vietnamese factory workers were subjected ...  ...  SeeGULL - GPT Augmentation
4    The Mexican leader managed a large organization  ...  SeeGULL - GPT Augmentation

[5 rows x 4 columns]
First few examples from merged testing data:
                                                text  ...                   data_name
0  The Somali refugees were labeled as unacquaint...  ...  SeeGULL - GPT Augmentation
1  The Swiss man was considered spaghetti by his ...  ...  SeeGULL - GPT Augmentation
2  Barundians are often criticized for their unre...  ...  SeeGULL - GPT Augmentation
3             The Afghans stars had to beg for music  ...  SeeGULL - GPT Augmentation
4  The Mexican popcorn popped energetically throu...  ...  SeeGULL - GPT Augmentation

[5 rows x 4 columns]
Train data merged size: 45760
Test data merged size: 11441
Number of unique labels: 2
[codecarbon INFO @ 13:12:15] [setup] RAM Tracking...
[codecarbon INFO @ 13:12:15] [setup] GPU Tracking...
[codecarbon INFO @ 13:12:15] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 13:12:15] [setup] CPU Tracking...
[codecarbon WARNING @ 13:12:15] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon WARNING @ 13:12:17] We saw that you have a AMD Ryzen 7 7700 8-Core Processor but we don't know it. Please contact us.
[codecarbon INFO @ 13:12:17] CPU Model on constant consumption mode: AMD Ryzen 7 7700 8-Core Processor
[codecarbon INFO @ 13:12:17] >>> Tracker's metadata:
[codecarbon INFO @ 13:12:17]   Platform system: Windows-11-10.0.26100-SP0
[codecarbon INFO @ 13:12:17]   Python version: 3.12.12
[codecarbon INFO @ 13:12:17]   CodeCarbon version: 2.8.0
[codecarbon INFO @ 13:12:17]   Available RAM : 47.116 GB
[codecarbon INFO @ 13:12:17]   CPU count: 16
[codecarbon INFO @ 13:12:17]   CPU model: AMD Ryzen 7 7700 8-Core Processor
[codecarbon INFO @ 13:12:17]   GPU count: 1
[codecarbon INFO @ 13:12:17]   GPU model: 1 x NVIDIA GeForce RTX 5060 Ti
[codecarbon INFO @ 13:12:20] Saving emissions data to file D:\UCL\HEARTS-Text-Stereotype-Detection-main\Model Training and Evaluation\emissions.csv
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--albert--albert-base-v2\snapshots\8e2f239c5f8a2c0f253781ca60135db913e5c80c\config.json
Model config AlbertConfig {
  "_name_or_path": "albert/albert-base-v2",
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

loading weights file model.safetensors from cache at C:\Users\32618\.cache\huggingface\hub\models--albert--albert-base-v2\snapshots\8e2f239c5f8a2c0f253781ca60135db913e5c80c\model.safetensors
Some weights of the model checkpoint at albert/albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--albert--albert-base-v2\snapshots\8e2f239c5f8a2c0f253781ca60135db913e5c80c\config.json
Model config AlbertConfig {
  "_name_or_path": "albert/albert-base-v2",
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

loading file spiece.model from cache at C:\Users\32618\.cache\huggingface\hub\models--albert--albert-base-v2\snapshots\8e2f239c5f8a2c0f253781ca60135db913e5c80c\spiece.model
loading file tokenizer.json from cache at C:\Users\32618\.cache\huggingface\hub\models--albert--albert-base-v2\snapshots\8e2f239c5f8a2c0f253781ca60135db913e5c80c\tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--albert--albert-base-v2\snapshots\8e2f239c5f8a2c0f253781ca60135db913e5c80c\tokenizer_config.json
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--albert--albert-base-v2\snapshots\8e2f239c5f8a2c0f253781ca60135db913e5c80c\config.json
Model config AlbertConfig {
  "_name_or_path": "albert/albert-base-v2",
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33194/33194 [00:00<00:00, 40745.01 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33194/33194 [00:01<00:00, 20266.41 examples/s]
Sample tokenized input from train: {'group': 'religion', 'text': 'Have you ever met someone named Mohammed? I need a new pair of pajamas.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 36953, 'input_ids': [2, 57, 42, 462, 798, 737, 377, 13479, 60, 31, 376, 21, 78, 2146, 16, 23358, 18, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8299/8299 [00:00<00:00, 49054.42 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8299/8299 [00:00<00:00, 17395.39 examples/s]
Sample tokenized input from validation: {'group': 'religion', 'text': 'Have you ever met someone named Mohammed? I need a new pair of pajamas.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 36953, 'input_ids': [2, 57, 42, 462, 798, 737, 377, 13479, 60, 31, 376, 21, 78, 2146, 16, 23358, 18, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
D:\UCL\HEARTS-Text-Stereotype-Detection-main\Model Training and Evaluation\BERT_Models_Fine_Tuning.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
[codecarbon ERROR @ 13:12:28] Error: Another instance of codecarbon is probably running as we find `C:\Users\32618\AppData\Local\Temp\.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.
The following columns in the training set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running training *****
  Num examples = 33,194
  Num Epochs = 6
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 3,114
  Number of trainable parameters = 11,685,122
[codecarbon WARNING @ 13:12:28] Another instance of codecarbon is already running. Exiting.
  1%|          | 23/3114 [00:06<14:49,  3.48it/s][codecarbon INFO @ 13:12:35] Energy consumed for RAM : 0.000074 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:12:35] Energy consumed for all GPUs : 0.000288 kWh. Total GPU Power : 69.20967055537815 W
[codecarbon INFO @ 13:12:35] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:12:35] 0.000539 kWh of electricity used since the beginning.
  2%|â–         | 76/3114 [00:21<15:20,  3.30it/s][codecarbon INFO @ 13:12:50] Energy consumed for RAM : 0.000147 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:12:50] Energy consumed for all GPUs : 0.000888 kWh. Total GPU Power : 143.7956143510262 W
[codecarbon INFO @ 13:12:50] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:12:50] 0.001390 kWh of electricity used since the beginning.
  4%|â–         | 128/3114 [00:37<14:27,  3.44it/s][codecarbon INFO @ 13:13:05] Energy consumed for RAM : 0.000221 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:13:05] Energy consumed for all GPUs : 0.001483 kWh. Total GPU Power : 142.76529676641923 W
[codecarbon INFO @ 13:13:05] Energy consumed for all CPUs : 0.000531 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:13:05] 0.002235 kWh of electricity used since the beginning.
  6%|â–Œ         | 180/3114 [00:52<13:54,  3.52it/s][codecarbon INFO @ 13:13:20] Energy consumed for RAM : 0.000295 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:13:20] Energy consumed for all GPUs : 0.002087 kWh. Total GPU Power : 144.9642787411199 W
[codecarbon INFO @ 13:13:20] Energy consumed for all CPUs : 0.000709 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:13:20] 0.003090 kWh of electricity used since the beginning.
  7%|â–‹         | 233/3114 [01:06<13:46,  3.49it/s][codecarbon INFO @ 13:13:35] Energy consumed for RAM : 0.000368 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:13:35] Energy consumed for all GPUs : 0.002700 kWh. Total GPU Power : 146.99652146924154 W
[codecarbon INFO @ 13:13:35] Energy consumed for all CPUs : 0.000886 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:13:35] 0.003954 kWh of electricity used since the beginning.
  9%|â–‰         | 286/3114 [01:22<13:24,  3.52it/s][codecarbon INFO @ 13:13:50] Energy consumed for RAM : 0.000442 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:13:50] Energy consumed for all GPUs : 0.003308 kWh. Total GPU Power : 145.98699946727098 W
[codecarbon INFO @ 13:13:50] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:13:50] 0.004813 kWh of electricity used since the beginning.
 11%|â–ˆ         | 338/3114 [01:37<13:02,  3.55it/s][codecarbon INFO @ 13:14:05] Energy consumed for RAM : 0.000516 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:14:05] Energy consumed for all GPUs : 0.003924 kWh. Total GPU Power : 147.68794659000588 W
[codecarbon INFO @ 13:14:05] Energy consumed for all CPUs : 0.001240 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:14:05] 0.005680 kWh of electricity used since the beginning.
 13%|â–ˆâ–Ž        | 391/3114 [01:52<12:55,  3.51it/s][codecarbon INFO @ 13:14:20] Energy consumed for RAM : 0.000589 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:14:20] Energy consumed for all GPUs : 0.004540 kWh. Total GPU Power : 147.67250729699657 W
[codecarbon INFO @ 13:14:20] Energy consumed for all CPUs : 0.001417 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:14:20] 0.006546 kWh of electricity used since the beginning.
[codecarbon INFO @ 13:14:20] 0.012955 g.CO2eq/s mean an estimation of 408.5390959459104 kg.CO2eq/year
 14%|â–ˆâ–        | 444/3114 [02:07<12:43,  3.50it/s][codecarbon INFO @ 13:14:35] Energy consumed for RAM : 0.000663 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:14:35] Energy consumed for all GPUs : 0.005157 kWh. Total GPU Power : 148.1212927320199 W
[codecarbon INFO @ 13:14:35] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:14:35] 0.007414 kWh of electricity used since the beginning.
 16%|â–ˆâ–Œ        | 498/3114 [02:22<12:19,  3.54it/s][codecarbon INFO @ 13:14:50] Energy consumed for RAM : 0.000736 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:14:50] Energy consumed for all GPUs : 0.005776 kWh. Total GPU Power : 148.39157363153828 W
[codecarbon INFO @ 13:14:50] Energy consumed for all CPUs : 0.001772 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:14:50] 0.008284 kWh of electricity used since the beginning.
 16%|â–ˆâ–Œ        | 500/3114 [02:22<12:28,  3.49it/s]{'loss': 0.5037, 'grad_norm': 8.987113952636719, 'learning_rate': 1.678869621066153e-05, 'epoch': 0.96}
 17%|â–ˆâ–‹        | 519/3114 [02:28<11:14,  3.85it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 8299
  Batch size = 64

  0%|          | 0/130 [00:00<?, ?it/s]
  2%|â–         | 3/130 [00:00<00:05, 21.22it/s]
  5%|â–         | 6/130 [00:00<00:07, 16.25it/s]
  6%|â–Œ         | 8/130 [00:00<00:07, 15.45it/s]
  8%|â–Š         | 10/130 [00:00<00:08, 14.87it/s]
  9%|â–‰         | 12/130 [00:00<00:08, 14.67it/s]
 11%|â–ˆ         | 14/130 [00:00<00:08, 14.40it/s]
 12%|â–ˆâ–        | 16/130 [00:01<00:08, 13.84it/s]
 14%|â–ˆâ–        | 18/130 [00:01<00:08, 12.92it/s]
 15%|â–ˆâ–Œ        | 20/130 [00:01<00:08, 12.49it/s]
 17%|â–ˆâ–‹        | 22/130 [00:01<00:08, 12.09it/s]
 18%|â–ˆâ–Š        | 24/130 [00:01<00:08, 11.92it/s]
 20%|â–ˆâ–ˆ        | 26/130 [00:01<00:08, 11.71it/s]
 22%|â–ˆâ–ˆâ–       | 28/130 [00:02<00:08, 11.64it/s]
 23%|â–ˆâ–ˆâ–Ž       | 30/130 [00:02<00:08, 11.52it/s]
 25%|â–ˆâ–ˆâ–       | 32/130 [00:02<00:08, 11.53it/s]
 26%|â–ˆâ–ˆâ–Œ       | 34/130 [00:02<00:08, 11.42it/s]
 28%|â–ˆâ–ˆâ–Š       | 36/130 [00:02<00:08, 11.43it/s]
 29%|â–ˆâ–ˆâ–‰       | 38/130 [00:03<00:08, 11.39it/s]
 31%|â–ˆâ–ˆâ–ˆ       | 40/130 [00:03<00:07, 11.37it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 42/130 [00:03<00:07, 11.32it/s]
 34%|â–ˆâ–ˆâ–ˆâ–      | 44/130 [00:03<00:07, 11.36it/s]
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 46/130 [00:03<00:07, 11.33it/s]
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 48/130 [00:03<00:07, 11.54it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 50/130 [00:04<00:06, 11.79it/s]
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 52/130 [00:04<00:06, 12.03it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 54/130 [00:04<00:06, 12.08it/s]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 56/130 [00:04<00:06, 12.26it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 58/130 [00:04<00:05, 12.32it/s]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 60/130 [00:04<00:05, 12.42it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 62/130 [00:05<00:05, 12.42it/s]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 64/130 [00:05<00:05, 12.62it/s]
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 66/130 [00:05<00:04, 12.87it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 68/130 [00:05<00:04, 13.15it/s]
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 70/130 [00:05<00:04, 13.26it/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 72/130 [00:05<00:04, 13.42it/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 74/130 [00:05<00:04, 13.42it/s]
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 76/130 [00:06<00:03, 13.52it/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 78/130 [00:06<00:03, 13.52it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 80/130 [00:06<00:03, 13.49it/s]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 82/130 [00:06<00:03, 13.43it/s]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 84/130 [00:06<00:03, 13.44it/s]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 86/130 [00:06<00:03, 13.36it/s]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 88/130 [00:06<00:03, 13.38it/s]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 90/130 [00:07<00:03, 13.31it/s]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 92/130 [00:07<00:02, 13.35it/s]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 94/130 [00:07<00:02, 13.25it/s]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 96/130 [00:07<00:02, 13.13it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 98/130 [00:07<00:02, 12.92it/s]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 100/130 [00:07<00:02, 12.94it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 102/130 [00:08<00:02, 12.83it/s]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 104/130 [00:08<00:02, 12.88it/s]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 106/130 [00:08<00:01, 12.79it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 108/130 [00:08<00:01, 12.84it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 110/130 [00:08<00:01, 12.33it/s]
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 112/130 [00:08<00:01, 11.65it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 114/130 [00:09<00:01, 11.15it/s][codecarbon INFO @ 13:15:05] Energy consumed for RAM : 0.000810 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:15:05] Energy consumed for all GPUs : 0.006418 kWh. Total GPU Power : 153.9414778347887 W
[codecarbon INFO @ 13:15:05] Energy consumed for all CPUs : 0.001949 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:15:05] 0.009177 kWh of electricity used since the beginning.

 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 116/130 [00:09<00:01, 10.90it/s]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 118/130 [00:09<00:01, 10.53it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 120/130 [00:09<00:00, 10.45it/s]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 122/130 [00:09<00:00, 10.37it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 124/130 [00:10<00:00, 10.37it/s]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 126/130 [00:10<00:00, 10.69it/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 128/130 [00:10<00:00, 11.38it/s]
                                                  
 17%|â–ˆâ–‹        | 519/3114 [02:38<11:14,  3.85it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 130/130 [00:10<00:00, 12.46it/s]
                                                 Saving model checkpoint to model_output_albertv2\mgsd_trained\checkpoint-519
Configuration saved in model_output_albertv2\mgsd_trained\checkpoint-519\config.json
{'eval_loss': 0.43353918194770813, 'eval_precision': 0.7515490478788351, 'eval_recall': 0.7490298309775116, 'eval_f1': 0.7502409100856343, 'eval_balanced accuracy': 0.7490298309775116, 'eval_runtime': 10.575, 'eval_samples_per_second': 784.778, 'eval_steps_per_second': 12.293, 'epoch': 1.0}
Model weights saved in model_output_albertv2\mgsd_trained\checkpoint-519\model.safetensors
tokenizer config file saved in model_output_albertv2\mgsd_trained\checkpoint-519\tokenizer_config.json
Special tokens file saved in model_output_albertv2\mgsd_trained\checkpoint-519\special_tokens_map.json
 18%|â–ˆâ–Š        | 566/3114 [02:52<11:53,  3.57it/s][codecarbon INFO @ 13:15:20] Energy consumed for RAM : 0.000884 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:15:20] Energy consumed for all GPUs : 0.007034 kWh. Total GPU Power : 147.69339959418957 W
[codecarbon INFO @ 13:15:20] Energy consumed for all CPUs : 0.002126 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:15:20] 0.010044 kWh of electricity used since the beginning.
 20%|â–ˆâ–‰        | 619/3114 [03:07<11:42,  3.55it/s][codecarbon INFO @ 13:15:35] Energy consumed for RAM : 0.000958 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:15:35] Energy consumed for all GPUs : 0.007655 kWh. Total GPU Power : 149.12800141672372 W
[codecarbon INFO @ 13:15:35] Energy consumed for all CPUs : 0.002303 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:15:35] 0.010916 kWh of electricity used since the beginning.
 22%|â–ˆâ–ˆâ–       | 672/3114 [03:22<11:34,  3.52it/s][codecarbon INFO @ 13:15:50] Energy consumed for RAM : 0.001031 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:15:50] Energy consumed for all GPUs : 0.008275 kWh. Total GPU Power : 148.59245331334736 W
[codecarbon INFO @ 13:15:50] Energy consumed for all CPUs : 0.002481 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:15:50] 0.011787 kWh of electricity used since the beginning.
 23%|â–ˆâ–ˆâ–Ž       | 725/3114 [03:37<11:04,  3.59it/s][codecarbon INFO @ 13:16:05] Energy consumed for RAM : 0.001105 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:16:05] Energy consumed for all GPUs : 0.008897 kWh. Total GPU Power : 149.19734539295038 W
[codecarbon INFO @ 13:16:05] Energy consumed for all CPUs : 0.002658 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:16:05] 0.012660 kWh of electricity used since the beginning.
 25%|â–ˆâ–ˆâ–       | 778/3114 [03:52<10:51,  3.58it/s][codecarbon INFO @ 13:16:20] Energy consumed for RAM : 0.001178 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:16:20] Energy consumed for all GPUs : 0.009524 kWh. Total GPU Power : 150.4561460496672 W
[codecarbon INFO @ 13:16:20] Energy consumed for all CPUs : 0.002835 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:16:20] 0.013538 kWh of electricity used since the beginning.
[codecarbon INFO @ 13:16:20] 0.013833 g.CO2eq/s mean an estimation of 436.23552364381436 kg.CO2eq/year
 27%|â–ˆâ–ˆâ–‹       | 832/3114 [04:07<10:30,  3.62it/s][codecarbon INFO @ 13:16:35] Energy consumed for RAM : 0.001252 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:16:35] Energy consumed for all GPUs : 0.010149 kWh. Total GPU Power : 149.80284479343766 W
[codecarbon INFO @ 13:16:35] Energy consumed for all CPUs : 0.003012 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:16:35] 0.014413 kWh of electricity used since the beginning.
 28%|â–ˆâ–ˆâ–Š       | 885/3114 [04:22<10:38,  3.49it/s][codecarbon INFO @ 13:16:50] Energy consumed for RAM : 0.001326 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:16:50] Energy consumed for all GPUs : 0.010771 kWh. Total GPU Power : 149.44052851442424 W
[codecarbon INFO @ 13:16:50] Energy consumed for all CPUs : 0.003189 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:16:50] 0.015286 kWh of electricity used since the beginning.
 30%|â–ˆâ–ˆâ–ˆ       | 938/3114 [04:37<10:24,  3.48it/s][codecarbon INFO @ 13:17:05] Energy consumed for RAM : 0.001399 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:17:05] Energy consumed for all GPUs : 0.011394 kWh. Total GPU Power : 149.35189178656364 W
[codecarbon INFO @ 13:17:05] Energy consumed for all CPUs : 0.003366 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:17:05] 0.016160 kWh of electricity used since the beginning.
 32%|â–ˆâ–ˆâ–ˆâ–      | 991/3114 [04:52<10:03,  3.52it/s][codecarbon INFO @ 13:17:20] Energy consumed for RAM : 0.001473 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:17:20] Energy consumed for all GPUs : 0.012017 kWh. Total GPU Power : 149.438624722619 W
[codecarbon INFO @ 13:17:20] Energy consumed for all CPUs : 0.003544 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:17:20] 0.017033 kWh of electricity used since the beginning.
 32%|â–ˆâ–ˆâ–ˆâ–      | 1000/3114 [04:54<09:54,  3.55it/s]{'loss': 0.3817, 'grad_norm': 11.847031593322754, 'learning_rate': 1.357739242132306e-05, 'epoch': 1.93}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1038/3114 [05:05<08:30,  4.07it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 8299
  Batch size = 64

  0%|          | 0/130 [00:00<?, ?it/s]
  2%|â–         | 3/130 [00:00<00:06, 21.02it/s]
  5%|â–         | 6/130 [00:00<00:07, 16.21it/s]
  6%|â–Œ         | 8/130 [00:00<00:07, 15.42it/s]
  8%|â–Š         | 10/130 [00:00<00:08, 14.93it/s]
  9%|â–‰         | 12/130 [00:00<00:08, 14.70it/s]
 11%|â–ˆ         | 14/130 [00:00<00:08, 14.44it/s]
 12%|â–ˆâ–        | 16/130 [00:01<00:08, 13.83it/s]
 14%|â–ˆâ–        | 18/130 [00:01<00:08, 12.94it/s]
 15%|â–ˆâ–Œ        | 20/130 [00:01<00:08, 12.44it/s]
 17%|â–ˆâ–‹        | 22/130 [00:01<00:08, 12.07it/s][codecarbon INFO @ 13:17:35] Energy consumed for RAM : 0.001547 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:17:35] Energy consumed for all GPUs : 0.012644 kWh. Total GPU Power : 150.5189954860015 W
[codecarbon INFO @ 13:17:35] Energy consumed for all CPUs : 0.003721 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:17:35] 0.017912 kWh of electricity used since the beginning.

 18%|â–ˆâ–Š        | 24/130 [00:01<00:08, 11.92it/s]
 20%|â–ˆâ–ˆ        | 26/130 [00:01<00:08, 11.62it/s]
 22%|â–ˆâ–ˆâ–       | 28/130 [00:02<00:08, 11.58it/s]
 23%|â–ˆâ–ˆâ–Ž       | 30/130 [00:02<00:08, 11.48it/s]
 25%|â–ˆâ–ˆâ–       | 32/130 [00:02<00:08, 11.50it/s]
 26%|â–ˆâ–ˆâ–Œ       | 34/130 [00:02<00:08, 11.37it/s]
 28%|â–ˆâ–ˆâ–Š       | 36/130 [00:02<00:08, 11.35it/s]
 29%|â–ˆâ–ˆâ–‰       | 38/130 [00:03<00:08, 11.30it/s]
 31%|â–ˆâ–ˆâ–ˆ       | 40/130 [00:03<00:07, 11.34it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 42/130 [00:03<00:07, 11.27it/s]
 34%|â–ˆâ–ˆâ–ˆâ–      | 44/130 [00:03<00:07, 11.30it/s]
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 46/130 [00:03<00:07, 11.25it/s]
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 48/130 [00:03<00:07, 11.47it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 50/130 [00:04<00:06, 11.73it/s]
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 52/130 [00:04<00:06, 11.95it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 54/130 [00:04<00:06, 12.05it/s]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 56/130 [00:04<00:06, 12.22it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 58/130 [00:04<00:05, 12.25it/s]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 60/130 [00:04<00:05, 12.39it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 62/130 [00:05<00:05, 12.36it/s]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 64/130 [00:05<00:05, 12.60it/s]
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 66/130 [00:05<00:04, 12.86it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 68/130 [00:05<00:04, 13.13it/s]
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 70/130 [00:05<00:04, 13.22it/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 72/130 [00:05<00:04, 13.34it/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 74/130 [00:05<00:04, 13.38it/s]
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 76/130 [00:06<00:03, 13.52it/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 78/130 [00:06<00:03, 13.51it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 80/130 [00:06<00:03, 13.45it/s]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 82/130 [00:06<00:03, 13.35it/s]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 84/130 [00:06<00:03, 13.37it/s]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 86/130 [00:06<00:03, 13.30it/s]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 88/130 [00:06<00:03, 13.33it/s]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 90/130 [00:07<00:03, 13.25it/s]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 92/130 [00:07<00:02, 13.30it/s]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 94/130 [00:07<00:02, 13.19it/s]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 96/130 [00:07<00:02, 13.01it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 98/130 [00:07<00:02, 12.90it/s]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 100/130 [00:07<00:02, 12.94it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 102/130 [00:08<00:02, 12.84it/s]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 104/130 [00:08<00:02, 12.84it/s]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 106/130 [00:08<00:01, 12.78it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 108/130 [00:08<00:01, 12.78it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 110/130 [00:08<00:01, 12.30it/s]
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 112/130 [00:08<00:01, 11.65it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 114/130 [00:09<00:01, 11.14it/s]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 116/130 [00:09<00:01, 10.88it/s]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 118/130 [00:09<00:01, 10.64it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 120/130 [00:09<00:00, 10.55it/s]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 122/130 [00:09<00:00, 10.35it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 124/130 [00:10<00:00, 10.33it/s]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 126/130 [00:10<00:00, 10.66it/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 128/130 [00:10<00:00, 11.36it/s]
{'eval_loss': 0.3997514545917511, 'eval_precision': 0.7990228347623709, 'eval_recall': 0.7620368084274634, 'eval_f1': 0.774469779722433, 'eval_balanced accuracy': 0.7620368084274634, 'eval_runtime': 10.5979, 'eval_samples_per_second': 783.079, 'eval_steps_per_second': 12.267, 'epoch': 2.0}
                                                   
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1038/3114 [05:16<08:30,  4.07it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 130/130 [00:10<00:00, 12.46it/s]
                                                 Saving model checkpoint to model_output_albertv2\mgsd_trained\checkpoint-1038
Configuration saved in model_output_albertv2\mgsd_trained\checkpoint-1038\config.json
Model weights saved in model_output_albertv2\mgsd_trained\checkpoint-1038\model.safetensors
tokenizer config file saved in model_output_albertv2\mgsd_trained\checkpoint-1038\tokenizer_config.json
Special tokens file saved in model_output_albertv2\mgsd_trained\checkpoint-1038\special_tokens_map.json
Deleting older checkpoint [model_output_albertv2\mgsd_trained\checkpoint-519] due to args.save_total_limit
 34%|â–ˆâ–ˆâ–ˆâ–      | 1059/3114 [05:22<09:46,  3.50it/s][codecarbon INFO @ 13:17:50] Energy consumed for RAM : 0.001620 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:17:50] Energy consumed for all GPUs : 0.013287 kWh. Total GPU Power : 154.12710776916092 W
[codecarbon INFO @ 13:17:50] Energy consumed for all CPUs : 0.003898 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:17:50] 0.018805 kWh of electricity used since the beginning.
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1111/3114 [05:37<09:16,  3.60it/s][codecarbon INFO @ 13:18:06] Energy consumed for RAM : 0.001694 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:18:06] Energy consumed for all GPUs : 0.013912 kWh. Total GPU Power : 150.02595360972816 W
[codecarbon INFO @ 13:18:06] Energy consumed for all CPUs : 0.004075 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:18:06] 0.019682 kWh of electricity used since the beginning.
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1164/3114 [05:52<09:02,  3.59it/s][codecarbon INFO @ 13:18:21] Energy consumed for RAM : 0.001768 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:18:21] Energy consumed for all GPUs : 0.014540 kWh. Total GPU Power : 150.59480415712966 W
[codecarbon INFO @ 13:18:21] Energy consumed for all CPUs : 0.004252 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:18:21] 0.020560 kWh of electricity used since the beginning.
[codecarbon INFO @ 13:18:21] 0.013897 g.CO2eq/s mean an estimation of 438.2608131345113 kg.CO2eq/year
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1218/3114 [06:07<08:53,  3.55it/s][codecarbon INFO @ 13:18:36] Energy consumed for RAM : 0.001841 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:18:36] Energy consumed for all GPUs : 0.015169 kWh. Total GPU Power : 150.82234596301294 W
[codecarbon INFO @ 13:18:36] Energy consumed for all CPUs : 0.004429 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:18:36] 0.021440 kWh of electricity used since the beginning.
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1271/3114 [06:22<08:45,  3.51it/s][codecarbon INFO @ 13:18:51] Energy consumed for RAM : 0.001915 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:18:51] Energy consumed for all GPUs : 0.015797 kWh. Total GPU Power : 150.65715638672677 W
[codecarbon INFO @ 13:18:51] Energy consumed for all CPUs : 0.004607 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:18:51] 0.022319 kWh of electricity used since the beginning.
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1324/3114 [06:37<08:34,  3.48it/s][codecarbon INFO @ 13:19:06] Energy consumed for RAM : 0.001989 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:19:06] Energy consumed for all GPUs : 0.016423 kWh. Total GPU Power : 150.30652483558197 W
[codecarbon INFO @ 13:19:06] Energy consumed for all CPUs : 0.004784 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:19:06] 0.023196 kWh of electricity used since the beginning.
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1377/3114 [06:52<08:10,  3.54it/s][codecarbon INFO @ 13:19:21] Energy consumed for RAM : 0.002062 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:19:21] Energy consumed for all GPUs : 0.017055 kWh. Total GPU Power : 151.554687536898 W
[codecarbon INFO @ 13:19:21] Energy consumed for all CPUs : 0.004961 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:19:21] 0.024079 kWh of electricity used since the beginning.
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1430/3114 [07:07<07:43,  3.64it/s][codecarbon INFO @ 13:19:36] Energy consumed for RAM : 0.002136 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:19:36] Energy consumed for all GPUs : 0.017683 kWh. Total GPU Power : 150.59557145465658 W
[codecarbon INFO @ 13:19:36] Energy consumed for all CPUs : 0.005138 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:19:36] 0.024957 kWh of electricity used since the beginning.
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1483/3114 [07:22<07:33,  3.60it/s][codecarbon INFO @ 13:19:51] Energy consumed for RAM : 0.002210 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:19:51] Energy consumed for all GPUs : 0.018311 kWh. Total GPU Power : 150.47745532814957 W
[codecarbon INFO @ 13:19:51] Energy consumed for all CPUs : 0.005315 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:19:51] 0.025836 kWh of electricity used since the beginning.
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1500/3114 [07:27<07:43,  3.48it/s]{'loss': 0.2965, 'grad_norm': 27.670242309570312, 'learning_rate': 1.0366088631984585e-05, 'epoch': 2.89}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1536/3114 [07:37<07:33,  3.48it/s][codecarbon INFO @ 13:20:06] Energy consumed for RAM : 0.002283 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:20:06] Energy consumed for all GPUs : 0.018940 kWh. Total GPU Power : 150.93308850797436 W
[codecarbon INFO @ 13:20:06] Energy consumed for all CPUs : 0.005493 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:20:06] 0.026715 kWh of electricity used since the beginning.
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1557/3114 [07:43<06:27,  4.02it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 8299
  Batch size = 64

  0%|          | 0/130 [00:00<?, ?it/s]
  2%|â–         | 3/130 [00:00<00:06, 20.88it/s]
  5%|â–         | 6/130 [00:00<00:07, 16.20it/s]
  6%|â–Œ         | 8/130 [00:00<00:07, 15.46it/s]
  8%|â–Š         | 10/130 [00:00<00:08, 14.97it/s]
  9%|â–‰         | 12/130 [00:00<00:08, 14.69it/s]
 11%|â–ˆ         | 14/130 [00:00<00:08, 14.49it/s]
 12%|â–ˆâ–        | 16/130 [00:01<00:08, 13.88it/s]
 14%|â–ˆâ–        | 18/130 [00:01<00:08, 12.94it/s]
 15%|â–ˆâ–Œ        | 20/130 [00:01<00:08, 12.49it/s]
 17%|â–ˆâ–‹        | 22/130 [00:01<00:08, 12.09it/s]
 18%|â–ˆâ–Š        | 24/130 [00:01<00:08, 11.93it/s]
 20%|â–ˆâ–ˆ        | 26/130 [00:01<00:08, 11.71it/s]
 22%|â–ˆâ–ˆâ–       | 28/130 [00:02<00:08, 11.66it/s]
 23%|â–ˆâ–ˆâ–Ž       | 30/130 [00:02<00:08, 11.54it/s]
 25%|â–ˆâ–ˆâ–       | 32/130 [00:02<00:08, 11.52it/s]
 26%|â–ˆâ–ˆâ–Œ       | 34/130 [00:02<00:08, 11.41it/s]
 28%|â–ˆâ–ˆâ–Š       | 36/130 [00:02<00:08, 11.41it/s]
 29%|â–ˆâ–ˆâ–‰       | 38/130 [00:03<00:08, 11.35it/s]
 31%|â–ˆâ–ˆâ–ˆ       | 40/130 [00:03<00:07, 11.33it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 42/130 [00:03<00:07, 11.26it/s]
 34%|â–ˆâ–ˆâ–ˆâ–      | 44/130 [00:03<00:07, 11.33it/s]
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 46/130 [00:03<00:07, 11.26it/s]
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 48/130 [00:03<00:07, 11.48it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 50/130 [00:04<00:06, 11.72it/s]
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 52/130 [00:04<00:06, 11.92it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 54/130 [00:04<00:06, 12.00it/s]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 56/130 [00:04<00:06, 12.15it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 58/130 [00:04<00:05, 12.18it/s]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 60/130 [00:04<00:05, 12.33it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 62/130 [00:05<00:05, 12.32it/s]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 64/130 [00:05<00:05, 12.53it/s]
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 66/130 [00:05<00:04, 12.84it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 68/130 [00:05<00:04, 13.12it/s]
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 70/130 [00:05<00:04, 13.25it/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 72/130 [00:05<00:04, 13.43it/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 74/130 [00:05<00:04, 13.45it/s]
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 76/130 [00:06<00:03, 13.54it/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 78/130 [00:06<00:03, 13.57it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 80/130 [00:06<00:03, 13.53it/s]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 82/130 [00:06<00:03, 13.38it/s]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 84/130 [00:06<00:03, 13.41it/s]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 86/130 [00:06<00:03, 13.32it/s]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 88/130 [00:06<00:03, 13.35it/s]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 90/130 [00:07<00:03, 13.27it/s]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 92/130 [00:07<00:02, 13.32it/s]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 94/130 [00:07<00:02, 13.17it/s]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 96/130 [00:07<00:02, 13.09it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 98/130 [00:07<00:02, 12.91it/s]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 100/130 [00:07<00:02, 12.89it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 102/130 [00:08<00:02, 12.78it/s]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 104/130 [00:08<00:02, 12.80it/s]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 106/130 [00:08<00:01, 12.74it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 108/130 [00:08<00:01, 12.78it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 110/130 [00:08<00:01, 12.25it/s]
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 112/130 [00:08<00:01, 11.58it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 114/130 [00:09<00:01, 11.10it/s]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 116/130 [00:09<00:01, 10.85it/s][codecarbon INFO @ 13:20:21] Energy consumed for RAM : 0.002357 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:20:21] Energy consumed for all GPUs : 0.019592 kWh. Total GPU Power : 156.46524974930705 W
[codecarbon INFO @ 13:20:21] Energy consumed for all CPUs : 0.005670 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:20:21] 0.027618 kWh of electricity used since the beginning.
[codecarbon INFO @ 13:20:21] 0.013967 g.CO2eq/s mean an estimation of 440.4717494648265 kg.CO2eq/year

 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 118/130 [00:09<00:01, 10.60it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 120/130 [00:09<00:00, 10.33it/s]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 122/130 [00:09<00:00, 10.32it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 124/130 [00:10<00:00, 10.26it/s]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 126/130 [00:10<00:00, 10.66it/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 128/130 [00:10<00:00, 11.29it/s]
                                                   
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1557/3114 [07:53<06:27,  4.02it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 130/130 [00:10<00:00, 12.52it/s]
                                                 Saving model checkpoint to model_output_albertv2\mgsd_trained\checkpoint-1557
Configuration saved in model_output_albertv2\mgsd_trained\checkpoint-1557\config.json
{'eval_loss': 0.41851210594177246, 'eval_precision': 0.8044090721534027, 'eval_recall': 0.7768242508178507, 'eval_f1': 0.7871211097469084, 'eval_balanced accuracy': 0.7768242508178507, 'eval_runtime': 10.6013, 'eval_samples_per_second': 782.826, 'eval_steps_per_second': 12.263, 'epoch': 3.0}
Model weights saved in model_output_albertv2\mgsd_trained\checkpoint-1557\model.safetensors
tokenizer config file saved in model_output_albertv2\mgsd_trained\checkpoint-1557\tokenizer_config.json
Special tokens file saved in model_output_albertv2\mgsd_trained\checkpoint-1557\special_tokens_map.json
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1605/3114 [08:07<07:10,  3.50it/s][codecarbon INFO @ 13:20:36] Energy consumed for RAM : 0.002431 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:20:36] Energy consumed for all GPUs : 0.020217 kWh. Total GPU Power : 149.8895408927814 W
[codecarbon INFO @ 13:20:36] Energy consumed for all CPUs : 0.005847 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:20:36] 0.028494 kWh of electricity used since the beginning.
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1658/3114 [08:22<06:56,  3.50it/s][codecarbon INFO @ 13:20:51] Energy consumed for RAM : 0.002504 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:20:51] Energy consumed for all GPUs : 0.020847 kWh. Total GPU Power : 151.1921684439513 W
[codecarbon INFO @ 13:20:51] Energy consumed for all CPUs : 0.006024 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:20:51] 0.029375 kWh of electricity used since the beginning.
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1710/3114 [08:37<06:44,  3.47it/s][codecarbon INFO @ 13:21:06] Energy consumed for RAM : 0.002578 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:21:06] Energy consumed for all GPUs : 0.021476 kWh. Total GPU Power : 150.97239013652876 W
[codecarbon INFO @ 13:21:06] Energy consumed for all CPUs : 0.006201 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:21:06] 0.030255 kWh of electricity used since the beginning.
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1764/3114 [08:52<06:21,  3.53it/s][codecarbon INFO @ 13:21:21] Energy consumed for RAM : 0.002652 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:21:21] Energy consumed for all GPUs : 0.022107 kWh. Total GPU Power : 151.29487319576012 W
[codecarbon INFO @ 13:21:21] Energy consumed for all CPUs : 0.006378 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:21:21] 0.031137 kWh of electricity used since the beginning.
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1817/3114 [09:07<06:08,  3.52it/s][codecarbon INFO @ 13:21:36] Energy consumed for RAM : 0.002725 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:21:36] Energy consumed for all GPUs : 0.022736 kWh. Total GPU Power : 150.89753577191513 W
[codecarbon INFO @ 13:21:36] Energy consumed for all CPUs : 0.006556 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:21:36] 0.032017 kWh of electricity used since the beginning.
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1870/3114 [09:22<05:57,  3.48it/s][codecarbon INFO @ 13:21:51] Energy consumed for RAM : 0.002799 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:21:51] Energy consumed for all GPUs : 0.023369 kWh. Total GPU Power : 151.91119564376922 W
[codecarbon INFO @ 13:21:51] Energy consumed for all CPUs : 0.006733 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:21:51] 0.032901 kWh of electricity used since the beginning.
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1923/3114 [09:37<05:42,  3.48it/s][codecarbon INFO @ 13:22:06] Energy consumed for RAM : 0.002872 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:22:06] Energy consumed for all GPUs : 0.023998 kWh. Total GPU Power : 150.8525392942632 W
[codecarbon INFO @ 13:22:06] Energy consumed for all CPUs : 0.006910 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:22:06] 0.033781 kWh of electricity used since the beginning.
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1977/3114 [09:52<05:22,  3.53it/s][codecarbon INFO @ 13:22:21] Energy consumed for RAM : 0.002946 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:22:21] Energy consumed for all GPUs : 0.024631 kWh. Total GPU Power : 151.75438168409394 W
[codecarbon INFO @ 13:22:21] Energy consumed for all CPUs : 0.007087 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:22:21] 0.034664 kWh of electricity used since the beginning.
[codecarbon INFO @ 13:22:21] 0.013942 g.CO2eq/s mean an estimation of 439.66612950388156 kg.CO2eq/year
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2000/3114 [09:59<05:14,  3.54it/s]{'loss': 0.2074, 'grad_norm': 10.227544784545898, 'learning_rate': 7.154784842646115e-06, 'epoch': 3.85}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2030/3114 [10:07<05:11,  3.48it/s][codecarbon INFO @ 13:22:36] Energy consumed for RAM : 0.003020 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:22:36] Energy consumed for all GPUs : 0.025261 kWh. Total GPU Power : 151.05099176453956 W
[codecarbon INFO @ 13:22:36] Energy consumed for all CPUs : 0.007264 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:22:36] 0.035545 kWh of electricity used since the beginning.
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2076/3114 [10:20<04:26,  3.90it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 8299
  Batch size = 64

  0%|          | 0/130 [00:00<?, ?it/s]
  2%|â–         | 3/130 [00:00<00:05, 21.33it/s]
  5%|â–         | 6/130 [00:00<00:07, 16.35it/s]
  6%|â–Œ         | 8/130 [00:00<00:07, 15.52it/s]
  8%|â–Š         | 10/130 [00:00<00:08, 14.93it/s]
  9%|â–‰         | 12/130 [00:00<00:08, 14.71it/s]
 11%|â–ˆ         | 14/130 [00:00<00:08, 14.42it/s]
 12%|â–ˆâ–        | 16/130 [00:01<00:08, 13.83it/s]
 14%|â–ˆâ–        | 18/130 [00:01<00:08, 12.90it/s]
 15%|â–ˆâ–Œ        | 20/130 [00:01<00:08, 12.44it/s]
 17%|â–ˆâ–‹        | 22/130 [00:01<00:08, 12.03it/s]
 18%|â–ˆâ–Š        | 24/130 [00:01<00:08, 11.86it/s]
 20%|â–ˆâ–ˆ        | 26/130 [00:01<00:08, 11.68it/s][codecarbon INFO @ 13:22:51] Energy consumed for RAM : 0.003093 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:22:51] Energy consumed for all GPUs : 0.025895 kWh. Total GPU Power : 152.11614490663055 W
[codecarbon INFO @ 13:22:51] Energy consumed for all CPUs : 0.007442 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:22:51] 0.036430 kWh of electricity used since the beginning.

 22%|â–ˆâ–ˆâ–       | 28/130 [00:02<00:08, 11.60it/s]
 23%|â–ˆâ–ˆâ–Ž       | 30/130 [00:02<00:08, 11.35it/s]
 25%|â–ˆâ–ˆâ–       | 32/130 [00:02<00:08, 11.39it/s]
 26%|â–ˆâ–ˆâ–Œ       | 34/130 [00:02<00:08, 11.28it/s]
 28%|â–ˆâ–ˆâ–Š       | 36/130 [00:02<00:08, 11.26it/s]
 29%|â–ˆâ–ˆâ–‰       | 38/130 [00:03<00:08, 11.17it/s]
 31%|â–ˆâ–ˆâ–ˆ       | 40/130 [00:03<00:08, 11.23it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 42/130 [00:03<00:07, 11.23it/s]
 34%|â–ˆâ–ˆâ–ˆâ–      | 44/130 [00:03<00:07, 11.27it/s]
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 46/130 [00:03<00:07, 11.24it/s]
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 48/130 [00:03<00:07, 11.43it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 50/130 [00:04<00:06, 11.69it/s]
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 52/130 [00:04<00:06, 11.96it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 54/130 [00:04<00:06, 12.08it/s]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 56/130 [00:04<00:06, 12.22it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 58/130 [00:04<00:05, 12.25it/s]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 60/130 [00:04<00:05, 12.33it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 62/130 [00:05<00:05, 12.31it/s]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 64/130 [00:05<00:05, 12.56it/s]
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 66/130 [00:05<00:04, 12.89it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 68/130 [00:05<00:04, 13.16it/s]
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 70/130 [00:05<00:04, 13.30it/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 72/130 [00:05<00:04, 13.47it/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 74/130 [00:05<00:04, 13.50it/s]
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 76/130 [00:06<00:03, 13.60it/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 78/130 [00:06<00:03, 13.55it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 80/130 [00:06<00:03, 13.52it/s]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 82/130 [00:06<00:03, 13.44it/s]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 84/130 [00:06<00:03, 13.42it/s]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 86/130 [00:06<00:03, 13.31it/s]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 88/130 [00:06<00:03, 13.33it/s]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 90/130 [00:07<00:03, 13.23it/s]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 92/130 [00:07<00:02, 13.30it/s]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 94/130 [00:07<00:02, 13.15it/s]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 96/130 [00:07<00:02, 13.09it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 98/130 [00:07<00:02, 12.95it/s]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 100/130 [00:07<00:02, 12.94it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 102/130 [00:08<00:02, 12.84it/s]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 104/130 [00:08<00:02, 12.86it/s]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 106/130 [00:08<00:01, 12.75it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 108/130 [00:08<00:01, 12.79it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 110/130 [00:08<00:01, 12.24it/s]
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 112/130 [00:08<00:01, 11.60it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 114/130 [00:09<00:01, 11.13it/s]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 116/130 [00:09<00:01, 10.85it/s]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 118/130 [00:09<00:01, 10.61it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 120/130 [00:09<00:00, 10.52it/s]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 122/130 [00:09<00:00, 10.40it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 124/130 [00:10<00:00, 10.36it/s]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 126/130 [00:10<00:00, 10.67it/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 128/130 [00:10<00:00, 11.36it/s]
                                                   
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2076/3114 [10:31<04:26,  3.90it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 130/130 [00:10<00:00, 12.49it/s]
                                                 Saving model checkpoint to model_output_albertv2\mgsd_trained\checkpoint-2076
Configuration saved in model_output_albertv2\mgsd_trained\checkpoint-2076\config.json
{'eval_loss': 0.4647621512413025, 'eval_precision': 0.8040177575217999, 'eval_recall': 0.788542856150855, 'eval_f1': 0.795072029782208, 'eval_balanced accuracy': 0.788542856150855, 'eval_runtime': 10.6047, 'eval_samples_per_second': 782.577, 'eval_steps_per_second': 12.259, 'epoch': 4.0}
Model weights saved in model_output_albertv2\mgsd_trained\checkpoint-2076\model.safetensors
tokenizer config file saved in model_output_albertv2\mgsd_trained\checkpoint-2076\tokenizer_config.json
Special tokens file saved in model_output_albertv2\mgsd_trained\checkpoint-2076\special_tokens_map.json
Deleting older checkpoint [model_output_albertv2\mgsd_trained\checkpoint-1557] due to args.save_total_limit
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2097/3114 [10:37<04:55,  3.44it/s][codecarbon INFO @ 13:23:06] Energy consumed for RAM : 0.003167 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:23:06] Energy consumed for all GPUs : 0.026542 kWh. Total GPU Power : 155.32940261372312 W
[codecarbon INFO @ 13:23:06] Energy consumed for all CPUs : 0.007619 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:23:06] 0.037328 kWh of electricity used since the beginning.
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2151/3114 [10:52<04:36,  3.48it/s][codecarbon INFO @ 13:23:21] Energy consumed for RAM : 0.003241 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:23:21] Energy consumed for all GPUs : 0.027176 kWh. Total GPU Power : 151.95910663004912 W
[codecarbon INFO @ 13:23:21] Energy consumed for all CPUs : 0.007796 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:23:21] 0.038213 kWh of electricity used since the beginning.
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2204/3114 [11:07<04:19,  3.51it/s][codecarbon INFO @ 13:23:36] Energy consumed for RAM : 0.003314 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:23:36] Energy consumed for all GPUs : 0.027806 kWh. Total GPU Power : 151.22239394203322 W
[codecarbon INFO @ 13:23:36] Energy consumed for all CPUs : 0.007973 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:23:36] 0.039094 kWh of electricity used since the beginning.
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2256/3114 [11:22<04:06,  3.48it/s][codecarbon INFO @ 13:23:51] Energy consumed for RAM : 0.003388 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:23:51] Energy consumed for all GPUs : 0.028436 kWh. Total GPU Power : 150.98078897571372 W
[codecarbon INFO @ 13:23:51] Energy consumed for all CPUs : 0.008150 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:23:51] 0.039975 kWh of electricity used since the beginning.
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2309/3114 [11:37<03:42,  3.62it/s][codecarbon INFO @ 13:24:06] Energy consumed for RAM : 0.003462 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:24:06] Energy consumed for all GPUs : 0.029068 kWh. Total GPU Power : 151.79427917844717 W
[codecarbon INFO @ 13:24:06] Energy consumed for all CPUs : 0.008328 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:24:06] 0.040858 kWh of electricity used since the beginning.
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2362/3114 [11:52<03:36,  3.48it/s][codecarbon INFO @ 13:24:21] Energy consumed for RAM : 0.003535 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:24:21] Energy consumed for all GPUs : 0.029704 kWh. Total GPU Power : 152.3902091023921 W
[codecarbon INFO @ 13:24:21] Energy consumed for all CPUs : 0.008505 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:24:21] 0.041744 kWh of electricity used since the beginning.
[codecarbon INFO @ 13:24:21] 0.014008 g.CO2eq/s mean an estimation of 441.7700053669155 kg.CO2eq/year
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2416/3114 [12:07<03:12,  3.62it/s][codecarbon INFO @ 13:24:36] Energy consumed for RAM : 0.003609 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:24:36] Energy consumed for all GPUs : 0.030336 kWh. Total GPU Power : 151.82873871682074 W
[codecarbon INFO @ 13:24:36] Energy consumed for all CPUs : 0.008682 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:24:36] 0.042627 kWh of electricity used since the beginning.
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2469/3114 [12:22<03:04,  3.49it/s][codecarbon INFO @ 13:24:51] Energy consumed for RAM : 0.003683 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:24:51] Energy consumed for all GPUs : 0.030969 kWh. Total GPU Power : 151.7360536606215 W
[codecarbon INFO @ 13:24:51] Energy consumed for all CPUs : 0.008859 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:24:51] 0.043511 kWh of electricity used since the beginning.
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2500/3114 [12:31<02:56,  3.48it/s]{'loss': 0.1292, 'grad_norm': 42.9583740234375, 'learning_rate': 3.9434810533076434e-06, 'epoch': 4.82}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2522/3114 [12:37<02:50,  3.48it/s][codecarbon INFO @ 13:25:06] Energy consumed for RAM : 0.003756 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:25:06] Energy consumed for all GPUs : 0.031601 kWh. Total GPU Power : 151.59218255431222 W
[codecarbon INFO @ 13:25:06] Energy consumed for all CPUs : 0.009036 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:25:06] 0.044394 kWh of electricity used since the beginning.
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2575/3114 [12:52<02:31,  3.56it/s][codecarbon INFO @ 13:25:21] Energy consumed for RAM : 0.003830 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:25:21] Energy consumed for all GPUs : 0.032232 kWh. Total GPU Power : 151.28077491589366 W
[codecarbon INFO @ 13:25:21] Energy consumed for all CPUs : 0.009213 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:25:21] 0.045275 kWh of electricity used since the beginning.
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2595/3114 [12:58<02:15,  3.82it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 8299
  Batch size = 64

  0%|          | 0/130 [00:00<?, ?it/s]
  2%|â–         | 3/130 [00:00<00:05, 21.19it/s]
  5%|â–         | 6/130 [00:00<00:07, 16.08it/s]
  6%|â–Œ         | 8/130 [00:00<00:07, 15.37it/s]
  8%|â–Š         | 10/130 [00:00<00:08, 14.83it/s]
  9%|â–‰         | 12/130 [00:00<00:08, 14.58it/s]
 11%|â–ˆ         | 14/130 [00:00<00:08, 14.25it/s]
 12%|â–ˆâ–        | 16/130 [00:01<00:08, 13.73it/s]
 14%|â–ˆâ–        | 18/130 [00:01<00:08, 12.85it/s]
 15%|â–ˆâ–Œ        | 20/130 [00:01<00:08, 12.42it/s]
 17%|â–ˆâ–‹        | 22/130 [00:01<00:08, 12.04it/s]
 18%|â–ˆâ–Š        | 24/130 [00:01<00:08, 11.86it/s]
 20%|â–ˆâ–ˆ        | 26/130 [00:01<00:08, 11.67it/s]
 22%|â–ˆâ–ˆâ–       | 28/130 [00:02<00:08, 11.63it/s]
 23%|â–ˆâ–ˆâ–Ž       | 30/130 [00:02<00:08, 11.51it/s]
 25%|â–ˆâ–ˆâ–       | 32/130 [00:02<00:08, 11.50it/s]
 26%|â–ˆâ–ˆâ–Œ       | 34/130 [00:02<00:08, 11.38it/s]
 28%|â–ˆâ–ˆâ–Š       | 36/130 [00:02<00:08, 11.36it/s]
 29%|â–ˆâ–ˆâ–‰       | 38/130 [00:03<00:08, 11.28it/s]
 31%|â–ˆâ–ˆâ–ˆ       | 40/130 [00:03<00:07, 11.31it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 42/130 [00:03<00:07, 11.26it/s]
 34%|â–ˆâ–ˆâ–ˆâ–      | 44/130 [00:03<00:07, 11.30it/s]
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 46/130 [00:03<00:07, 11.24it/s]
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 48/130 [00:03<00:07, 11.45it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 50/130 [00:04<00:06, 11.70it/s]
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 52/130 [00:04<00:06, 11.98it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 54/130 [00:04<00:06, 12.06it/s]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 56/130 [00:04<00:06, 12.23it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 58/130 [00:04<00:05, 12.25it/s]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 60/130 [00:04<00:05, 12.33it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 62/130 [00:05<00:05, 12.33it/s]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 64/130 [00:05<00:05, 12.58it/s]
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 66/130 [00:05<00:04, 12.86it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 68/130 [00:05<00:04, 13.12it/s]
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 70/130 [00:05<00:04, 13.28it/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 72/130 [00:05<00:04, 13.41it/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 74/130 [00:05<00:04, 13.40it/s]
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 76/130 [00:06<00:03, 13.53it/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 78/130 [00:06<00:03, 13.56it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 80/130 [00:06<00:03, 13.48it/s]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 82/130 [00:06<00:03, 13.33it/s]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 84/130 [00:06<00:03, 13.36it/s]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 86/130 [00:06<00:03, 13.30it/s]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 88/130 [00:06<00:03, 13.33it/s]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 90/130 [00:07<00:03, 13.25it/s]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 92/130 [00:07<00:02, 13.28it/s]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 94/130 [00:07<00:02, 13.13it/s]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 96/130 [00:07<00:02, 13.07it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 98/130 [00:07<00:02, 12.92it/s]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 100/130 [00:07<00:02, 12.86it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 102/130 [00:08<00:02, 12.80it/s]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 104/130 [00:08<00:02, 12.81it/s]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 106/130 [00:08<00:01, 12.71it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 108/130 [00:08<00:01, 12.77it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 110/130 [00:08<00:01, 12.27it/s]
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 112/130 [00:08<00:01, 11.61it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 114/130 [00:09<00:01, 11.11it/s]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 116/130 [00:09<00:01, 10.86it/s][codecarbon INFO @ 13:25:36] Energy consumed for RAM : 0.003904 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:25:36] Energy consumed for all GPUs : 0.032886 kWh. Total GPU Power : 157.00684757317103 W
[codecarbon INFO @ 13:25:36] Energy consumed for all CPUs : 0.009391 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:25:36] 0.046180 kWh of electricity used since the beginning.

 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 118/130 [00:09<00:01, 10.59it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 120/130 [00:09<00:00, 10.37it/s]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 122/130 [00:09<00:00, 10.35it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 124/130 [00:10<00:00, 10.28it/s]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 126/130 [00:10<00:00, 10.60it/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 128/130 [00:10<00:00, 11.23it/s]
{'eval_loss': 0.5885089635848999, 'eval_precision': 0.8017523537972616, 'eval_recall': 0.7808900042178466, 'eval_f1': 0.7891971166461512, 'eval_balanced accuracy': 0.7808900042178466, 'eval_runtime': 10.6194, 'eval_samples_per_second': 781.496, 'eval_steps_per_second': 12.242, 'epoch': 5.0}
                                                   
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2595/3114 [13:08<02:15,  3.82it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 130/130 [00:10<00:00, 12.47it/s]
                                                 Saving model checkpoint to model_output_albertv2\mgsd_trained\checkpoint-2595
Configuration saved in model_output_albertv2\mgsd_trained\checkpoint-2595\config.json
Model weights saved in model_output_albertv2\mgsd_trained\checkpoint-2595\model.safetensors
tokenizer config file saved in model_output_albertv2\mgsd_trained\checkpoint-2595\tokenizer_config.json
Special tokens file saved in model_output_albertv2\mgsd_trained\checkpoint-2595\special_tokens_map.json
Deleting older checkpoint [model_output_albertv2\mgsd_trained\checkpoint-2076] due to args.save_total_limit
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2642/3114 [13:22<02:15,  3.49it/s][codecarbon INFO @ 13:25:51] Energy consumed for RAM : 0.003977 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:25:51] Energy consumed for all GPUs : 0.033511 kWh. Total GPU Power : 149.89324527786482 W
[codecarbon INFO @ 13:25:51] Energy consumed for all CPUs : 0.009568 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:25:51] 0.047056 kWh of electricity used since the beginning.
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2694/3114 [13:37<02:00,  3.48it/s][codecarbon INFO @ 13:26:06] Energy consumed for RAM : 0.004051 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:26:06] Energy consumed for all GPUs : 0.034140 kWh. Total GPU Power : 150.91391927706826 W
[codecarbon INFO @ 13:26:06] Energy consumed for all CPUs : 0.009745 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:26:06] 0.047936 kWh of electricity used since the beginning.
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2747/3114 [13:52<01:44,  3.53it/s][codecarbon INFO @ 13:26:21] Energy consumed for RAM : 0.004125 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:26:21] Energy consumed for all GPUs : 0.034773 kWh. Total GPU Power : 151.8232042242794 W
[codecarbon INFO @ 13:26:21] Energy consumed for all CPUs : 0.009922 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:26:21] 0.048819 kWh of electricity used since the beginning.
[codecarbon INFO @ 13:26:21] 0.014003 g.CO2eq/s mean an estimation of 441.58305606654005 kg.CO2eq/year
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2801/3114 [14:07<01:28,  3.52it/s][codecarbon INFO @ 13:26:36] Energy consumed for RAM : 0.004198 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:26:36] Energy consumed for all GPUs : 0.035407 kWh. Total GPU Power : 152.27755148569727 W
[codecarbon INFO @ 13:26:36] Energy consumed for all CPUs : 0.010099 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:26:36] 0.049705 kWh of electricity used since the beginning.
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2854/3114 [14:22<01:11,  3.62it/s][codecarbon INFO @ 13:26:51] Energy consumed for RAM : 0.004272 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:26:51] Energy consumed for all GPUs : 0.036041 kWh. Total GPU Power : 152.09206402630315 W
[codecarbon INFO @ 13:26:51] Energy consumed for all CPUs : 0.010276 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:26:51] 0.050589 kWh of electricity used since the beginning.
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2907/3114 [14:37<00:57,  3.60it/s][codecarbon INFO @ 13:27:06] Energy consumed for RAM : 0.004345 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:27:06] Energy consumed for all GPUs : 0.036676 kWh. Total GPU Power : 152.40521638440296 W
[codecarbon INFO @ 13:27:06] Energy consumed for all CPUs : 0.010453 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:27:06] 0.051475 kWh of electricity used since the beginning.
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2960/3114 [14:52<00:43,  3.50it/s][codecarbon INFO @ 13:27:21] Energy consumed for RAM : 0.004419 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:27:21] Energy consumed for all GPUs : 0.037311 kWh. Total GPU Power : 152.14565308106828 W
[codecarbon INFO @ 13:27:21] Energy consumed for all CPUs : 0.010631 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:27:21] 0.052360 kWh of electricity used since the beginning.
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3000/3114 [15:03<00:33,  3.44it/s]{'loss': 0.0692, 'grad_norm': 31.06598472595215, 'learning_rate': 7.321772639691716e-07, 'epoch': 5.78}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3013/3114 [15:07<00:29,  3.43it/s][codecarbon INFO @ 13:27:36] Energy consumed for RAM : 0.004493 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:27:36] Energy consumed for all GPUs : 0.037948 kWh. Total GPU Power : 152.8648575696891 W
[codecarbon INFO @ 13:27:36] Energy consumed for all CPUs : 0.010808 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:27:36] 0.053248 kWh of electricity used since the beginning.
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3066/3114 [15:22<00:13,  3.49it/s][codecarbon INFO @ 13:27:51] Energy consumed for RAM : 0.004566 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:27:51] Energy consumed for all GPUs : 0.038582 kWh. Total GPU Power : 152.13788955972694 W
[codecarbon INFO @ 13:27:51] Energy consumed for all CPUs : 0.010985 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:27:51] 0.054133 kWh of electricity used since the beginning.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3114/3114 [15:36<00:00,  3.87it/s]Saving model checkpoint to model_output_albertv2\mgsd_trained\checkpoint-3114
Configuration saved in model_output_albertv2\mgsd_trained\checkpoint-3114\config.json
Model weights saved in model_output_albertv2\mgsd_trained\checkpoint-3114\model.safetensors
tokenizer config file saved in model_output_albertv2\mgsd_trained\checkpoint-3114\tokenizer_config.json
Special tokens file saved in model_output_albertv2\mgsd_trained\checkpoint-3114\special_tokens_map.json
Deleting older checkpoint [model_output_albertv2\mgsd_trained\checkpoint-2595] due to args.save_total_limit
The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 8299
  Batch size = 64

  0%|          | 0/130 [00:00<?, ?it/s]
  2%|â–         | 3/130 [00:00<00:06, 19.03it/s]
  4%|â–         | 5/130 [00:00<00:07, 16.33it/s]
  5%|â–Œ         | 7/130 [00:00<00:08, 15.24it/s]
  7%|â–‹         | 9/130 [00:00<00:08, 14.83it/s]
  8%|â–Š         | 11/130 [00:00<00:08, 14.53it/s]
 10%|â–ˆ         | 13/130 [00:00<00:08, 14.38it/s]
 12%|â–ˆâ–        | 15/130 [00:01<00:08, 14.17it/s]
 13%|â–ˆâ–Ž        | 17/130 [00:01<00:08, 13.21it/s]
 15%|â–ˆâ–        | 19/130 [00:01<00:08, 12.53it/s][codecarbon INFO @ 13:28:06] Energy consumed for RAM : 0.004640 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:28:06] Energy consumed for all GPUs : 0.039214 kWh. Total GPU Power : 151.73209794975293 W
[codecarbon INFO @ 13:28:06] Energy consumed for all CPUs : 0.011162 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:28:06] 0.055016 kWh of electricity used since the beginning.

 16%|â–ˆâ–Œ        | 21/130 [00:01<00:08, 12.21it/s]
 18%|â–ˆâ–Š        | 23/130 [00:01<00:09, 11.75it/s]
 19%|â–ˆâ–‰        | 25/130 [00:01<00:08, 11.68it/s]
 21%|â–ˆâ–ˆ        | 27/130 [00:02<00:08, 11.55it/s]
 22%|â–ˆâ–ˆâ–       | 29/130 [00:02<00:08, 11.54it/s]
 24%|â–ˆâ–ˆâ–       | 31/130 [00:02<00:08, 11.41it/s]
 25%|â–ˆâ–ˆâ–Œ       | 33/130 [00:02<00:08, 11.42it/s]
 27%|â–ˆâ–ˆâ–‹       | 35/130 [00:02<00:08, 11.32it/s]
 28%|â–ˆâ–ˆâ–Š       | 37/130 [00:02<00:08, 11.34it/s]
 30%|â–ˆâ–ˆâ–ˆ       | 39/130 [00:03<00:08, 11.25it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 41/130 [00:03<00:07, 11.27it/s]
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 43/130 [00:03<00:07, 11.22it/s]
 35%|â–ˆâ–ˆâ–ˆâ–      | 45/130 [00:03<00:07, 11.27it/s]
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 47/130 [00:03<00:07, 11.23it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 49/130 [00:04<00:06, 11.60it/s]
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 51/130 [00:04<00:06, 11.80it/s]
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 53/130 [00:04<00:06, 12.04it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 55/130 [00:04<00:06, 12.09it/s]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 57/130 [00:04<00:05, 12.25it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 59/130 [00:04<00:05, 12.26it/s]
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 61/130 [00:04<00:05, 12.34it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 63/130 [00:05<00:05, 12.28it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 65/130 [00:05<00:05, 12.70it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 67/130 [00:05<00:04, 12.95it/s]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 69/130 [00:05<00:04, 13.20it/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 71/130 [00:05<00:04, 13.30it/s]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 73/130 [00:05<00:04, 13.44it/s]
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 75/130 [00:06<00:04, 13.49it/s]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 77/130 [00:06<00:03, 13.61it/s]
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 79/130 [00:06<00:03, 13.53it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 81/130 [00:06<00:03, 13.48it/s]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 83/130 [00:06<00:03, 13.34it/s]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 85/130 [00:06<00:03, 13.37it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 87/130 [00:06<00:03, 13.33it/s]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 89/130 [00:07<00:03, 13.37it/s]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 91/130 [00:07<00:02, 13.26it/s]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 93/130 [00:07<00:02, 13.30it/s]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 95/130 [00:07<00:02, 13.07it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 97/130 [00:07<00:02, 13.02it/s]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 99/130 [00:07<00:02, 12.85it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 101/130 [00:07<00:02, 12.85it/s]
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 103/130 [00:08<00:02, 12.69it/s]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 105/130 [00:08<00:01, 12.76it/s]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 107/130 [00:08<00:01, 12.68it/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 109/130 [00:08<00:01, 12.71it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 111/130 [00:08<00:01, 11.78it/s]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 113/130 [00:09<00:01, 11.28it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 115/130 [00:09<00:01, 10.90it/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 117/130 [00:09<00:01, 10.70it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 119/130 [00:09<00:01, 10.50it/s]
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 121/130 [00:09<00:00, 10.42it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 123/130 [00:10<00:00, 10.30it/s]
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 125/130 [00:10<00:00, 10.32it/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 127/130 [00:10<00:00, 11.02it/s]
                                                   
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3114/3114 [15:46<00:00,  3.87it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 130/130 [00:10<00:00, 11.67it/s]
                                                 Saving model checkpoint to model_output_albertv2\mgsd_trained\checkpoint-3114
Configuration saved in model_output_albertv2\mgsd_trained\checkpoint-3114\config.json
{'eval_loss': 0.7173662185668945, 'eval_precision': 0.7939672038372705, 'eval_recall': 0.7878942979342839, 'eval_f1': 0.7907171934203504, 'eval_balanced accuracy': 0.7878942979342839, 'eval_runtime': 10.6277, 'eval_samples_per_second': 780.886, 'eval_steps_per_second': 12.232, 'epoch': 6.0}
Model weights saved in model_output_albertv2\mgsd_trained\checkpoint-3114\model.safetensors
tokenizer config file saved in model_output_albertv2\mgsd_trained\checkpoint-3114\tokenizer_config.json
Special tokens file saved in model_output_albertv2\mgsd_trained\checkpoint-3114\special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from model_output_albertv2\mgsd_trained\checkpoint-1038 (score: 0.3997514545917511).
{'train_runtime': 946.9792, 'train_samples_per_second': 210.315, 'train_steps_per_second': 3.288, 'train_loss': 0.2570854858013783, 'epoch': 6.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3114/3114 [15:46<00:00,  3.87it/s]Deleting older checkpoint [model_output_albertv2\mgsd_trained\checkpoint-3114] due to args.save_total_limit
[codecarbon WARNING @ 13:28:15] Another instance of codecarbon is already running. Exiting.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3114/3114 [15:46<00:00,  3.29it/s]
Saving model checkpoint to model_output_albertv2\mgsd_trained
Configuration saved in model_output_albertv2\mgsd_trained\config.json
Model weights saved in model_output_albertv2\mgsd_trained\model.safetensors
tokenizer config file saved in model_output_albertv2\mgsd_trained\tokenizer_config.json
Special tokens file saved in model_output_albertv2\mgsd_trained\special_tokens_map.json
[codecarbon INFO @ 13:28:15] Energy consumed for RAM : 0.004686 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:28:15] Energy consumed for all GPUs : 0.039629 kWh. Total GPU Power : 160.18171576076958 W
[codecarbon INFO @ 13:28:15] Energy consumed for all CPUs : 0.011272 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:28:15] 0.055587 kWh of electricity used since the beginning.
[codecarbon INFO @ 13:28:15] 0.014060 g.CO2eq/s mean an estimation of 443.3909056433508 kg.CO2eq/year
loading configuration file model_output_albertv2/mgsd_trained\config.json
Model config AlbertConfig {
  "_name_or_path": "model_output_albertv2/mgsd_trained",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

loading weights file model_output_albertv2/mgsd_trained\model.safetensors
Estimated total emissions: 0.013206965019006428 kg CO2
Number of unique labels: 2
All model checkpoint weights were used when initializing AlbertForSequenceClassification.

All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2/mgsd_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
loading file spiece.model
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 653/653 [00:00<00:00, 61944.60 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 653/653 [00:00<00:00, 21742.50 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Disabling tokenizer parallelism, we're using DataLoader multithreading already
Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3236, 'input_ids': [2, 127, 148, 72, 50, 3398, 50, 19196, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
loading configuration file model_output_albertv2/mgsd_trained\config.json
Model config AlbertConfig {
  "_name_or_path": "model_output_albertv2/mgsd_trained",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

loading weights file model_output_albertv2/mgsd_trained\model.safetensors
Number of unique labels: 2
All model checkpoint weights were used when initializing AlbertForSequenceClassification.

All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2/mgsd_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
loading file spiece.model
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 414/414 [00:00<00:00, 14917.37 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 414/414 [00:00<00:00, 15916.93 examples/s]
Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 840, 'input_ids': [2, 14, 16330, 8790, 46, 14348, 28, 367, 26692, 108, 1427, 34, 109, 3, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'labels': 0}
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
loading configuration file model_output_albertv2/mgsd_trained\config.json
Model config AlbertConfig {
  "_name_or_path": "model_output_albertv2/mgsd_trained",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

loading weights file model_output_albertv2/mgsd_trained\model.safetensors
Number of unique labels: 2
All model checkpoint weights were used when initializing AlbertForSequenceClassification.

All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2/mgsd_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
loading file spiece.model
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10374/10374 [00:00<00:00, 17214.69 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10374/10374 [00:00<00:00, 18562.95 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Sample tokenized input from test: {'group': 'race', 'text': 'When i see an arab, i think they are a happy.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 17851, 'input_ids': [2, 76, 31, 196, 40, 3666, 15, 31, 277, 59, 50, 21, 1700, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
Number of unique labels: 2
loading configuration file model_output_albertv2/mgsd_trained\config.json
Model config AlbertConfig {
  "_name_or_path": "model_output_albertv2/mgsd_trained",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

loading weights file model_output_albertv2/mgsd_trained\model.safetensors
All model checkpoint weights were used when initializing AlbertForSequenceClassification.

All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2/mgsd_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
loading file spiece.model
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11441/11441 [00:00<00:00, 19631.26 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11441/11441 [00:00<00:00, 21140.00 examples/s]
Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [2, 14, 16330, 8790, 46, 14348, 28, 367, 26692, 108, 1427, 34, 109, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Number of unique labels: 2
[codecarbon INFO @ 13:36:18] [setup] RAM Tracking...
[codecarbon INFO @ 13:36:18] [setup] GPU Tracking...
[codecarbon INFO @ 13:36:18] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 13:36:18] [setup] CPU Tracking...
[codecarbon WARNING @ 13:36:18] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon WARNING @ 13:36:20] We saw that you have a AMD Ryzen 7 7700 8-Core Processor but we don't know it. Please contact us.
[codecarbon INFO @ 13:36:20] CPU Model on constant consumption mode: AMD Ryzen 7 7700 8-Core Processor
[codecarbon INFO @ 13:36:20] >>> Tracker's metadata:
[codecarbon INFO @ 13:36:20]   Platform system: Windows-11-10.0.26100-SP0
[codecarbon INFO @ 13:36:20]   Python version: 3.12.12
[codecarbon INFO @ 13:36:20]   CodeCarbon version: 2.8.0
[codecarbon INFO @ 13:36:20]   Available RAM : 47.116 GB
[codecarbon INFO @ 13:36:20]   CPU count: 16
[codecarbon INFO @ 13:36:20]   CPU model: AMD Ryzen 7 7700 8-Core Processor
[codecarbon INFO @ 13:36:20]   GPU count: 1
[codecarbon INFO @ 13:36:20]   GPU model: 1 x NVIDIA GeForce RTX 5060 Ti
[codecarbon INFO @ 13:36:24] Saving emissions data to file D:\UCL\HEARTS-Text-Stereotype-Detection-main\Model Training and Evaluation\emissions.csv
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--albert--albert-base-v2\snapshots\8e2f239c5f8a2c0f253781ca60135db913e5c80c\config.json
Model config AlbertConfig {
  "_name_or_path": "albert/albert-base-v2",
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

loading weights file model.safetensors from cache at C:\Users\32618\.cache\huggingface\hub\models--albert--albert-base-v2\snapshots\8e2f239c5f8a2c0f253781ca60135db913e5c80c\model.safetensors
Some weights of the model checkpoint at albert/albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--albert--albert-base-v2\snapshots\8e2f239c5f8a2c0f253781ca60135db913e5c80c\config.json
Model config AlbertConfig {
  "_name_or_path": "albert/albert-base-v2",
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

loading file spiece.model from cache at C:\Users\32618\.cache\huggingface\hub\models--albert--albert-base-v2\snapshots\8e2f239c5f8a2c0f253781ca60135db913e5c80c\spiece.model
loading file tokenizer.json from cache at C:\Users\32618\.cache\huggingface\hub\models--albert--albert-base-v2\snapshots\8e2f239c5f8a2c0f253781ca60135db913e5c80c\tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--albert--albert-base-v2\snapshots\8e2f239c5f8a2c0f253781ca60135db913e5c80c\tokenizer_config.json
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--albert--albert-base-v2\snapshots\8e2f239c5f8a2c0f253781ca60135db913e5c80c\config.json
Model config AlbertConfig {
  "_name_or_path": "albert/albert-base-v2",
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2088/2088 [00:00<00:00, 28595.94 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2088/2088 [00:00<00:00, 22939.63 examples/s]
Map:   0%|          | 0/523 [00:00<?, ? examples/s]Sample tokenized input from train: {'text': 'we expect that nonbinary people are out to share their views.', 'label': 0, 'group': 'NB', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 1788, 'input_ids': [2, 95, 4186, 30, 538, 4772, 1857, 148, 50, 70, 20, 1891, 66, 4146, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 523/523 [00:00<00:00, 23733.55 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 523/523 [00:00<00:00, 20883.87 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
PyTorch: setting up devices
Sample tokenized input from validation: {'text': 'we expect that nonbinary people are out to share their views.', 'label': 0, 'group': 'NB', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 1788, 'input_ids': [2, 95, 4186, 30, 538, 4772, 1857, 148, 50, 70, 20, 1891, 66, 4146, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
D:\UCL\HEARTS-Text-Stereotype-Detection-main\Model Training and Evaluation\BERT_Models_Fine_Tuning.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
[codecarbon ERROR @ 13:36:25] Error: Another instance of codecarbon is probably running as we find `C:\Users\32618\AppData\Local\Temp\.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.
The following columns in the training set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running training *****
  Num examples = 2,088
  Num Epochs = 6
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 198
  Number of trainable parameters = 11,685,122
[codecarbon WARNING @ 13:36:25] Another instance of codecarbon is already running. Exiting.
 16%|â–ˆâ–Œ        | 32/198 [00:03<00:17,  9.39it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 523
  Batch size = 64

  0%|          | 0/9 [00:00<?, ?it/s]
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:00<00:00, 28.49it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:00<00:00, 22.84it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 24.99it/s]
                                                
 17%|â–ˆâ–‹        | 33/198 [00:03<00:17,  9.39it/s]
                                             Saving model checkpoint to model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-33
Configuration saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-33\config.json
{'eval_loss': 0.32882454991340637, 'eval_precision': 0.8834205654062521, 'eval_recall': 0.8646097850008465, 'eval_f1': 0.8730362406797295, 'eval_balanced accuracy': 0.8646097850008465, 'eval_runtime': 0.4194, 'eval_samples_per_second': 1246.926, 'eval_steps_per_second': 21.458, 'epoch': 1.0}
Model weights saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-33\model.safetensors
tokenizer config file saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-33\tokenizer_config.json
Special tokens file saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-33\special_tokens_map.json
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/198 [00:07<00:14,  9.07it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 523
  Batch size = 64

  0%|          | 0/9 [00:00<?, ?it/s]
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:00<00:00, 26.79it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:00<00:00, 14.38it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 18.36it/s]
                                                
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/198 [00:08<00:14,  9.07it/s]
                                             Saving model checkpoint to model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-66
Configuration saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-66\config.json
{'eval_loss': 0.2026868462562561, 'eval_precision': 0.9435466138962181, 'eval_recall': 0.9098019299136617, 'eval_f1': 0.9242138820460803, 'eval_balanced accuracy': 0.9098019299136617, 'eval_runtime': 0.5644, 'eval_samples_per_second': 926.575, 'eval_steps_per_second': 15.945, 'epoch': 2.0}
Model weights saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-66\model.safetensors
tokenizer config file saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-66\tokenizer_config.json
Special tokens file saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-66\special_tokens_map.json
Deleting older checkpoint [model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-33] due to args.save_total_limit
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/198 [00:11<00:10,  9.35it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 523
  Batch size = 64

  0%|          | 0/9 [00:00<?, ?it/s]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:00<00:00, 25.77it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:00<00:00, 22.32it/s]
{'eval_loss': 0.09679010510444641, 'eval_precision': 0.9768076989769378, 'eval_recall': 0.9655070255628915, 'eval_f1': 0.9708858428717633, 'eval_balanced accuracy': 0.9655070255628915, 'eval_runtime': 0.4177, 'eval_samples_per_second': 1252.012, 'eval_steps_per_second': 21.545, 'epoch': 3.0}
                                                
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 99/198 [00:12<00:10,  9.35it/s]
                                             Saving model checkpoint to model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-99
Configuration saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-99\config.json
Model weights saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-99\model.safetensors
tokenizer config file saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-99\tokenizer_config.json
Special tokens file saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-99\special_tokens_map.json
Deleting older checkpoint [model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-66] due to args.save_total_limit
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/198 [00:13<00:09,  9.02it/s][codecarbon INFO @ 13:36:39] Energy consumed for RAM : 0.000074 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:36:39] Energy consumed for all GPUs : 0.000553 kWh. Total GPU Power : 132.60810318733087 W
[codecarbon INFO @ 13:36:39] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:36:39] 0.000804 kWh of electricity used since the beginning.
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/198 [00:15<00:07,  9.38it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 523
  Batch size = 64

  0%|          | 0/9 [00:00<?, ?it/s]
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:00<00:00, 29.77it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:00<00:00, 22.57it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 24.49it/s]
                                                 
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 132/198 [00:16<00:07,  9.38it/s]
                                             Saving model checkpoint to model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-132
Configuration saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-132\config.json
{'eval_loss': 0.0725172907114029, 'eval_precision': 0.9779590301289285, 'eval_recall': 0.9732012866091078, 'eval_f1': 0.975530914921761, 'eval_balanced accuracy': 0.9732012866091078, 'eval_runtime': 0.4206, 'eval_samples_per_second': 1243.358, 'eval_steps_per_second': 21.396, 'epoch': 4.0}
Model weights saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-132\model.safetensors
tokenizer config file saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-132\tokenizer_config.json
Special tokens file saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-132\special_tokens_map.json
Deleting older checkpoint [model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-99] due to args.save_total_limit
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 164/198 [00:19<00:03,  9.36it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 523
  Batch size = 64

  0%|          | 0/9 [00:00<?, ?it/s]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:00<00:00, 25.97it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:00<00:00, 22.77it/s]
                                                 
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/198 [00:20<00:03,  9.36it/s]
                                             Saving model checkpoint to model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-165
Configuration saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-165\config.json
{'eval_loss': 0.05597148463129997, 'eval_precision': 0.9855015270580969, 'eval_recall': 0.9790248857287963, 'eval_f1': 0.9821745057941378, 'eval_balanced accuracy': 0.9790248857287963, 'eval_runtime': 0.4134, 'eval_samples_per_second': 1265.192, 'eval_steps_per_second': 21.772, 'epoch': 5.0}
Model weights saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-165\model.safetensors
tokenizer config file saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-165\tokenizer_config.json
Special tokens file saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-165\special_tokens_map.json
Deleting older checkpoint [model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-132] due to args.save_total_limit
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 197/198 [00:24<00:00,  9.20it/s]Saving model checkpoint to model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-198
Configuration saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-198\config.json
Model weights saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-198\model.safetensors
tokenizer config file saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-198\tokenizer_config.json
Special tokens file saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-198\special_tokens_map.json
The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 523
  Batch size = 64

  0%|          | 0/9 [00:00<?, ?it/s]
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:00<00:00, 27.42it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:00<00:00, 22.12it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 24.54it/s]
                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:24<00:00,  9.20it/s]
                                             Saving model checkpoint to model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-198
Configuration saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-198\config.json
{'eval_loss': 0.04858456552028656, 'eval_precision': 0.9793967280163599, 'eval_recall': 0.9762315896394109, 'eval_f1': 0.9777923092601399, 'eval_balanced accuracy': 0.9762315896394109, 'eval_runtime': 0.4364, 'eval_samples_per_second': 1198.516, 'eval_steps_per_second': 20.625, 'epoch': 6.0}
Model weights saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-198\model.safetensors
tokenizer config file saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-198\tokenizer_config.json
Special tokens file saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-198\special_tokens_map.json
Deleting older checkpoint [model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-165] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from model_output_albertv2\winoqueer_gpt_augmentation_trained\checkpoint-198 (score: 0.04858456552028656).
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:25<00:00,  9.20it/s][codecarbon WARNING @ 13:36:50] Another instance of codecarbon is already running. Exiting.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:25<00:00,  7.88it/s]
Saving model checkpoint to model_output_albertv2\winoqueer_gpt_augmentation_trained
Configuration saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\config.json
{'train_runtime': 25.1142, 'train_samples_per_second': 498.842, 'train_steps_per_second': 7.884, 'train_loss': 0.16284888681739268, 'epoch': 6.0}
Model weights saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\model.safetensors
tokenizer config file saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\tokenizer_config.json
Special tokens file saved in model_output_albertv2\winoqueer_gpt_augmentation_trained\special_tokens_map.json
[codecarbon INFO @ 13:36:50] Energy consumed for RAM : 0.000129 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:36:50] Energy consumed for all GPUs : 0.001004 kWh. Total GPU Power : 143.1563313286831 W
[codecarbon INFO @ 13:36:50] Energy consumed for all CPUs : 0.000311 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:36:50] 0.001444 kWh of electricity used since the beginning.
Estimated total emissions: 0.0003431031582100386 kg CO2
Number of unique labels: 2
loading configuration file model_output_albertv2/winoqueer_gpt_augmentation_trained\config.json
Model config AlbertConfig {
  "_name_or_path": "model_output_albertv2/winoqueer_gpt_augmentation_trained",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

loading weights file model_output_albertv2/winoqueer_gpt_augmentation_trained\model.safetensors
All model checkpoint weights were used when initializing AlbertForSequenceClassification.

All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2/winoqueer_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
loading file spiece.model
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 653/653 [00:00<00:00, 20988.88 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 653/653 [00:00<00:00, 18646.05 examples/s]
Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3236, 'input_ids': [2, 127, 148, 72, 50, 3398, 50, 19196, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Number of unique labels: 2
loading configuration file model_output_albertv2/winoqueer_gpt_augmentation_trained\config.json
Model config AlbertConfig {
  "_name_or_path": "model_output_albertv2/winoqueer_gpt_augmentation_trained",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

loading weights file model_output_albertv2/winoqueer_gpt_augmentation_trained\model.safetensors
All model checkpoint weights were used when initializing AlbertForSequenceClassification.

All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2/winoqueer_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
loading file spiece.model
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 414/414 [00:00<00:00, 16229.49 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 414/414 [00:00<00:00, 13846.89 examples/s]
Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 840, 'input_ids': [2, 14, 16330, 8790, 46, 14348, 28, 367, 26692, 108, 1427, 34, 109, 3, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'labels': 0}
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Number of unique labels: 2
loading configuration file model_output_albertv2/winoqueer_gpt_augmentation_trained\config.json
Model config AlbertConfig {
  "_name_or_path": "model_output_albertv2/winoqueer_gpt_augmentation_trained",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

loading weights file model_output_albertv2/winoqueer_gpt_augmentation_trained\model.safetensors
All model checkpoint weights were used when initializing AlbertForSequenceClassification.

All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2/winoqueer_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
loading file spiece.model
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10374/10374 [00:00<00:00, 18662.27 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10374/10374 [00:00<00:00, 18453.60 examples/s]
Sample tokenized input from test: {'group': 'race', 'text': 'When i see an arab, i think they are a happy.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 17851, 'input_ids': [2, 76, 31, 196, 40, 3666, 15, 31, 277, 59, 50, 21, 1700, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
loading configuration file model_output_albertv2/winoqueer_gpt_augmentation_trained\config.json
Model config AlbertConfig {
  "_name_or_path": "model_output_albertv2/winoqueer_gpt_augmentation_trained",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

loading weights file model_output_albertv2/winoqueer_gpt_augmentation_trained\model.safetensors
Number of unique labels: 2
All model checkpoint weights were used when initializing AlbertForSequenceClassification.

All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2/winoqueer_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
loading file spiece.model
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11441/11441 [00:00<00:00, 20031.45 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11441/11441 [00:00<00:00, 21071.90 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [2, 14, 16330, 8790, 46, 14348, 28, 367, 26692, 108, 1427, 34, 109, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
Number of unique labels: 2
[codecarbon INFO @ 13:44:48] [setup] RAM Tracking...
[codecarbon INFO @ 13:44:48] [setup] GPU Tracking...
[codecarbon INFO @ 13:44:48] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 13:44:48] [setup] CPU Tracking...
[codecarbon WARNING @ 13:44:48] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon WARNING @ 13:44:50] We saw that you have a AMD Ryzen 7 7700 8-Core Processor but we don't know it. Please contact us.
[codecarbon INFO @ 13:44:50] CPU Model on constant consumption mode: AMD Ryzen 7 7700 8-Core Processor
[codecarbon INFO @ 13:44:50] >>> Tracker's metadata:
[codecarbon INFO @ 13:44:50]   Platform system: Windows-11-10.0.26100-SP0
[codecarbon INFO @ 13:44:50]   Python version: 3.12.12
[codecarbon INFO @ 13:44:50]   CodeCarbon version: 2.8.0
[codecarbon INFO @ 13:44:50]   Available RAM : 47.116 GB
[codecarbon INFO @ 13:44:50]   CPU count: 16
[codecarbon INFO @ 13:44:50]   CPU model: AMD Ryzen 7 7700 8-Core Processor
[codecarbon INFO @ 13:44:50]   GPU count: 1
[codecarbon INFO @ 13:44:50]   GPU model: 1 x NVIDIA GeForce RTX 5060 Ti
[codecarbon INFO @ 13:44:54] Saving emissions data to file D:\UCL\HEARTS-Text-Stereotype-Detection-main\Model Training and Evaluation\emissions.csv
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--albert--albert-base-v2\snapshots\8e2f239c5f8a2c0f253781ca60135db913e5c80c\config.json
Model config AlbertConfig {
  "_name_or_path": "albert/albert-base-v2",
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

loading weights file model.safetensors from cache at C:\Users\32618\.cache\huggingface\hub\models--albert--albert-base-v2\snapshots\8e2f239c5f8a2c0f253781ca60135db913e5c80c\model.safetensors
Some weights of the model checkpoint at albert/albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--albert--albert-base-v2\snapshots\8e2f239c5f8a2c0f253781ca60135db913e5c80c\config.json
Model config AlbertConfig {
  "_name_or_path": "albert/albert-base-v2",
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

loading file spiece.model from cache at C:\Users\32618\.cache\huggingface\hub\models--albert--albert-base-v2\snapshots\8e2f239c5f8a2c0f253781ca60135db913e5c80c\spiece.model
loading file tokenizer.json from cache at C:\Users\32618\.cache\huggingface\hub\models--albert--albert-base-v2\snapshots\8e2f239c5f8a2c0f253781ca60135db913e5c80c\tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--albert--albert-base-v2\snapshots\8e2f239c5f8a2c0f253781ca60135db913e5c80c\tokenizer_config.json
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--albert--albert-base-v2\snapshots\8e2f239c5f8a2c0f253781ca60135db913e5c80c\config.json
Model config AlbertConfig {
  "_name_or_path": "albert/albert-base-v2",
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1324/1324 [00:00<00:00, 22169.94 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1324/1324 [00:00<00:00, 16944.82 examples/s]
Map:   0%|          | 0/332 [00:00<?, ? examples/s]Sample tokenized input from train: {'text': 'The average Chadian income is modest compared to developed countries', 'label': 0, 'group': 'Chadian', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 1249, 'input_ids': [2, 14, 862, 8708, 806, 1587, 25, 8904, 2428, 20, 885, 1166, 3, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 332/332 [00:00<00:00, 15072.73 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 332/332 [00:00<00:00, 19440.30 examples/s]
Sample tokenized input from validation: {'text': 'The average Chadian income is modest compared to developed countries', 'label': 0, 'group': 'Chadian', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 1249, 'input_ids': [2, 14, 862, 8708, 806, 1587, 25, 8904, 2428, 20, 885, 1166, 3, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
D:\UCL\HEARTS-Text-Stereotype-Detection-main\Model Training and Evaluation\BERT_Models_Fine_Tuning.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
[codecarbon ERROR @ 13:44:55] Error: Another instance of codecarbon is probably running as we find `C:\Users\32618\AppData\Local\Temp\.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.
The following columns in the training set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running training *****
  Num examples = 1,324
  Num Epochs = 6
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 126
  Number of trainable parameters = 11,685,122
[codecarbon WARNING @ 13:44:55] Another instance of codecarbon is already running. Exiting.
 17%|â–ˆâ–‹        | 21/126 [00:02<00:08, 11.94it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 332
  Batch size = 64

  0%|          | 0/6 [00:00<?, ?it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:00<00:00, 42.40it/s]
{'eval_loss': 0.38720178604125977, 'eval_precision': 0.8391313884623821, 'eval_recall': 0.8151318739554033, 'eval_f1': 0.8249831309041835, 'eval_balanced accuracy': 0.8151318739554033, 'eval_runtime': 0.1615, 'eval_samples_per_second': 2056.204, 'eval_steps_per_second': 37.16, 'epoch': 1.0}
                                                
 17%|â–ˆâ–‹        | 21/126 [00:02<00:08, 11.94it/s]
                                             Saving model checkpoint to model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-21
Configuration saved in model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-21\config.json
Model weights saved in model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-21\model.safetensors
tokenizer config file saved in model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-21\tokenizer_config.json
Special tokens file saved in model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-21\special_tokens_map.json
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 41/126 [00:04<00:07, 11.38it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 332
  Batch size = 64

  0%|          | 0/6 [00:00<?, ?it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:00<00:00, 42.64it/s]
                                                
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 42/126 [00:04<00:07, 11.38it/s]
                                             Saving model checkpoint to model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-42
Configuration saved in model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-42\config.json
{'eval_loss': 0.3659713566303253, 'eval_precision': 0.8915216586449464, 'eval_recall': 0.8017610370551547, 'eval_f1': 0.8270833333333334, 'eval_balanced accuracy': 0.8017610370551547, 'eval_runtime': 0.1605, 'eval_samples_per_second': 2068.796, 'eval_steps_per_second': 37.388, 'epoch': 2.0}
Model weights saved in model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-42\model.safetensors
tokenizer config file saved in model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-42\tokenizer_config.json
Special tokens file saved in model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-42\special_tokens_map.json
Deleting older checkpoint [model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-21] due to args.save_total_limit
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 63/126 [00:06<00:05, 11.94it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 332
  Batch size = 64

  0%|          | 0/6 [00:00<?, ?it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:00<00:00, 42.36it/s]
                                                
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 63/126 [00:06<00:05, 11.94it/s]
                                             Saving model checkpoint to model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-63
Configuration saved in model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-63\config.json
{'eval_loss': 0.23933503031730652, 'eval_precision': 0.9021035598705502, 'eval_recall': 0.9254616607557784, 'eval_f1': 0.9114220496249963, 'eval_balanced accuracy': 0.9254616607557784, 'eval_runtime': 0.161, 'eval_samples_per_second': 2061.626, 'eval_steps_per_second': 37.258, 'epoch': 3.0}
Model weights saved in model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-63\model.safetensors
tokenizer config file saved in model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-63\tokenizer_config.json
Special tokens file saved in model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-63\special_tokens_map.json
Deleting older checkpoint [model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-42] due to args.save_total_limit
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 84/126 [00:08<00:03, 10.63it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 332
  Batch size = 64

  0%|          | 0/6 [00:00<?, ?it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:00<00:00, 42.70it/s]
{'eval_loss': 0.21955735981464386, 'eval_precision': 0.9102987421383648, 'eval_recall': 0.9255024255024256, 'eval_f1': 0.9170190856103096, 'eval_balanced accuracy': 0.9255024255024256, 'eval_runtime': 0.1608, 'eval_samples_per_second': 2064.202, 'eval_steps_per_second': 37.305, 'epoch': 4.0}
                                                
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 84/126 [00:08<00:03, 10.63it/s]
                                             Saving model checkpoint to model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-84
Configuration saved in model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-84\config.json
Model weights saved in model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-84\model.safetensors
tokenizer config file saved in model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-84\tokenizer_config.json
Special tokens file saved in model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-84\special_tokens_map.json
Deleting older checkpoint [model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-63] due to args.save_total_limit
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 104/126 [00:10<00:01, 11.31it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 332
  Batch size = 64

  0%|          | 0/6 [00:00<?, ?it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:00<00:00, 42.43it/s]
{'eval_loss': 0.22134937345981598, 'eval_precision': 0.9214350006429215, 'eval_recall': 0.9008193714076067, 'eval_f1': 0.9099181833361163, 'eval_balanced accuracy': 0.9008193714076067, 'eval_runtime': 0.1606, 'eval_samples_per_second': 2066.911, 'eval_steps_per_second': 37.354, 'epoch': 5.0}
                                                 
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 105/126 [00:10<00:01, 11.31it/s]
                                             Saving model checkpoint to model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-105
Configuration saved in model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-105\config.json
Model weights saved in model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-105\model.safetensors
tokenizer config file saved in model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-105\tokenizer_config.json
Special tokens file saved in model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-105\special_tokens_map.json
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:12<00:00, 11.89it/s]Saving model checkpoint to model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-126
Configuration saved in model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-126\config.json
Model weights saved in model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-126\model.safetensors
tokenizer config file saved in model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-126\tokenizer_config.json
Special tokens file saved in model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-126\special_tokens_map.json
Deleting older checkpoint [model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-105] due to args.save_total_limit
The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 332
  Batch size = 64

  0%|          | 0/6 [00:00<?, ?it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:00<00:00, 41.91it/s]
                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:13<00:00, 11.89it/s]
                                             Saving model checkpoint to model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-126
Configuration saved in model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-126\config.json
{'eval_loss': 0.22091789543628693, 'eval_precision': 0.9215662890081495, 'eval_recall': 0.9322897558191676, 'eval_f1': 0.9265250281667472, 'eval_balanced accuracy': 0.9322897558191676, 'eval_runtime': 0.1624, 'eval_samples_per_second': 2043.942, 'eval_steps_per_second': 36.939, 'epoch': 6.0}
Model weights saved in model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-126\model.safetensors
tokenizer config file saved in model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-126\tokenizer_config.json
Special tokens file saved in model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-126\special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-84 (score: 0.21955735981464386).
{'train_runtime': 13.2478, 'train_samples_per_second': 599.646, 'train_steps_per_second': 9.511, 'train_loss': 0.22002426026359437, 'epoch': 6.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:13<00:00, 11.89it/s]Deleting older checkpoint [model_output_albertv2\seegull_gpt_augmentation_trained\checkpoint-126] due to args.save_total_limit
[codecarbon WARNING @ 13:45:08] Another instance of codecarbon is already running. Exiting.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:13<00:00,  9.51it/s]
Saving model checkpoint to model_output_albertv2\seegull_gpt_augmentation_trained
Configuration saved in model_output_albertv2\seegull_gpt_augmentation_trained\config.json
Model weights saved in model_output_albertv2\seegull_gpt_augmentation_trained\model.safetensors
tokenizer config file saved in model_output_albertv2\seegull_gpt_augmentation_trained\tokenizer_config.json
Special tokens file saved in model_output_albertv2\seegull_gpt_augmentation_trained\special_tokens_map.json
[codecarbon INFO @ 13:45:08] Energy consumed for RAM : 0.000072 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:45:08] Energy consumed for all GPUs : 0.000521 kWh. Total GPU Power : 128.6583633414648 W
[codecarbon INFO @ 13:45:08] Energy consumed for all CPUs : 0.000172 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:45:08] 0.000764 kWh of electricity used since the beginning.
loading configuration file model_output_albertv2/seegull_gpt_augmentation_trained\config.json
Model config AlbertConfig {
  "_name_or_path": "model_output_albertv2/seegull_gpt_augmentation_trained",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

loading weights file model_output_albertv2/seegull_gpt_augmentation_trained\model.safetensors
Estimated total emissions: 0.00018157916561595023 kg CO2
Number of unique labels: 2
All model checkpoint weights were used when initializing AlbertForSequenceClassification.

All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2/seegull_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
loading file spiece.model
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 653/653 [00:00<00:00, 24113.05 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 653/653 [00:00<00:00, 21191.55 examples/s]
Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3236, 'input_ids': [2, 127, 148, 72, 50, 3398, 50, 19196, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
loading configuration file model_output_albertv2/seegull_gpt_augmentation_trained\config.json
Model config AlbertConfig {
  "_name_or_path": "model_output_albertv2/seegull_gpt_augmentation_trained",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

loading weights file model_output_albertv2/seegull_gpt_augmentation_trained\model.safetensors
Number of unique labels: 2
All model checkpoint weights were used when initializing AlbertForSequenceClassification.

All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2/seegull_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
loading file spiece.model
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 414/414 [00:00<00:00, 16526.84 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 414/414 [00:00<00:00, 16197.85 examples/s]
Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 840, 'input_ids': [2, 14, 16330, 8790, 46, 14348, 28, 367, 26692, 108, 1427, 34, 109, 3, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'labels': 0}
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
loading configuration file model_output_albertv2/seegull_gpt_augmentation_trained\config.json
Model config AlbertConfig {
  "_name_or_path": "model_output_albertv2/seegull_gpt_augmentation_trained",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

loading weights file model_output_albertv2/seegull_gpt_augmentation_trained\model.safetensors
Number of unique labels: 2
All model checkpoint weights were used when initializing AlbertForSequenceClassification.

All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2/seegull_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
loading file spiece.model
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10374/10374 [00:00<00:00, 18672.64 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10374/10374 [00:00<00:00, 19125.87 examples/s]
Sample tokenized input from test: {'group': 'race', 'text': 'When i see an arab, i think they are a happy.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 17851, 'input_ids': [2, 76, 31, 196, 40, 3666, 15, 31, 277, 59, 50, 21, 1700, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
loading configuration file model_output_albertv2/seegull_gpt_augmentation_trained\config.json
Model config AlbertConfig {
  "_name_or_path": "model_output_albertv2/seegull_gpt_augmentation_trained",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

loading weights file model_output_albertv2/seegull_gpt_augmentation_trained\model.safetensors
Number of unique labels: 2
All model checkpoint weights were used when initializing AlbertForSequenceClassification.

All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2/seegull_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
loading file spiece.model
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11441/11441 [00:00<00:00, 19801.46 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11441/11441 [00:00<00:00, 21427.43 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [2, 14, 16330, 8790, 46, 14348, 28, 367, 26692, 108, 1427, 34, 109, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
Number of unique labels: 2
[codecarbon INFO @ 13:53:09] [setup] RAM Tracking...
[codecarbon INFO @ 13:53:09] [setup] GPU Tracking...
[codecarbon INFO @ 13:53:09] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 13:53:09] [setup] CPU Tracking...
[codecarbon WARNING @ 13:53:09] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon WARNING @ 13:53:11] We saw that you have a AMD Ryzen 7 7700 8-Core Processor but we don't know it. Please contact us.
[codecarbon INFO @ 13:53:11] CPU Model on constant consumption mode: AMD Ryzen 7 7700 8-Core Processor
[codecarbon INFO @ 13:53:11] >>> Tracker's metadata:
[codecarbon INFO @ 13:53:11]   Platform system: Windows-11-10.0.26100-SP0
[codecarbon INFO @ 13:53:11]   Python version: 3.12.12
[codecarbon INFO @ 13:53:11]   CodeCarbon version: 2.8.0
[codecarbon INFO @ 13:53:11]   Available RAM : 47.116 GB
[codecarbon INFO @ 13:53:11]   CPU count: 16
[codecarbon INFO @ 13:53:11]   CPU model: AMD Ryzen 7 7700 8-Core Processor
[codecarbon INFO @ 13:53:11]   GPU count: 1
[codecarbon INFO @ 13:53:11]   GPU model: 1 x NVIDIA GeForce RTX 5060 Ti
[codecarbon INFO @ 13:53:14] Saving emissions data to file D:\UCL\HEARTS-Text-Stereotype-Detection-main\Model Training and Evaluation\emissions.csv
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--albert--albert-base-v2\snapshots\8e2f239c5f8a2c0f253781ca60135db913e5c80c\config.json
Model config AlbertConfig {
  "_name_or_path": "albert/albert-base-v2",
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

loading weights file model.safetensors from cache at C:\Users\32618\.cache\huggingface\hub\models--albert--albert-base-v2\snapshots\8e2f239c5f8a2c0f253781ca60135db913e5c80c\model.safetensors
Some weights of the model checkpoint at albert/albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--albert--albert-base-v2\snapshots\8e2f239c5f8a2c0f253781ca60135db913e5c80c\config.json
Model config AlbertConfig {
  "_name_or_path": "albert/albert-base-v2",
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

loading file spiece.model from cache at C:\Users\32618\.cache\huggingface\hub\models--albert--albert-base-v2\snapshots\8e2f239c5f8a2c0f253781ca60135db913e5c80c\spiece.model
loading file tokenizer.json from cache at C:\Users\32618\.cache\huggingface\hub\models--albert--albert-base-v2\snapshots\8e2f239c5f8a2c0f253781ca60135db913e5c80c\tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--albert--albert-base-v2\snapshots\8e2f239c5f8a2c0f253781ca60135db913e5c80c\tokenizer_config.json
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--albert--albert-base-v2\snapshots\8e2f239c5f8a2c0f253781ca60135db913e5c80c\config.json
Model config AlbertConfig {
  "_name_or_path": "albert/albert-base-v2",
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36608/36608 [00:02<00:00, 17236.36 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36608/36608 [00:02<00:00, 15343.78 examples/s]
Sample tokenized input from train: {'text': 'The commander could tell the soldier was disciplined.', 'label': 1, 'group': 'profession', 'data_name': 'MGSD', '__index_level_0__': 7380, 'input_ids': [2, 14, 1647, 110, 494, 14, 3994, 23, 7927, 43, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1}
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9152/9152 [00:00<00:00, 20101.97 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9152/9152 [00:00<00:00, 23339.09 examples/s]
Sample tokenized input from validation: {'text': 'The commander could tell the soldier was disciplined.', 'label': 1, 'group': 'profession', 'data_name': 'MGSD', '__index_level_0__': 7380, 'input_ids': [2, 14, 1647, 110, 494, 14, 3994, 23, 7927, 43, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1}
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
D:\UCL\HEARTS-Text-Stereotype-Detection-main\Model Training and Evaluation\BERT_Models_Fine_Tuning.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
[codecarbon ERROR @ 13:53:20] Error: Another instance of codecarbon is probably running as we find `C:\Users\32618\AppData\Local\Temp\.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.
The following columns in the training set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running training *****
  Num examples = 36,608
  Num Epochs = 6
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 3,432
  Number of trainable parameters = 11,685,122
[codecarbon WARNING @ 13:53:20] Another instance of codecarbon is already running. Exiting.
  1%|          | 34/3432 [00:08<14:26,  3.92it/s][codecarbon INFO @ 13:53:29] Energy consumed for RAM : 0.000074 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:53:29] Energy consumed for all GPUs : 0.000382 kWh. Total GPU Power : 91.63670514155594 W
[codecarbon INFO @ 13:53:29] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:53:29] 0.000633 kWh of electricity used since the beginning.
  3%|â–Ž         | 93/3432 [00:23<14:10,  3.93it/s][codecarbon INFO @ 13:53:44] Energy consumed for RAM : 0.000147 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:53:44] Energy consumed for all GPUs : 0.001020 kWh. Total GPU Power : 153.13836093214942 W
[codecarbon INFO @ 13:53:44] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:53:44] 0.001522 kWh of electricity used since the beginning.
  4%|â–         | 152/3432 [00:38<13:59,  3.91it/s][codecarbon INFO @ 13:53:59] Energy consumed for RAM : 0.000221 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:53:59] Energy consumed for all GPUs : 0.001665 kWh. Total GPU Power : 154.79681179049274 W
[codecarbon INFO @ 13:53:59] Energy consumed for all CPUs : 0.000531 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:53:59] 0.002418 kWh of electricity used since the beginning.
  6%|â–Œ         | 211/3432 [00:53<13:38,  3.93it/s][codecarbon INFO @ 13:54:14] Energy consumed for RAM : 0.000295 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:54:14] Energy consumed for all GPUs : 0.002312 kWh. Total GPU Power : 155.17485083488273 W
[codecarbon INFO @ 13:54:14] Energy consumed for all CPUs : 0.000708 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:54:14] 0.003315 kWh of electricity used since the beginning.
  8%|â–Š         | 270/3432 [01:08<13:28,  3.91it/s][codecarbon INFO @ 13:54:29] Energy consumed for RAM : 0.000368 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:54:29] Energy consumed for all GPUs : 0.002960 kWh. Total GPU Power : 155.4798563442388 W
[codecarbon INFO @ 13:54:29] Energy consumed for all CPUs : 0.000886 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:54:29] 0.004214 kWh of electricity used since the beginning.
 10%|â–‰         | 328/3432 [01:23<13:31,  3.82it/s][codecarbon INFO @ 13:54:44] Energy consumed for RAM : 0.000442 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:54:44] Energy consumed for all GPUs : 0.003609 kWh. Total GPU Power : 155.64012521826032 W
[codecarbon INFO @ 13:54:44] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:54:44] 0.005114 kWh of electricity used since the beginning.
 11%|â–ˆ         | 386/3432 [01:38<13:09,  3.86it/s][codecarbon INFO @ 13:54:59] Energy consumed for RAM : 0.000516 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:54:59] Energy consumed for all GPUs : 0.004255 kWh. Total GPU Power : 154.99863270166747 W
[codecarbon INFO @ 13:54:59] Energy consumed for all CPUs : 0.001240 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:54:59] 0.006011 kWh of electricity used since the beginning.
 13%|â–ˆâ–Ž        | 445/3432 [01:53<12:48,  3.89it/s][codecarbon INFO @ 13:55:14] Energy consumed for RAM : 0.000589 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:55:14] Energy consumed for all GPUs : 0.004904 kWh. Total GPU Power : 155.63539472344368 W
[codecarbon INFO @ 13:55:14] Energy consumed for all CPUs : 0.001417 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:55:14] 0.006910 kWh of electricity used since the beginning.
[codecarbon INFO @ 13:55:14] 0.013676 g.CO2eq/s mean an estimation of 431.29312849462525 kg.CO2eq/year
 15%|â–ˆâ–        | 500/3432 [02:08<12:34,  3.88it/s]{'loss': 0.473, 'grad_norm': 8.839498519897461, 'learning_rate': 1.7086247086247088e-05, 'epoch': 0.87}
 15%|â–ˆâ–        | 503/3432 [02:08<12:28,  3.91it/s][codecarbon INFO @ 13:55:29] Energy consumed for RAM : 0.000663 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:55:29] Energy consumed for all GPUs : 0.005554 kWh. Total GPU Power : 155.97099015705825 W
[codecarbon INFO @ 13:55:29] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:55:29] 0.007811 kWh of electricity used since the beginning.
 16%|â–ˆâ–‹        | 562/3432 [02:23<12:15,  3.90it/s][codecarbon INFO @ 13:55:44] Energy consumed for RAM : 0.000736 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:55:44] Energy consumed for all GPUs : 0.006206 kWh. Total GPU Power : 156.4378924286088 W
[codecarbon INFO @ 13:55:44] Energy consumed for all CPUs : 0.001772 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:55:44] 0.008714 kWh of electricity used since the beginning.
 17%|â–ˆâ–‹        | 572/3432 [02:26<12:12,  3.91it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 9152
  Batch size = 64

  0%|          | 0/143 [00:00<?, ?it/s]
  2%|â–         | 3/143 [00:00<00:07, 17.86it/s]
  3%|â–Ž         | 5/143 [00:00<00:09, 14.53it/s]
  5%|â–         | 7/143 [00:00<00:10, 13.44it/s]
  6%|â–‹         | 9/143 [00:00<00:10, 12.95it/s]
  8%|â–Š         | 11/143 [00:00<00:10, 12.62it/s]
  9%|â–‰         | 13/143 [00:00<00:10, 12.48it/s]
 10%|â–ˆ         | 15/143 [00:01<00:10, 12.37it/s]
 12%|â–ˆâ–        | 17/143 [00:01<00:11, 11.06it/s]
 13%|â–ˆâ–Ž        | 19/143 [00:01<00:11, 10.34it/s]
 15%|â–ˆâ–        | 21/143 [00:01<00:12,  9.90it/s]
 16%|â–ˆâ–Œ        | 23/143 [00:02<00:12,  9.60it/s]
 17%|â–ˆâ–‹        | 24/143 [00:02<00:12,  9.48it/s]
 17%|â–ˆâ–‹        | 25/143 [00:02<00:12,  9.30it/s]
 18%|â–ˆâ–Š        | 26/143 [00:02<00:12,  9.25it/s]
 19%|â–ˆâ–‰        | 27/143 [00:02<00:12,  9.20it/s]
 20%|â–ˆâ–‰        | 28/143 [00:02<00:12,  9.15it/s]
 20%|â–ˆâ–ˆ        | 29/143 [00:02<00:12,  9.08it/s]
 21%|â–ˆâ–ˆ        | 30/143 [00:02<00:12,  9.05it/s]
 22%|â–ˆâ–ˆâ–       | 31/143 [00:02<00:12,  9.03it/s]
 22%|â–ˆâ–ˆâ–       | 32/143 [00:03<00:12,  9.06it/s]
 24%|â–ˆâ–ˆâ–       | 34/143 [00:03<00:11,  9.79it/s]
 25%|â–ˆâ–ˆâ–Œ       | 36/143 [00:03<00:10,  9.95it/s]
 27%|â–ˆâ–ˆâ–‹       | 38/143 [00:03<00:10, 10.11it/s]
 28%|â–ˆâ–ˆâ–Š       | 40/143 [00:03<00:10,  9.99it/s]
 29%|â–ˆâ–ˆâ–‰       | 42/143 [00:04<00:09, 10.21it/s]
 31%|â–ˆâ–ˆâ–ˆ       | 44/143 [00:04<00:09, 10.37it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 46/143 [00:04<00:09, 10.50it/s]
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 48/143 [00:04<00:08, 10.97it/s]
 35%|â–ˆâ–ˆâ–ˆâ–      | 50/143 [00:04<00:07, 11.64it/s]
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 52/143 [00:04<00:07, 12.23it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 54/143 [00:04<00:07, 12.64it/s]
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 56/143 [00:05<00:06, 12.98it/s]
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 58/143 [00:05<00:06, 13.20it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 60/143 [00:05<00:06, 13.36it/s]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 62/143 [00:05<00:06, 13.44it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 64/143 [00:05<00:05, 13.20it/s]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 66/143 [00:05<00:05, 12.98it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 68/143 [00:06<00:05, 12.78it/s]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 70/143 [00:06<00:05, 12.68it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 72/143 [00:06<00:05, 12.62it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 74/143 [00:06<00:05, 12.58it/s]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 76/143 [00:06<00:05, 12.54it/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78/143 [00:06<00:05, 12.46it/s]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 80/143 [00:07<00:05, 11.89it/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 82/143 [00:07<00:05, 11.52it/s]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 84/143 [00:07<00:05, 11.21it/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 86/143 [00:07<00:05, 10.94it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 88/143 [00:07<00:05, 10.86it/s]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 90/143 [00:07<00:04, 10.74it/s]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 92/143 [00:08<00:04, 10.63it/s]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 94/143 [00:08<00:04, 10.62it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 96/143 [00:08<00:04, 10.65it/s]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 98/143 [00:08<00:04, 10.71it/s]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 100/143 [00:08<00:04, 10.64it/s]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 102/143 [00:09<00:03, 10.65it/s]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 104/143 [00:09<00:03, 10.74it/s]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 106/143 [00:09<00:03, 10.76it/s]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 108/143 [00:09<00:03, 10.79it/s]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 110/143 [00:09<00:03, 10.86it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 112/143 [00:10<00:02, 11.35it/s]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 114/143 [00:10<00:02, 12.03it/s]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 116/143 [00:10<00:02, 12.56it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 118/143 [00:10<00:01, 12.92it/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 120/143 [00:10<00:01, 13.15it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 122/143 [00:10<00:01, 13.33it/s]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 124/143 [00:10<00:01, 13.56it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 126/143 [00:11<00:01, 13.67it/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 128/143 [00:11<00:01, 13.40it/s]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 130/143 [00:11<00:00, 13.20it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 132/143 [00:11<00:00, 13.28it/s]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 134/143 [00:11<00:00, 13.28it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 136/143 [00:11<00:00, 13.47it/s]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 138/143 [00:11<00:00, 13.56it/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 140/143 [00:12<00:00, 13.63it/s]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 142/143 [00:12<00:00, 13.86it/s]
{'eval_loss': 0.40467745065689087, 'eval_precision': 0.7857675425847335, 'eval_recall': 0.7501613848481933, 'eval_f1': 0.7620472356201313, 'eval_balanced accuracy': 0.7501613848481933, 'eval_runtime': 12.3511, 'eval_samples_per_second': 740.984, 'eval_steps_per_second': 11.578, 'epoch': 1.0}
                                                  
 17%|â–ˆâ–‹        | 572/3432 [02:38<12:12,  3.91it/s]
                                                 Saving model checkpoint to model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-572
Configuration saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-572\config.json
Model weights saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-572\model.safetensors
tokenizer config file saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-572\tokenizer_config.json
Special tokens file saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-572\special_tokens_map.json
[codecarbon INFO @ 13:55:59] Energy consumed for RAM : 0.000810 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:55:59] Energy consumed for all GPUs : 0.006867 kWh. Total GPU Power : 158.4518280940241 W
[codecarbon INFO @ 13:55:59] Energy consumed for all CPUs : 0.001949 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:55:59] 0.009626 kWh of electricity used since the beginning.
 18%|â–ˆâ–Š        | 630/3432 [02:53<12:42,  3.67it/s][codecarbon INFO @ 13:56:14] Energy consumed for RAM : 0.000884 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:56:14] Energy consumed for all GPUs : 0.007515 kWh. Total GPU Power : 155.34031808299994 W
[codecarbon INFO @ 13:56:14] Energy consumed for all CPUs : 0.002126 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:56:14] 0.010525 kWh of electricity used since the beginning.
 20%|â–ˆâ–ˆ        | 687/3432 [03:08<12:04,  3.79it/s][codecarbon INFO @ 13:56:29] Energy consumed for RAM : 0.000957 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:56:29] Energy consumed for all GPUs : 0.008165 kWh. Total GPU Power : 155.9782188402238 W
[codecarbon INFO @ 13:56:29] Energy consumed for all CPUs : 0.002303 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:56:29] 0.011426 kWh of electricity used since the beginning.
 22%|â–ˆâ–ˆâ–       | 745/3432 [03:23<11:06,  4.03it/s][codecarbon INFO @ 13:56:44] Energy consumed for RAM : 0.001031 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:56:44] Energy consumed for all GPUs : 0.008815 kWh. Total GPU Power : 155.80917006481315 W
[codecarbon INFO @ 13:56:44] Energy consumed for all CPUs : 0.002481 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:56:44] 0.012326 kWh of electricity used since the beginning.
 23%|â–ˆâ–ˆâ–Ž       | 804/3432 [03:38<11:15,  3.89it/s][codecarbon INFO @ 13:56:59] Energy consumed for RAM : 0.001105 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:56:59] Energy consumed for all GPUs : 0.009472 kWh. Total GPU Power : 157.6046798191783 W
[codecarbon INFO @ 13:56:59] Energy consumed for all CPUs : 0.002658 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:56:59] 0.013234 kWh of electricity used since the beginning.
 25%|â–ˆâ–ˆâ–Œ       | 862/3432 [03:53<11:12,  3.82it/s][codecarbon INFO @ 13:57:14] Energy consumed for RAM : 0.001178 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:57:14] Energy consumed for all GPUs : 0.010128 kWh. Total GPU Power : 157.36507954975767 W
[codecarbon INFO @ 13:57:14] Energy consumed for all CPUs : 0.002835 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:57:14] 0.014141 kWh of electricity used since the beginning.
[codecarbon INFO @ 13:57:14] 0.014307 g.CO2eq/s mean an estimation of 451.169986008496 kg.CO2eq/year
 27%|â–ˆâ–ˆâ–‹       | 920/3432 [04:08<10:58,  3.82it/s][codecarbon INFO @ 13:57:29] Energy consumed for RAM : 0.001252 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:57:29] Energy consumed for all GPUs : 0.010782 kWh. Total GPU Power : 156.8866767512305 W
[codecarbon INFO @ 13:57:29] Energy consumed for all CPUs : 0.003012 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:57:29] 0.015046 kWh of electricity used since the beginning.
 28%|â–ˆâ–ˆâ–Š       | 978/3432 [04:23<10:27,  3.91it/s][codecarbon INFO @ 13:57:44] Energy consumed for RAM : 0.001326 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:57:44] Energy consumed for all GPUs : 0.011435 kWh. Total GPU Power : 156.6508859651498 W
[codecarbon INFO @ 13:57:44] Energy consumed for all CPUs : 0.003189 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:57:44] 0.015950 kWh of electricity used since the beginning.
 29%|â–ˆâ–ˆâ–‰       | 1000/3432 [04:29<10:24,  3.90it/s]{'loss': 0.3457, 'grad_norm': 10.531039237976074, 'learning_rate': 1.4172494172494174e-05, 'epoch': 1.75}
 30%|â–ˆâ–ˆâ–ˆ       | 1037/3432 [04:38<10:23,  3.84it/s][codecarbon INFO @ 13:57:59] Energy consumed for RAM : 0.001399 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:57:59] Energy consumed for all GPUs : 0.012093 kWh. Total GPU Power : 158.02624589656517 W
[codecarbon INFO @ 13:57:59] Energy consumed for all CPUs : 0.003366 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:57:59] 0.016859 kWh of electricity used since the beginning.
 32%|â–ˆâ–ˆâ–ˆâ–      | 1092/3432 [04:53<10:40,  3.65it/s][codecarbon INFO @ 13:58:14] Energy consumed for RAM : 0.001473 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:58:14] Energy consumed for all GPUs : 0.012719 kWh. Total GPU Power : 150.1482376229218 W
[codecarbon INFO @ 13:58:14] Energy consumed for all CPUs : 0.003544 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:58:14] 0.017736 kWh of electricity used since the beginning.
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1144/3432 [05:07<09:34,  3.98it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 9152
  Batch size = 64

  0%|          | 0/143 [00:00<?, ?it/s]
  2%|â–         | 3/143 [00:00<00:08, 17.33it/s]
  3%|â–Ž         | 5/143 [00:00<00:09, 14.37it/s]
  5%|â–         | 7/143 [00:00<00:10, 13.20it/s]
  6%|â–‹         | 9/143 [00:00<00:10, 12.81it/s]
  8%|â–Š         | 11/143 [00:00<00:10, 12.46it/s]
  9%|â–‰         | 13/143 [00:01<00:10, 12.39it/s]
 10%|â–ˆ         | 15/143 [00:01<00:10, 12.22it/s]
 12%|â–ˆâ–        | 17/143 [00:01<00:11, 10.96it/s][codecarbon INFO @ 13:58:29] Energy consumed for RAM : 0.001547 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:58:29] Energy consumed for all GPUs : 0.013382 kWh. Total GPU Power : 158.93816307729122 W
[codecarbon INFO @ 13:58:29] Energy consumed for all CPUs : 0.003721 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:58:29] 0.018650 kWh of electricity used since the beginning.

 13%|â–ˆâ–Ž        | 19/143 [00:01<00:12, 10.28it/s]
 15%|â–ˆâ–        | 21/143 [00:01<00:12,  9.84it/s]
 16%|â–ˆâ–Œ        | 23/143 [00:02<00:12,  9.51it/s]
 17%|â–ˆâ–‹        | 24/143 [00:02<00:12,  9.43it/s]
 17%|â–ˆâ–‹        | 25/143 [00:02<00:12,  9.32it/s]
 18%|â–ˆâ–Š        | 26/143 [00:02<00:12,  9.22it/s]
 19%|â–ˆâ–‰        | 27/143 [00:02<00:12,  9.14it/s]
 20%|â–ˆâ–‰        | 28/143 [00:02<00:12,  9.13it/s]
 20%|â–ˆâ–ˆ        | 29/143 [00:02<00:12,  9.02it/s]
 21%|â–ˆâ–ˆ        | 30/143 [00:02<00:12,  9.03it/s]
 22%|â–ˆâ–ˆâ–       | 31/143 [00:02<00:12,  8.95it/s]
 22%|â–ˆâ–ˆâ–       | 32/143 [00:03<00:12,  9.00it/s]
 24%|â–ˆâ–ˆâ–       | 34/143 [00:03<00:11,  9.66it/s]
 25%|â–ˆâ–ˆâ–Œ       | 36/143 [00:03<00:10, 10.11it/s]
 27%|â–ˆâ–ˆâ–‹       | 38/143 [00:03<00:10, 10.31it/s]
 28%|â–ˆâ–ˆâ–Š       | 40/143 [00:03<00:09, 10.51it/s]
 29%|â–ˆâ–ˆâ–‰       | 42/143 [00:04<00:09, 10.55it/s]
 31%|â–ˆâ–ˆâ–ˆ       | 44/143 [00:04<00:09, 10.67it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 46/143 [00:04<00:09, 10.66it/s]
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 48/143 [00:04<00:08, 11.10it/s]
 35%|â–ˆâ–ˆâ–ˆâ–      | 50/143 [00:04<00:07, 11.71it/s]
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 52/143 [00:04<00:07, 12.27it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 54/143 [00:04<00:07, 12.60it/s]
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 56/143 [00:05<00:06, 12.96it/s]
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 58/143 [00:05<00:06, 13.10it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 60/143 [00:05<00:06, 13.30it/s]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 62/143 [00:05<00:06, 13.37it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 64/143 [00:05<00:06, 13.11it/s]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 66/143 [00:05<00:05, 12.85it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 68/143 [00:06<00:05, 12.77it/s]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 70/143 [00:06<00:05, 12.61it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 72/143 [00:06<00:05, 12.60it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 74/143 [00:06<00:05, 12.51it/s]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 76/143 [00:06<00:05, 12.55it/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78/143 [00:06<00:05, 12.44it/s]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 80/143 [00:07<00:05, 11.91it/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 82/143 [00:07<00:05, 11.46it/s]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 84/143 [00:07<00:05, 11.24it/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 86/143 [00:07<00:05, 11.02it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 88/143 [00:07<00:05, 10.94it/s]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 90/143 [00:07<00:04, 10.82it/s]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 92/143 [00:08<00:04, 10.82it/s]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 94/143 [00:08<00:04, 10.75it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 96/143 [00:08<00:04, 10.82it/s]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 98/143 [00:08<00:04, 10.82it/s]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 100/143 [00:08<00:03, 10.87it/s]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 102/143 [00:09<00:03, 10.84it/s]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 104/143 [00:09<00:03, 10.87it/s]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 106/143 [00:09<00:03, 10.85it/s]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 108/143 [00:09<00:03, 10.89it/s]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 110/143 [00:09<00:03, 10.88it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 112/143 [00:09<00:02, 11.68it/s]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 114/143 [00:10<00:02, 12.29it/s]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 116/143 [00:10<00:02, 12.78it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 118/143 [00:10<00:01, 13.09it/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 120/143 [00:10<00:01, 13.37it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 122/143 [00:10<00:01, 13.52it/s]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 124/143 [00:10<00:01, 13.72it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 126/143 [00:10<00:01, 13.69it/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 128/143 [00:11<00:01, 13.74it/s]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 130/143 [00:11<00:00, 13.68it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 132/143 [00:11<00:00, 13.69it/s]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 134/143 [00:11<00:00, 13.61it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 136/143 [00:11<00:00, 13.68it/s]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 138/143 [00:11<00:00, 13.63it/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 140/143 [00:11<00:00, 13.68it/s]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 142/143 [00:12<00:00, 13.85it/s]
{'eval_loss': 0.36263352632522583, 'eval_precision': 0.8155530497935324, 'eval_recall': 0.8136968647312148, 'eval_f1': 0.8146065838601255, 'eval_balanced accuracy': 0.8136968647312148, 'eval_runtime': 12.2809, 'eval_samples_per_second': 745.22, 'eval_steps_per_second': 11.644, 'epoch': 2.0}
                                                   
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1144/3432 [05:19<09:34,  3.98it/s]
                                                 Saving model checkpoint to model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1144
Configuration saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1144\config.json
Model weights saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1144\model.safetensors
tokenizer config file saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1144\tokenizer_config.json
Special tokens file saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1144\special_tokens_map.json
Deleting older checkpoint [model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-572] due to args.save_total_limit
 34%|â–ˆâ–ˆâ–ˆâ–      | 1160/3432 [05:23<10:26,  3.63it/s][codecarbon INFO @ 13:58:44] Energy consumed for RAM : 0.001620 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:58:44] Energy consumed for all GPUs : 0.014046 kWh. Total GPU Power : 159.23250717703115 W
[codecarbon INFO @ 13:58:44] Energy consumed for all CPUs : 0.003898 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:58:44] 0.019565 kWh of electricity used since the beginning.
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1219/3432 [05:39<09:30,  3.88it/s][codecarbon INFO @ 13:58:59] Energy consumed for RAM : 0.001694 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:58:59] Energy consumed for all GPUs : 0.014706 kWh. Total GPU Power : 158.31850376530883 W
[codecarbon INFO @ 13:58:59] Energy consumed for all CPUs : 0.004075 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:58:59] 0.020475 kWh of electricity used since the beginning.
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1277/3432 [05:53<09:14,  3.89it/s][codecarbon INFO @ 13:59:14] Energy consumed for RAM : 0.001768 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:59:14] Energy consumed for all GPUs : 0.015367 kWh. Total GPU Power : 158.5790379676596 W
[codecarbon INFO @ 13:59:14] Energy consumed for all CPUs : 0.004252 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:59:14] 0.021386 kWh of electricity used since the beginning.
[codecarbon INFO @ 13:59:14] 0.014338 g.CO2eq/s mean an estimation of 452.1498322498486 kg.CO2eq/year
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1336/3432 [06:09<08:55,  3.91it/s][codecarbon INFO @ 13:59:29] Energy consumed for RAM : 0.001841 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:59:29] Energy consumed for all GPUs : 0.016028 kWh. Total GPU Power : 158.76013474027505 W
[codecarbon INFO @ 13:59:29] Energy consumed for all CPUs : 0.004429 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:59:29] 0.022299 kWh of electricity used since the beginning.
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1394/3432 [06:23<08:42,  3.90it/s][codecarbon INFO @ 13:59:44] Energy consumed for RAM : 0.001915 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:59:44] Energy consumed for all GPUs : 0.016690 kWh. Total GPU Power : 158.61684122521908 W
[codecarbon INFO @ 13:59:44] Energy consumed for all CPUs : 0.004607 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:59:44] 0.023211 kWh of electricity used since the beginning.
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1453/3432 [06:38<08:29,  3.89it/s][codecarbon INFO @ 13:59:59] Energy consumed for RAM : 0.001989 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 13:59:59] Energy consumed for all GPUs : 0.017353 kWh. Total GPU Power : 159.09106800040936 W
[codecarbon INFO @ 13:59:59] Energy consumed for all CPUs : 0.004784 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 13:59:59] 0.024126 kWh of electricity used since the beginning.
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1500/3432 [06:50<08:17,  3.89it/s]{'loss': 0.2686, 'grad_norm': 11.247385025024414, 'learning_rate': 1.1258741258741259e-05, 'epoch': 2.62}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1512/3432 [06:54<08:12,  3.90it/s][codecarbon INFO @ 14:00:14] Energy consumed for RAM : 0.002062 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:00:14] Energy consumed for all GPUs : 0.018015 kWh. Total GPU Power : 158.76139115754566 W
[codecarbon INFO @ 14:00:14] Energy consumed for all CPUs : 0.004961 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:00:14] 0.025039 kWh of electricity used since the beginning.
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1570/3432 [07:08<07:59,  3.88it/s][codecarbon INFO @ 14:00:29] Energy consumed for RAM : 0.002136 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:00:29] Energy consumed for all GPUs : 0.018677 kWh. Total GPU Power : 158.69393793703685 W
[codecarbon INFO @ 14:00:29] Energy consumed for all CPUs : 0.005138 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:00:29] 0.025951 kWh of electricity used since the beginning.
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1629/3432 [07:24<07:44,  3.88it/s][codecarbon INFO @ 14:00:44] Energy consumed for RAM : 0.002210 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:00:44] Energy consumed for all GPUs : 0.019342 kWh. Total GPU Power : 159.43564433102165 W
[codecarbon INFO @ 14:00:44] Energy consumed for all CPUs : 0.005316 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:00:44] 0.026867 kWh of electricity used since the beginning.
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1687/3432 [07:38<07:30,  3.88it/s][codecarbon INFO @ 14:00:59] Energy consumed for RAM : 0.002283 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:00:59] Energy consumed for all GPUs : 0.020005 kWh. Total GPU Power : 159.1115664490472 W
[codecarbon INFO @ 14:00:59] Energy consumed for all CPUs : 0.005493 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:00:59] 0.027781 kWh of electricity used since the beginning.
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1716/3432 [07:46<07:17,  3.92it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 9152
  Batch size = 64

  0%|          | 0/143 [00:00<?, ?it/s]
  2%|â–         | 3/143 [00:00<00:07, 17.55it/s]
  3%|â–Ž         | 5/143 [00:00<00:09, 14.30it/s]
  5%|â–         | 7/143 [00:00<00:10, 13.20it/s]
  6%|â–‹         | 9/143 [00:00<00:10, 12.81it/s]
  8%|â–Š         | 11/143 [00:00<00:10, 12.49it/s]
  9%|â–‰         | 13/143 [00:01<00:10, 12.36it/s]
 10%|â–ˆ         | 15/143 [00:01<00:10, 12.14it/s]
 12%|â–ˆâ–        | 17/143 [00:01<00:11, 10.96it/s]
 13%|â–ˆâ–Ž        | 19/143 [00:01<00:12, 10.26it/s]
 15%|â–ˆâ–        | 21/143 [00:01<00:12,  9.83it/s]
 16%|â–ˆâ–Œ        | 23/143 [00:02<00:12,  9.54it/s]
 17%|â–ˆâ–‹        | 24/143 [00:02<00:12,  9.45it/s]
 17%|â–ˆâ–‹        | 25/143 [00:02<00:12,  9.32it/s]
 18%|â–ˆâ–Š        | 26/143 [00:02<00:12,  9.25it/s]
 19%|â–ˆâ–‰        | 27/143 [00:02<00:12,  9.14it/s]
 20%|â–ˆâ–‰        | 28/143 [00:02<00:12,  9.11it/s]
 20%|â–ˆâ–ˆ        | 29/143 [00:02<00:12,  9.01it/s]
 21%|â–ˆâ–ˆ        | 30/143 [00:02<00:12,  9.01it/s]
 22%|â–ˆâ–ˆâ–       | 31/143 [00:02<00:12,  8.97it/s]
 22%|â–ˆâ–ˆâ–       | 32/143 [00:03<00:12,  8.98it/s]
 24%|â–ˆâ–ˆâ–       | 34/143 [00:03<00:11,  9.70it/s]
 25%|â–ˆâ–ˆâ–Œ       | 36/143 [00:03<00:10, 10.13it/s]
 27%|â–ˆâ–ˆâ–‹       | 38/143 [00:03<00:10, 10.32it/s]
 28%|â–ˆâ–ˆâ–Š       | 40/143 [00:03<00:09, 10.51it/s]
 29%|â–ˆâ–ˆâ–‰       | 42/143 [00:04<00:09, 10.53it/s]
 31%|â–ˆâ–ˆâ–ˆ       | 44/143 [00:04<00:09, 10.66it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 46/143 [00:04<00:09, 10.67it/s]
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 48/143 [00:04<00:08, 11.11it/s]
 35%|â–ˆâ–ˆâ–ˆâ–      | 50/143 [00:04<00:07, 11.71it/s]
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 52/143 [00:04<00:07, 12.30it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 54/143 [00:04<00:07, 12.59it/s]
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 56/143 [00:05<00:06, 12.94it/s]
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 58/143 [00:05<00:06, 13.10it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 60/143 [00:05<00:06, 13.31it/s]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 62/143 [00:05<00:06, 13.33it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 64/143 [00:05<00:06, 13.09it/s]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 66/143 [00:05<00:05, 12.84it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 68/143 [00:06<00:05, 12.74it/s]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 70/143 [00:06<00:05, 12.58it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 72/143 [00:06<00:05, 12.60it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 74/143 [00:06<00:05, 12.49it/s]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 76/143 [00:06<00:05, 12.51it/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78/143 [00:06<00:05, 12.39it/s]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 80/143 [00:07<00:05, 11.86it/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 82/143 [00:07<00:05, 11.45it/s]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 84/143 [00:07<00:05, 11.24it/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 86/143 [00:07<00:05, 11.03it/s][codecarbon INFO @ 14:01:14] Energy consumed for RAM : 0.002357 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:01:14] Energy consumed for all GPUs : 0.020674 kWh. Total GPU Power : 160.48978223218003 W
[codecarbon INFO @ 14:01:14] Energy consumed for all CPUs : 0.005670 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:01:14] 0.028700 kWh of electricity used since the beginning.
[codecarbon INFO @ 14:01:14] 0.014472 g.CO2eq/s mean an estimation of 456.38052002006856 kg.CO2eq/year

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 88/143 [00:07<00:05, 10.96it/s]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 90/143 [00:07<00:04, 10.72it/s]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 92/143 [00:08<00:04, 10.74it/s]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 94/143 [00:08<00:04, 10.70it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 96/143 [00:08<00:04, 10.77it/s]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 98/143 [00:08<00:04, 10.80it/s]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 100/143 [00:08<00:03, 10.86it/s]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 102/143 [00:09<00:03, 10.82it/s]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 104/143 [00:09<00:03, 10.87it/s]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 106/143 [00:09<00:03, 10.83it/s]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 108/143 [00:09<00:03, 10.87it/s]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 110/143 [00:09<00:03, 10.86it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 112/143 [00:09<00:02, 11.67it/s]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 114/143 [00:10<00:02, 12.26it/s]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 116/143 [00:10<00:02, 12.77it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 118/143 [00:10<00:01, 13.08it/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 120/143 [00:10<00:01, 13.36it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 122/143 [00:10<00:01, 13.54it/s]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 124/143 [00:10<00:01, 13.74it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 126/143 [00:10<00:01, 13.76it/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 128/143 [00:11<00:01, 13.76it/s]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 130/143 [00:11<00:00, 13.74it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 132/143 [00:11<00:00, 13.77it/s]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 134/143 [00:11<00:00, 13.70it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 136/143 [00:11<00:00, 13.75it/s]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 138/143 [00:11<00:00, 13.69it/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 140/143 [00:11<00:00, 13.72it/s]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 142/143 [00:12<00:00, 13.85it/s]
{'eval_loss': 0.381797730922699, 'eval_precision': 0.81960045396345, 'eval_recall': 0.807416705299896, 'eval_f1': 0.8128042165386868, 'eval_balanced accuracy': 0.807416705299896, 'eval_runtime': 12.2885, 'eval_samples_per_second': 744.762, 'eval_steps_per_second': 11.637, 'epoch': 3.0}
                                                   
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1716/3432 [07:58<07:17,  3.92it/s]
                                                 Saving model checkpoint to model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1716
Configuration saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1716\config.json
Model weights saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1716\model.safetensors
tokenizer config file saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1716\tokenizer_config.json
Special tokens file saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1716\special_tokens_map.json
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1756/3432 [08:09<07:09,  3.90it/s][codecarbon INFO @ 14:01:29] Energy consumed for RAM : 0.002431 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:01:29] Energy consumed for all GPUs : 0.021336 kWh. Total GPU Power : 158.84128739924944 W
[codecarbon INFO @ 14:01:29] Energy consumed for all CPUs : 0.005847 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:01:29] 0.029613 kWh of electricity used since the beginning.
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1814/3432 [08:23<06:57,  3.88it/s][codecarbon INFO @ 14:01:44] Energy consumed for RAM : 0.002504 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:01:44] Energy consumed for all GPUs : 0.021998 kWh. Total GPU Power : 158.8948655125003 W
[codecarbon INFO @ 14:01:44] Energy consumed for all CPUs : 0.006024 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:01:44] 0.030526 kWh of electricity used since the beginning.
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1873/3432 [08:39<06:37,  3.93it/s][codecarbon INFO @ 14:01:59] Energy consumed for RAM : 0.002578 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:01:59] Energy consumed for all GPUs : 0.022660 kWh. Total GPU Power : 158.86998611695466 W
[codecarbon INFO @ 14:01:59] Energy consumed for all CPUs : 0.006201 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:01:59] 0.031439 kWh of electricity used since the beginning.
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1931/3432 [08:53<06:24,  3.91it/s][codecarbon INFO @ 14:02:14] Energy consumed for RAM : 0.002651 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:02:14] Energy consumed for all GPUs : 0.023323 kWh. Total GPU Power : 159.1988523564943 W
[codecarbon INFO @ 14:02:14] Energy consumed for all CPUs : 0.006378 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:02:14] 0.032353 kWh of electricity used since the beginning.
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1990/3432 [09:09<06:09,  3.91it/s][codecarbon INFO @ 14:02:29] Energy consumed for RAM : 0.002725 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:02:29] Energy consumed for all GPUs : 0.023987 kWh. Total GPU Power : 159.29559081514236 W
[codecarbon INFO @ 14:02:29] Energy consumed for all CPUs : 0.006556 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:02:29] 0.033268 kWh of electricity used since the beginning.
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2000/3432 [09:11<06:06,  3.91it/s]{'loss': 0.1949, 'grad_norm': 17.23011016845703, 'learning_rate': 8.344988344988347e-06, 'epoch': 3.5}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2048/3432 [09:24<05:55,  3.90it/s][codecarbon INFO @ 14:02:44] Energy consumed for RAM : 0.002799 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:02:44] Energy consumed for all GPUs : 0.024651 kWh. Total GPU Power : 159.25536834154568 W
[codecarbon INFO @ 14:02:44] Energy consumed for all CPUs : 0.006733 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:02:44] 0.034183 kWh of electricity used since the beginning.
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2107/3432 [09:39<05:41,  3.88it/s][codecarbon INFO @ 14:02:59] Energy consumed for RAM : 0.002872 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:02:59] Energy consumed for all GPUs : 0.025316 kWh. Total GPU Power : 159.29502981644973 W
[codecarbon INFO @ 14:02:59] Energy consumed for all CPUs : 0.006910 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:02:59] 0.035098 kWh of electricity used since the beginning.
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2165/3432 [09:54<05:23,  3.92it/s][codecarbon INFO @ 14:03:14] Energy consumed for RAM : 0.002946 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:03:14] Energy consumed for all GPUs : 0.025979 kWh. Total GPU Power : 159.22535193576866 W
[codecarbon INFO @ 14:03:14] Energy consumed for all CPUs : 0.007087 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:03:14] 0.036013 kWh of electricity used since the beginning.
[codecarbon INFO @ 14:03:14] 0.014470 g.CO2eq/s mean an estimation of 456.33621515772455 kg.CO2eq/year
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2223/3432 [10:08<05:08,  3.92it/s][codecarbon INFO @ 14:03:29] Energy consumed for RAM : 0.003020 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:03:29] Energy consumed for all GPUs : 0.026649 kWh. Total GPU Power : 160.67599406890068 W
[codecarbon INFO @ 14:03:29] Energy consumed for all CPUs : 0.007264 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:03:29] 0.036934 kWh of electricity used since the beginning.
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2282/3432 [10:24<04:56,  3.88it/s][codecarbon INFO @ 14:03:44] Energy consumed for RAM : 0.003093 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:03:44] Energy consumed for all GPUs : 0.027315 kWh. Total GPU Power : 159.66205790538044 W
[codecarbon INFO @ 14:03:44] Energy consumed for all CPUs : 0.007442 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:03:44] 0.037850 kWh of electricity used since the beginning.
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2288/3432 [10:25<04:53,  3.89it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 9152
  Batch size = 64

  0%|          | 0/143 [00:00<?, ?it/s]
  2%|â–         | 3/143 [00:00<00:08, 17.11it/s]
  3%|â–Ž         | 5/143 [00:00<00:09, 14.26it/s]
  5%|â–         | 7/143 [00:00<00:10, 13.16it/s]
  6%|â–‹         | 9/143 [00:00<00:10, 12.78it/s]
  8%|â–Š         | 11/143 [00:00<00:10, 12.46it/s]
  9%|â–‰         | 13/143 [00:01<00:10, 12.35it/s]
 10%|â–ˆ         | 15/143 [00:01<00:10, 12.18it/s]
 12%|â–ˆâ–        | 17/143 [00:01<00:11, 10.95it/s]
 13%|â–ˆâ–Ž        | 19/143 [00:01<00:12, 10.25it/s]
 15%|â–ˆâ–        | 21/143 [00:01<00:12,  9.83it/s]
 16%|â–ˆâ–Œ        | 23/143 [00:02<00:12,  9.57it/s]
 17%|â–ˆâ–‹        | 24/143 [00:02<00:12,  9.47it/s]
 17%|â–ˆâ–‹        | 25/143 [00:02<00:12,  9.35it/s]
 18%|â–ˆâ–Š        | 26/143 [00:02<00:12,  9.26it/s]
 19%|â–ˆâ–‰        | 27/143 [00:02<00:12,  9.14it/s]
 20%|â–ˆâ–‰        | 28/143 [00:02<00:12,  9.09it/s]
 20%|â–ˆâ–ˆ        | 29/143 [00:02<00:12,  9.07it/s]
 21%|â–ˆâ–ˆ        | 30/143 [00:02<00:12,  8.98it/s]
 22%|â–ˆâ–ˆâ–       | 31/143 [00:02<00:12,  8.97it/s]
 22%|â–ˆâ–ˆâ–       | 32/143 [00:03<00:12,  8.96it/s]
 24%|â–ˆâ–ˆâ–       | 34/143 [00:03<00:11,  9.65it/s]
 25%|â–ˆâ–ˆâ–Œ       | 36/143 [00:03<00:10, 10.07it/s]
 27%|â–ˆâ–ˆâ–‹       | 38/143 [00:03<00:10, 10.29it/s]
 28%|â–ˆâ–ˆâ–Š       | 40/143 [00:03<00:09, 10.48it/s]
 29%|â–ˆâ–ˆâ–‰       | 42/143 [00:04<00:09, 10.53it/s]
 31%|â–ˆâ–ˆâ–ˆ       | 44/143 [00:04<00:09, 10.63it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 46/143 [00:04<00:09, 10.64it/s]
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 48/143 [00:04<00:08, 11.05it/s]
 35%|â–ˆâ–ˆâ–ˆâ–      | 50/143 [00:04<00:07, 11.70it/s]
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 52/143 [00:04<00:07, 12.26it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 54/143 [00:04<00:07, 12.59it/s]
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 56/143 [00:05<00:06, 12.92it/s]
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 58/143 [00:05<00:06, 13.06it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 60/143 [00:05<00:06, 13.24it/s]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 62/143 [00:05<00:06, 13.34it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 64/143 [00:05<00:06, 13.08it/s]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 66/143 [00:05<00:05, 12.84it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 68/143 [00:06<00:05, 12.74it/s]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 70/143 [00:06<00:05, 12.58it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 72/143 [00:06<00:05, 12.56it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 74/143 [00:06<00:05, 12.47it/s]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 76/143 [00:06<00:05, 12.49it/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78/143 [00:06<00:05, 12.40it/s]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 80/143 [00:07<00:05, 11.85it/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 82/143 [00:07<00:05, 11.43it/s]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 84/143 [00:07<00:05, 11.21it/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 86/143 [00:07<00:05, 11.00it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 88/143 [00:07<00:05, 10.94it/s]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 90/143 [00:07<00:04, 10.80it/s]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 92/143 [00:08<00:04, 10.79it/s]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 94/143 [00:08<00:04, 10.71it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 96/143 [00:08<00:04, 10.80it/s]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 98/143 [00:08<00:04, 10.81it/s]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 100/143 [00:08<00:03, 10.85it/s]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 102/143 [00:09<00:03, 10.82it/s]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 104/143 [00:09<00:03, 10.87it/s]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 106/143 [00:09<00:03, 10.85it/s]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 108/143 [00:09<00:03, 10.88it/s]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 110/143 [00:09<00:03, 10.84it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 112/143 [00:09<00:02, 11.67it/s]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 114/143 [00:10<00:02, 12.24it/s]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 116/143 [00:10<00:02, 12.66it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 118/143 [00:10<00:01, 12.96it/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 120/143 [00:10<00:01, 13.30it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 122/143 [00:10<00:01, 13.44it/s]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 124/143 [00:10<00:01, 13.63it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 126/143 [00:10<00:01, 13.61it/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 128/143 [00:11<00:01, 13.70it/s]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 130/143 [00:11<00:00, 13.62it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 132/143 [00:11<00:00, 13.66it/s]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 134/143 [00:11<00:00, 13.54it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 136/143 [00:11<00:00, 13.62it/s]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 138/143 [00:11<00:00, 13.52it/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 140/143 [00:12<00:00, 13.60it/s]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 142/143 [00:12<00:00, 13.77it/s]
                                                   
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2288/3432 [10:37<04:53,  3.89it/s]
                                                 Saving model checkpoint to model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2288
Configuration saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2288\config.json
{'eval_loss': 0.4737245738506317, 'eval_precision': 0.8171741790146233, 'eval_recall': 0.8151590960115929, 'eval_f1': 0.8161451861276664, 'eval_balanced accuracy': 0.8151590960115929, 'eval_runtime': 12.3152, 'eval_samples_per_second': 743.148, 'eval_steps_per_second': 11.612, 'epoch': 4.0}
Model weights saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2288\model.safetensors
tokenizer config file saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2288\tokenizer_config.json
Special tokens file saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2288\special_tokens_map.json
Deleting older checkpoint [model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1716] due to args.save_total_limit
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2292/3432 [10:39<29:17,  1.54s/it][codecarbon INFO @ 14:03:59] Energy consumed for RAM : 0.003167 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:03:59] Energy consumed for all GPUs : 0.027983 kWh. Total GPU Power : 160.2357669721359 W
[codecarbon INFO @ 14:03:59] Energy consumed for all CPUs : 0.007619 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:03:59] 0.038769 kWh of electricity used since the beginning.
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2350/3432 [10:53<04:37,  3.89it/s][codecarbon INFO @ 14:04:14] Energy consumed for RAM : 0.003241 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:04:14] Energy consumed for all GPUs : 0.028650 kWh. Total GPU Power : 159.93601375432704 W
[codecarbon INFO @ 14:04:14] Energy consumed for all CPUs : 0.007796 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:04:14] 0.039686 kWh of electricity used since the beginning.
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2409/3432 [11:09<04:23,  3.89it/s][codecarbon INFO @ 14:04:29] Energy consumed for RAM : 0.003314 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:04:29] Energy consumed for all GPUs : 0.029314 kWh. Total GPU Power : 159.32191328419054 W
[codecarbon INFO @ 14:04:29] Energy consumed for all CPUs : 0.007973 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:04:29] 0.040601 kWh of electricity used since the beginning.
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2467/3432 [11:24<04:08,  3.88it/s][codecarbon INFO @ 14:04:44] Energy consumed for RAM : 0.003388 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:04:44] Energy consumed for all GPUs : 0.029979 kWh. Total GPU Power : 159.66286191025563 W
[codecarbon INFO @ 14:04:44] Energy consumed for all CPUs : 0.008150 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:04:44] 0.041518 kWh of electricity used since the beginning.
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2500/3432 [11:32<04:00,  3.88it/s]{'loss': 0.1218, 'grad_norm': 26.06224250793457, 'learning_rate': 5.431235431235432e-06, 'epoch': 4.37}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2526/3432 [11:39<03:53,  3.89it/s][codecarbon INFO @ 14:04:59] Energy consumed for RAM : 0.003462 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:04:59] Energy consumed for all GPUs : 0.030645 kWh. Total GPU Power : 159.71735689787235 W
[codecarbon INFO @ 14:04:59] Energy consumed for all CPUs : 0.008328 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:04:59] 0.042435 kWh of electricity used since the beginning.
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2584/3432 [11:54<03:38,  3.88it/s][codecarbon INFO @ 14:05:14] Energy consumed for RAM : 0.003535 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:05:14] Energy consumed for all GPUs : 0.031311 kWh. Total GPU Power : 159.75447274807456 W
[codecarbon INFO @ 14:05:14] Energy consumed for all CPUs : 0.008505 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:05:14] 0.043351 kWh of electricity used since the beginning.
[codecarbon INFO @ 14:05:14] 0.014521 g.CO2eq/s mean an estimation of 457.9421394487911 kg.CO2eq/year
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2643/3432 [12:09<03:22,  3.90it/s][codecarbon INFO @ 14:05:29] Energy consumed for RAM : 0.003609 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:05:29] Energy consumed for all GPUs : 0.031978 kWh. Total GPU Power : 159.97552912664796 W
[codecarbon INFO @ 14:05:29] Energy consumed for all CPUs : 0.008682 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:05:29] 0.044269 kWh of electricity used since the beginning.
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2701/3432 [12:24<03:08,  3.87it/s][codecarbon INFO @ 14:05:44] Energy consumed for RAM : 0.003683 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:05:44] Energy consumed for all GPUs : 0.032644 kWh. Total GPU Power : 159.8040058591254 W
[codecarbon INFO @ 14:05:44] Energy consumed for all CPUs : 0.008859 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:05:44] 0.045186 kWh of electricity used since the beginning.
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2759/3432 [12:39<02:53,  3.88it/s][codecarbon INFO @ 14:05:59] Energy consumed for RAM : 0.003756 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:05:59] Energy consumed for all GPUs : 0.033311 kWh. Total GPU Power : 159.97271308830528 W
[codecarbon INFO @ 14:05:59] Energy consumed for all CPUs : 0.009036 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:05:59] 0.046103 kWh of electricity used since the beginning.
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2818/3432 [12:54<02:38,  3.87it/s][codecarbon INFO @ 14:06:14] Energy consumed for RAM : 0.003830 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:06:14] Energy consumed for all GPUs : 0.033977 kWh. Total GPU Power : 159.78295464982318 W
[codecarbon INFO @ 14:06:14] Energy consumed for all CPUs : 0.009213 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:06:14] 0.047020 kWh of electricity used since the beginning.
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2860/3432 [13:05<02:25,  3.94it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 9152
  Batch size = 64

  0%|          | 0/143 [00:00<?, ?it/s]
  2%|â–         | 3/143 [00:00<00:08, 17.30it/s]
  3%|â–Ž         | 5/143 [00:00<00:09, 14.30it/s]
  5%|â–         | 7/143 [00:00<00:10, 13.18it/s]
  6%|â–‹         | 9/143 [00:00<00:10, 12.77it/s]
  8%|â–Š         | 11/143 [00:00<00:10, 12.43it/s]
  9%|â–‰         | 13/143 [00:01<00:10, 12.35it/s]
 10%|â–ˆ         | 15/143 [00:01<00:10, 12.18it/s]
 12%|â–ˆâ–        | 17/143 [00:01<00:11, 10.99it/s]
 13%|â–ˆâ–Ž        | 19/143 [00:01<00:12, 10.25it/s]
 15%|â–ˆâ–        | 21/143 [00:01<00:12,  9.84it/s]
 16%|â–ˆâ–Œ        | 23/143 [00:02<00:12,  9.56it/s]
 17%|â–ˆâ–‹        | 24/143 [00:02<00:12,  9.45it/s]
 17%|â–ˆâ–‹        | 25/143 [00:02<00:12,  9.36it/s]
 18%|â–ˆâ–Š        | 26/143 [00:02<00:13,  8.86it/s]
 19%|â–ˆâ–‰        | 27/143 [00:02<00:13,  8.83it/s]
 20%|â–ˆâ–‰        | 28/143 [00:02<00:12,  8.88it/s]
 20%|â–ˆâ–ˆ        | 29/143 [00:02<00:12,  8.85it/s]
 21%|â–ˆâ–ˆ        | 30/143 [00:02<00:12,  8.87it/s]
 22%|â–ˆâ–ˆâ–       | 31/143 [00:02<00:12,  8.79it/s]
 22%|â–ˆâ–ˆâ–       | 32/143 [00:03<00:12,  8.85it/s]
 24%|â–ˆâ–ˆâ–       | 34/143 [00:03<00:11,  9.54it/s]
 25%|â–ˆâ–ˆâ–Œ       | 36/143 [00:03<00:10, 10.00it/s]
 27%|â–ˆâ–ˆâ–‹       | 38/143 [00:03<00:10, 10.23it/s]
 28%|â–ˆâ–ˆâ–Š       | 40/143 [00:03<00:09, 10.46it/s]
 29%|â–ˆâ–ˆâ–‰       | 42/143 [00:04<00:09, 10.51it/s][codecarbon INFO @ 14:06:29] Energy consumed for RAM : 0.003904 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:06:29] Energy consumed for all GPUs : 0.034646 kWh. Total GPU Power : 160.66996860283743 W
[codecarbon INFO @ 14:06:29] Energy consumed for all CPUs : 0.009390 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:06:29] 0.047940 kWh of electricity used since the beginning.

 31%|â–ˆâ–ˆâ–ˆ       | 44/143 [00:04<00:09, 10.61it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 46/143 [00:04<00:09, 10.54it/s]
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 48/143 [00:04<00:08, 11.00it/s]
 35%|â–ˆâ–ˆâ–ˆâ–      | 50/143 [00:04<00:07, 11.64it/s]
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 52/143 [00:04<00:07, 12.25it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 54/143 [00:05<00:07, 12.65it/s]
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 56/143 [00:05<00:06, 12.93it/s]
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 58/143 [00:05<00:06, 13.12it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 60/143 [00:05<00:06, 13.29it/s]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 62/143 [00:05<00:06, 13.35it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 64/143 [00:05<00:06, 13.09it/s]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 66/143 [00:05<00:06, 12.83it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 68/143 [00:06<00:05, 12.73it/s]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 70/143 [00:06<00:05, 12.59it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 72/143 [00:06<00:05, 12.53it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 74/143 [00:06<00:05, 12.44it/s]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 76/143 [00:06<00:05, 12.48it/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78/143 [00:06<00:05, 12.35it/s]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 80/143 [00:07<00:05, 11.81it/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 82/143 [00:07<00:05, 11.41it/s]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 84/143 [00:07<00:05, 11.23it/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 86/143 [00:07<00:05, 11.02it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 88/143 [00:07<00:05, 10.95it/s]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 90/143 [00:08<00:04, 10.81it/s]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 92/143 [00:08<00:04, 10.78it/s]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 94/143 [00:08<00:04, 10.72it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 96/143 [00:08<00:04, 10.77it/s]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 98/143 [00:08<00:04, 10.78it/s]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 100/143 [00:08<00:03, 10.83it/s]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 102/143 [00:09<00:03, 10.79it/s]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 104/143 [00:09<00:03, 10.84it/s]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 106/143 [00:09<00:03, 10.82it/s]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 108/143 [00:09<00:03, 10.85it/s]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 110/143 [00:09<00:03, 10.85it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 112/143 [00:10<00:02, 11.66it/s]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 114/143 [00:10<00:02, 12.23it/s]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 116/143 [00:10<00:02, 12.71it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 118/143 [00:10<00:01, 13.02it/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 120/143 [00:10<00:01, 13.34it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 122/143 [00:10<00:01, 13.48it/s]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 124/143 [00:10<00:01, 13.69it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 126/143 [00:11<00:01, 13.67it/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 128/143 [00:11<00:01, 13.70it/s]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 130/143 [00:11<00:00, 13.63it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 132/143 [00:11<00:00, 13.69it/s]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 134/143 [00:11<00:00, 13.68it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 136/143 [00:11<00:00, 13.67it/s]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 138/143 [00:11<00:00, 13.61it/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 140/143 [00:12<00:00, 13.67it/s]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 142/143 [00:12<00:00, 13.79it/s]
                                                   
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2860/3432 [13:17<02:25,  3.94it/s]
                                                 Saving model checkpoint to model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2860
Configuration saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2860\config.json
{'eval_loss': 0.5850991010665894, 'eval_precision': 0.8183195927156076, 'eval_recall': 0.8171426572080859, 'eval_f1': 0.817723781103469, 'eval_balanced accuracy': 0.8171426572080859, 'eval_runtime': 12.3392, 'eval_samples_per_second': 741.701, 'eval_steps_per_second': 11.589, 'epoch': 5.0}
Model weights saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2860\model.safetensors
tokenizer config file saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2860\tokenizer_config.json
Special tokens file saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2860\special_tokens_map.json
Deleting older checkpoint [model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2288] due to args.save_total_limit
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2886/3432 [13:24<02:21,  3.87it/s][codecarbon INFO @ 14:06:44] Energy consumed for RAM : 0.003977 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:06:44] Energy consumed for all GPUs : 0.035315 kWh. Total GPU Power : 160.46316420934508 W
[codecarbon INFO @ 14:06:44] Energy consumed for all CPUs : 0.009568 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:06:44] 0.048860 kWh of electricity used since the beginning.
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2944/3432 [13:39<02:05,  3.88it/s][codecarbon INFO @ 14:06:59] Energy consumed for RAM : 0.004051 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:06:59] Energy consumed for all GPUs : 0.035982 kWh. Total GPU Power : 160.09126672262386 W
[codecarbon INFO @ 14:06:59] Energy consumed for all CPUs : 0.009745 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:06:59] 0.049778 kWh of electricity used since the beginning.
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3000/3432 [13:53<01:51,  3.87it/s]{'loss': 0.0686, 'grad_norm': 12.209943771362305, 'learning_rate': 2.517482517482518e-06, 'epoch': 5.24}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3002/3432 [13:54<01:50,  3.89it/s][codecarbon INFO @ 14:07:14] Energy consumed for RAM : 0.004124 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:07:14] Energy consumed for all GPUs : 0.036650 kWh. Total GPU Power : 160.1572022445593 W
[codecarbon INFO @ 14:07:14] Energy consumed for all CPUs : 0.009922 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:07:14] 0.050696 kWh of electricity used since the beginning.
[codecarbon INFO @ 14:07:14] 0.014537 g.CO2eq/s mean an estimation of 458.4263129708287 kg.CO2eq/year
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3061/3432 [14:09<01:32,  4.00it/s][codecarbon INFO @ 14:07:29] Energy consumed for RAM : 0.004198 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:07:29] Energy consumed for all GPUs : 0.037317 kWh. Total GPU Power : 160.0157666861431 W
[codecarbon INFO @ 14:07:29] Energy consumed for all CPUs : 0.010099 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:07:29] 0.051615 kWh of electricity used since the beginning.
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3119/3432 [14:24<01:20,  3.87it/s][codecarbon INFO @ 14:07:44] Energy consumed for RAM : 0.004272 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:07:44] Energy consumed for all GPUs : 0.037984 kWh. Total GPU Power : 159.86105051061193 W
[codecarbon INFO @ 14:07:44] Energy consumed for all CPUs : 0.010276 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:07:44] 0.052532 kWh of electricity used since the beginning.
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3178/3432 [14:39<01:04,  3.92it/s][codecarbon INFO @ 14:08:00] Energy consumed for RAM : 0.004345 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:08:00] Energy consumed for all GPUs : 0.038651 kWh. Total GPU Power : 160.11701954888656 W
[codecarbon INFO @ 14:08:00] Energy consumed for all CPUs : 0.010454 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:08:00] 0.053450 kWh of electricity used since the beginning.
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3236/3432 [14:54<00:49,  3.96it/s][codecarbon INFO @ 14:08:15] Energy consumed for RAM : 0.004419 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:08:15] Energy consumed for all GPUs : 0.039318 kWh. Total GPU Power : 160.1209895869959 W
[codecarbon INFO @ 14:08:15] Energy consumed for all CPUs : 0.010631 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:08:15] 0.054368 kWh of electricity used since the beginning.
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3295/3432 [15:09<00:35,  3.87it/s][codecarbon INFO @ 14:08:30] Energy consumed for RAM : 0.004493 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:08:30] Energy consumed for all GPUs : 0.039985 kWh. Total GPU Power : 159.91577002908815 W
[codecarbon INFO @ 14:08:30] Energy consumed for all CPUs : 0.010808 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:08:30] 0.055286 kWh of electricity used since the beginning.
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3353/3432 [15:24<00:20,  3.90it/s][codecarbon INFO @ 14:08:45] Energy consumed for RAM : 0.004566 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:08:45] Energy consumed for all GPUs : 0.040653 kWh. Total GPU Power : 160.19823120562668 W
[codecarbon INFO @ 14:08:45] Energy consumed for all CPUs : 0.010985 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:08:45] 0.056204 kWh of electricity used since the beginning.
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3412/3432 [15:39<00:05,  3.89it/s][codecarbon INFO @ 14:09:00] Energy consumed for RAM : 0.004640 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:09:00] Energy consumed for all GPUs : 0.041320 kWh. Total GPU Power : 160.11247575556428 W
[codecarbon INFO @ 14:09:00] Energy consumed for all CPUs : 0.011162 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:09:00] 0.057123 kWh of electricity used since the beginning.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3432/3432 [15:44<00:00,  3.92it/s]Saving model checkpoint to model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432
Configuration saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432\config.json
Model weights saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432\model.safetensors
tokenizer config file saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432\tokenizer_config.json
Special tokens file saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432\special_tokens_map.json
Deleting older checkpoint [model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2860] due to args.save_total_limit
The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 9152
  Batch size = 64

  0%|          | 0/143 [00:00<?, ?it/s]
  2%|â–         | 3/143 [00:00<00:08, 16.59it/s]
  3%|â–Ž         | 5/143 [00:00<00:09, 13.91it/s]
  5%|â–         | 7/143 [00:00<00:10, 13.16it/s]
  6%|â–‹         | 9/143 [00:00<00:10, 12.65it/s]
  8%|â–Š         | 11/143 [00:00<00:10, 12.49it/s]
  9%|â–‰         | 13/143 [00:01<00:10, 12.29it/s]
 10%|â–ˆ         | 15/143 [00:01<00:10, 12.19it/s]
 12%|â–ˆâ–        | 17/143 [00:01<00:11, 10.92it/s]
 13%|â–ˆâ–Ž        | 19/143 [00:01<00:12, 10.23it/s]
 15%|â–ˆâ–        | 21/143 [00:01<00:12,  9.81it/s]
 15%|â–ˆâ–Œ        | 22/143 [00:01<00:12,  9.65it/s]
 16%|â–ˆâ–Œ        | 23/143 [00:02<00:12,  9.49it/s]
 17%|â–ˆâ–‹        | 24/143 [00:02<00:12,  9.39it/s]
 17%|â–ˆâ–‹        | 25/143 [00:02<00:12,  9.24it/s]
 18%|â–ˆâ–Š        | 26/143 [00:02<00:12,  9.18it/s]
 19%|â–ˆâ–‰        | 27/143 [00:02<00:12,  9.04it/s]
 20%|â–ˆâ–‰        | 28/143 [00:02<00:12,  9.03it/s]
 20%|â–ˆâ–ˆ        | 29/143 [00:02<00:12,  8.98it/s]
 21%|â–ˆâ–ˆ        | 30/143 [00:02<00:12,  8.96it/s]
 22%|â–ˆâ–ˆâ–       | 31/143 [00:02<00:12,  8.95it/s]
 22%|â–ˆâ–ˆâ–       | 32/143 [00:03<00:12,  8.98it/s]
 24%|â–ˆâ–ˆâ–       | 34/143 [00:03<00:11,  9.67it/s]
 25%|â–ˆâ–ˆâ–Œ       | 36/143 [00:03<00:10, 10.10it/s]
 27%|â–ˆâ–ˆâ–‹       | 38/143 [00:03<00:10, 10.28it/s]
 28%|â–ˆâ–ˆâ–Š       | 40/143 [00:03<00:09, 10.48it/s]
 29%|â–ˆâ–ˆâ–‰       | 42/143 [00:04<00:09, 10.54it/s]
 31%|â–ˆâ–ˆâ–ˆ       | 44/143 [00:04<00:09, 10.63it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 46/143 [00:04<00:09, 10.63it/s]
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 48/143 [00:04<00:08, 11.07it/s]
 35%|â–ˆâ–ˆâ–ˆâ–      | 50/143 [00:04<00:07, 11.70it/s]
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 52/143 [00:04<00:07, 12.24it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 54/143 [00:04<00:07, 12.55it/s]
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 56/143 [00:05<00:06, 12.92it/s]
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 58/143 [00:05<00:06, 13.05it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 60/143 [00:05<00:06, 13.25it/s]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 62/143 [00:05<00:06, 13.32it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 64/143 [00:05<00:06, 13.08it/s]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 66/143 [00:05<00:06, 12.78it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 68/143 [00:06<00:05, 12.72it/s]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 70/143 [00:06<00:05, 12.56it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 72/143 [00:06<00:05, 12.55it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 74/143 [00:06<00:05, 12.46it/s]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 76/143 [00:06<00:05, 12.43it/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78/143 [00:06<00:05, 12.35it/s]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 80/143 [00:07<00:05, 11.83it/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 82/143 [00:07<00:05, 11.43it/s]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 84/143 [00:07<00:05, 11.16it/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 86/143 [00:07<00:05, 10.94it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 88/143 [00:07<00:05, 10.88it/s]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 90/143 [00:08<00:04, 10.78it/s]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 92/143 [00:08<00:04, 10.77it/s]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 94/143 [00:08<00:04, 10.70it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 96/143 [00:08<00:04, 10.77it/s]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 98/143 [00:08<00:04, 10.76it/s]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 100/143 [00:08<00:03, 10.80it/s]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 102/143 [00:09<00:03, 10.76it/s]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 104/143 [00:09<00:03, 10.83it/s]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 106/143 [00:09<00:03, 10.79it/s][codecarbon INFO @ 14:09:15] Energy consumed for RAM : 0.004714 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:09:15] Energy consumed for all GPUs : 0.041995 kWh. Total GPU Power : 161.76551038635122 W
[codecarbon INFO @ 14:09:15] Energy consumed for all CPUs : 0.011339 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:09:15] 0.058048 kWh of electricity used since the beginning.
[codecarbon INFO @ 14:09:15] 0.014546 g.CO2eq/s mean an estimation of 458.7327807208484 kg.CO2eq/year

 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 108/143 [00:09<00:03, 10.83it/s]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 110/143 [00:09<00:03, 10.65it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 112/143 [00:10<00:02, 11.50it/s]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 114/143 [00:10<00:02, 12.11it/s]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 116/143 [00:10<00:02, 12.65it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 118/143 [00:10<00:01, 12.97it/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 120/143 [00:10<00:01, 13.33it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 122/143 [00:10<00:01, 13.48it/s]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 124/143 [00:10<00:01, 13.68it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 126/143 [00:11<00:01, 13.65it/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 128/143 [00:11<00:01, 13.68it/s]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 130/143 [00:11<00:00, 13.57it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 132/143 [00:11<00:00, 13.62it/s]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 134/143 [00:11<00:00, 13.56it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 136/143 [00:11<00:00, 13.64it/s]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 138/143 [00:11<00:00, 13.58it/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 140/143 [00:12<00:00, 13.64it/s]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 142/143 [00:12<00:00, 13.78it/s]
{'eval_loss': 0.7602384686470032, 'eval_precision': 0.816903211963274, 'eval_recall': 0.8155343485854407, 'eval_f1': 0.8162087996595184, 'eval_balanced accuracy': 0.8155343485854407, 'eval_runtime': 12.3463, 'eval_samples_per_second': 741.277, 'eval_steps_per_second': 11.582, 'epoch': 6.0}
                                                   
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3432/3432 [15:56<00:00,  3.92it/s]
                                                 Saving model checkpoint to model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432
Configuration saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432\config.json
Model weights saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432\model.safetensors
tokenizer config file saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432\tokenizer_config.json
Special tokens file saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432\special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1144 (score: 0.36263352632522583).
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3432/3432 [15:57<00:00,  3.92it/s]Deleting older checkpoint [model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432] due to args.save_total_limit
[codecarbon WARNING @ 14:09:17] Another instance of codecarbon is already running. Exiting.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3432/3432 [15:57<00:00,  3.59it/s]
Saving model checkpoint to model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained
Configuration saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\config.json
{'train_runtime': 957.1432, 'train_samples_per_second': 229.483, 'train_steps_per_second': 3.586, 'train_loss': 0.21900712471186143, 'epoch': 6.0}
Model weights saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\model.safetensors
tokenizer config file saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\tokenizer_config.json
Special tokens file saved in model_output_albertv2\merged_winoqueer_seegull_gpt_augmentation_trained\special_tokens_map.json
[codecarbon INFO @ 14:09:17] Energy consumed for RAM : 0.004728 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:09:17] Energy consumed for all GPUs : 0.042114 kWh. Total GPU Power : 151.11186241957452 W
[codecarbon INFO @ 14:09:17] Energy consumed for all CPUs : 0.011373 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:09:17] 0.058214 kWh of electricity used since the beginning.
loading configuration file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\config.json
Model config AlbertConfig {
  "_name_or_path": "model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

loading weights file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\model.safetensors
Estimated total emissions: 0.013831096459847337 kg CO2
Number of unique labels: 2
All model checkpoint weights were used when initializing AlbertForSequenceClassification.

All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
loading file spiece.model
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 653/653 [00:00<00:00, 17351.82 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 653/653 [00:00<00:00, 15508.25 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3236, 'input_ids': [2, 127, 148, 72, 50, 3398, 50, 19196, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
loading configuration file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\config.json
Model config AlbertConfig {
  "_name_or_path": "model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

loading weights file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\model.safetensors
Number of unique labels: 2
All model checkpoint weights were used when initializing AlbertForSequenceClassification.

All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
loading file spiece.model
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 414/414 [00:00<00:00, 20640.72 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 414/414 [00:00<00:00, 14766.29 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 840, 'input_ids': [2, 14, 16330, 8790, 46, 14348, 28, 367, 26692, 108, 1427, 34, 109, 3, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'labels': 0}
Number of unique labels: 2
loading configuration file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\config.json
Model config AlbertConfig {
  "_name_or_path": "model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

loading weights file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\model.safetensors
All model checkpoint weights were used when initializing AlbertForSequenceClassification.

All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
loading file spiece.model
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10374/10374 [00:00<00:00, 18236.69 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10374/10374 [00:00<00:00, 18543.73 examples/s]
Sample tokenized input from test: {'group': 'race', 'text': 'When i see an arab, i think they are a happy.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 17851, 'input_ids': [2, 76, 31, 196, 40, 3666, 15, 31, 277, 59, 50, 21, 1700, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
loading configuration file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\config.json
Model config AlbertConfig {
  "_name_or_path": "model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

loading weights file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\model.safetensors
Number of unique labels: 2
All model checkpoint weights were used when initializing AlbertForSequenceClassification.

All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
loading file spiece.model
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11441/11441 [00:00<00:00, 17592.27 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11441/11441 [00:00<00:00, 18419.25 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [2, 14, 16330, 8790, 46, 14348, 28, 367, 26692, 108, 1427, 34, 109, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
Number of unique labels: 2
[codecarbon INFO @ 14:17:32] [setup] RAM Tracking...
[codecarbon INFO @ 14:17:32] [setup] GPU Tracking...
[codecarbon INFO @ 14:17:32] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 14:17:32] [setup] CPU Tracking...
[codecarbon WARNING @ 14:17:32] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon WARNING @ 14:17:35] We saw that you have a AMD Ryzen 7 7700 8-Core Processor but we don't know it. Please contact us.
[codecarbon INFO @ 14:17:35] CPU Model on constant consumption mode: AMD Ryzen 7 7700 8-Core Processor
[codecarbon INFO @ 14:17:35] >>> Tracker's metadata:
[codecarbon INFO @ 14:17:35]   Platform system: Windows-11-10.0.26100-SP0
[codecarbon INFO @ 14:17:35]   Python version: 3.12.12
[codecarbon INFO @ 14:17:35]   CodeCarbon version: 2.8.0
[codecarbon INFO @ 14:17:35]   Available RAM : 47.116 GB
[codecarbon INFO @ 14:17:35]   CPU count: 16
[codecarbon INFO @ 14:17:35]   CPU model: AMD Ryzen 7 7700 8-Core Processor
[codecarbon INFO @ 14:17:35]   GPU count: 1
[codecarbon INFO @ 14:17:35]   GPU model: 1 x NVIDIA GeForce RTX 5060 Ti
[codecarbon INFO @ 14:17:38] Saving emissions data to file D:\UCL\HEARTS-Text-Stereotype-Detection-main\Model Training and Evaluation\emissions.csv
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--distilbert--distilbert-base-uncased\snapshots\12040accade4e8a0f71eabdb258fecc2e7e948be\config.json
Model config DistilBertConfig {
  "_name_or_path": "distilbert/distilbert-base-uncased",
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.46.3",
  "vocab_size": 30522
}

loading weights file model.safetensors from cache at C:\Users\32618\.cache\huggingface\hub\models--distilbert--distilbert-base-uncased\snapshots\12040accade4e8a0f71eabdb258fecc2e7e948be\model.safetensors
Some weights of the model checkpoint at distilbert/distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--distilbert--distilbert-base-uncased\snapshots\12040accade4e8a0f71eabdb258fecc2e7e948be\config.json
Model config DistilBertConfig {
  "_name_or_path": "distilbert/distilbert-base-uncased",
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.46.3",
  "vocab_size": 30522
}

loading file vocab.txt from cache at C:\Users\32618\.cache\huggingface\hub\models--distilbert--distilbert-base-uncased\snapshots\12040accade4e8a0f71eabdb258fecc2e7e948be\vocab.txt
loading file tokenizer.json from cache at C:\Users\32618\.cache\huggingface\hub\models--distilbert--distilbert-base-uncased\snapshots\12040accade4e8a0f71eabdb258fecc2e7e948be\tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--distilbert--distilbert-base-uncased\snapshots\12040accade4e8a0f71eabdb258fecc2e7e948be\tokenizer_config.json
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--distilbert--distilbert-base-uncased\snapshots\12040accade4e8a0f71eabdb258fecc2e7e948be\config.json
Model config DistilBertConfig {
  "_name_or_path": "distilbert/distilbert-base-uncased",
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.46.3",
  "vocab_size": 30522
}

Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33194/33194 [00:01<00:00, 21417.05 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33194/33194 [00:01<00:00, 19924.93 examples/s]
Sample tokenized input from train: {'group': 'religion', 'text': 'Have you ever met someone named Mohammed? I need a new pair of pajamas.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 36953, 'input_ids': [101, 2031, 2017, 2412, 2777, 2619, 2315, 12619, 1029, 1045, 2342, 1037, 2047, 3940, 1997, 27621, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8299/8299 [00:00<00:00, 23285.88 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8299/8299 [00:00<00:00, 16346.80 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
PyTorch: setting up devices
Sample tokenized input from validation: {'group': 'religion', 'text': 'Have you ever met someone named Mohammed? I need a new pair of pajamas.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 36953, 'input_ids': [101, 2031, 2017, 2412, 2777, 2619, 2315, 12619, 1029, 1045, 2342, 1037, 2047, 3940, 1997, 27621, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
D:\UCL\HEARTS-Text-Stereotype-Detection-main\Model Training and Evaluation\BERT_Models_Fine_Tuning.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
[codecarbon ERROR @ 14:17:53] Error: Another instance of codecarbon is probably running as we find `C:\Users\32618\AppData\Local\Temp\.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.
The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running training *****
  Num examples = 33,194
  Num Epochs = 6
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 3,114
  Number of trainable parameters = 66,955,010
[codecarbon WARNING @ 14:17:53] Another instance of codecarbon is already running. Exiting.
  0%|          | 0/3114 [00:00<?, ?it/s][codecarbon INFO @ 14:17:53] Energy consumed for RAM : 0.000074 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:17:53] Energy consumed for all GPUs : 0.000073 kWh. Total GPU Power : 17.41907267467747 W
[codecarbon INFO @ 14:17:53] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:17:53] 0.000323 kWh of electricity used since the beginning.
  4%|â–Ž         | 115/3114 [00:15<06:36,  7.55it/s][codecarbon INFO @ 14:18:08] Energy consumed for RAM : 0.000147 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:18:08] Energy consumed for all GPUs : 0.000690 kWh. Total GPU Power : 148.14179105739345 W
[codecarbon INFO @ 14:18:08] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:18:08] 0.001192 kWh of electricity used since the beginning.
  7%|â–‹         | 231/3114 [00:30<06:09,  7.81it/s][codecarbon INFO @ 14:18:23] Energy consumed for RAM : 0.000221 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:18:23] Energy consumed for all GPUs : 0.001323 kWh. Total GPU Power : 151.63368765622786 W
[codecarbon INFO @ 14:18:23] Energy consumed for all CPUs : 0.000532 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:18:23] 0.002075 kWh of electricity used since the beginning.
 11%|â–ˆ         | 347/3114 [00:45<05:45,  8.01it/s][codecarbon INFO @ 14:18:38] Energy consumed for RAM : 0.000295 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:18:38] Energy consumed for all GPUs : 0.001961 kWh. Total GPU Power : 153.215859733656 W
[codecarbon INFO @ 14:18:38] Energy consumed for all CPUs : 0.000709 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:18:38] 0.002965 kWh of electricity used since the beginning.
 15%|â–ˆâ–        | 464/3114 [01:00<05:43,  7.71it/s][codecarbon INFO @ 14:18:53] Energy consumed for RAM : 0.000368 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:18:53] Energy consumed for all GPUs : 0.002604 kWh. Total GPU Power : 154.03170893452466 W
[codecarbon INFO @ 14:18:53] Energy consumed for all CPUs : 0.000886 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:18:53] 0.003858 kWh of electricity used since the beginning.
 16%|â–ˆâ–Œ        | 500/3114 [01:04<05:40,  7.68it/s]{'loss': 0.4805, 'grad_norm': 2.2849361896514893, 'learning_rate': 1.678869621066153e-05, 'epoch': 0.96}
 17%|â–ˆâ–‹        | 518/3114 [01:07<05:41,  7.59it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 8299
  Batch size = 64

  0%|          | 0/130 [00:00<?, ?it/s]
  4%|â–         | 5/130 [00:00<00:03, 39.85it/s]
  7%|â–‹         | 9/130 [00:00<00:03, 36.08it/s]
 10%|â–ˆ         | 13/130 [00:00<00:03, 34.65it/s]
 13%|â–ˆâ–Ž        | 17/130 [00:00<00:03, 33.06it/s]
 16%|â–ˆâ–Œ        | 21/130 [00:00<00:03, 30.72it/s]
 19%|â–ˆâ–‰        | 25/130 [00:00<00:03, 29.41it/s]
 22%|â–ˆâ–ˆâ–       | 28/130 [00:00<00:03, 28.75it/s]
 24%|â–ˆâ–ˆâ–       | 31/130 [00:01<00:03, 28.58it/s]
 26%|â–ˆâ–ˆâ–Œ       | 34/130 [00:01<00:03, 28.25it/s]
 28%|â–ˆâ–ˆâ–Š       | 37/130 [00:01<00:03, 28.03it/s]
 31%|â–ˆâ–ˆâ–ˆ       | 40/130 [00:01<00:03, 28.01it/s]
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 43/130 [00:01<00:03, 27.93it/s]
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 46/130 [00:01<00:03, 27.92it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 49/130 [00:01<00:02, 28.05it/s]
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 52/130 [00:01<00:02, 28.15it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 55/130 [00:01<00:02, 28.01it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 58/130 [00:01<00:02, 28.48it/s]
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 61/130 [00:02<00:02, 28.59it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 65/130 [00:02<00:02, 29.12it/s]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 69/130 [00:02<00:02, 30.27it/s]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 73/130 [00:02<00:01, 31.01it/s]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 77/130 [00:02<00:01, 31.56it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 81/130 [00:02<00:01, 31.58it/s]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 85/130 [00:02<00:01, 31.94it/s]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 89/130 [00:02<00:01, 31.97it/s]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 93/130 [00:03<00:01, 32.06it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 97/130 [00:03<00:01, 31.51it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 101/130 [00:03<00:00, 31.57it/s]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 105/130 [00:03<00:00, 30.97it/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 109/130 [00:03<00:00, 30.48it/s]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 113/130 [00:03<00:00, 28.07it/s]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 116/130 [00:03<00:00, 27.35it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 119/130 [00:04<00:00, 26.65it/s]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 122/130 [00:04<00:00, 26.24it/s]
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 125/130 [00:04<00:00, 25.81it/s]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 129/130 [00:04<00:00, 27.93it/s]
                                                  
 17%|â–ˆâ–‹        | 519/3114 [01:11<05:41,  7.59it/s]
                                                 Saving model checkpoint to model_output_distilbert\mgsd_trained\checkpoint-519
Configuration saved in model_output_distilbert\mgsd_trained\checkpoint-519\config.json
{'eval_loss': 0.4141310453414917, 'eval_precision': 0.7690004252504252, 'eval_recall': 0.7752678589798984, 'eval_f1': 0.7718257932488111, 'eval_balanced accuracy': 0.7752678589798984, 'eval_runtime': 4.4344, 'eval_samples_per_second': 1871.484, 'eval_steps_per_second': 29.316, 'epoch': 1.0}
Model weights saved in model_output_distilbert\mgsd_trained\checkpoint-519\model.safetensors
tokenizer config file saved in model_output_distilbert\mgsd_trained\checkpoint-519\tokenizer_config.json
Special tokens file saved in model_output_distilbert\mgsd_trained\checkpoint-519\special_tokens_map.json
 17%|â–ˆâ–‹        | 540/3114 [01:14<05:37,  7.64it/s][codecarbon INFO @ 14:19:08] Energy consumed for RAM : 0.000442 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:19:08] Energy consumed for all GPUs : 0.003239 kWh. Total GPU Power : 152.29362651955415 W
[codecarbon INFO @ 14:19:08] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:19:08] 0.004744 kWh of electricity used since the beginning.
 21%|â–ˆâ–ˆ        | 653/3114 [01:29<05:27,  7.51it/s][codecarbon INFO @ 14:19:23] Energy consumed for RAM : 0.000516 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:19:23] Energy consumed for all GPUs : 0.003872 kWh. Total GPU Power : 152.06726349762255 W
[codecarbon INFO @ 14:19:23] Energy consumed for all CPUs : 0.001240 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:19:23] 0.005628 kWh of electricity used since the beginning.
 25%|â–ˆâ–ˆâ–       | 769/3114 [01:45<05:11,  7.52it/s][codecarbon INFO @ 14:19:38] Energy consumed for RAM : 0.000589 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:19:38] Energy consumed for all GPUs : 0.004516 kWh. Total GPU Power : 154.38125692146977 W
[codecarbon INFO @ 14:19:38] Energy consumed for all CPUs : 0.001418 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:19:38] 0.006523 kWh of electricity used since the beginning.
[codecarbon INFO @ 14:19:38] 0.012906 g.CO2eq/s mean an estimation of 407.0065250624594 kg.CO2eq/year
 28%|â–ˆâ–ˆâ–Š       | 884/3114 [02:00<04:54,  7.57it/s][codecarbon INFO @ 14:19:53] Energy consumed for RAM : 0.000663 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:19:53] Energy consumed for all GPUs : 0.005159 kWh. Total GPU Power : 154.2723521633415 W
[codecarbon INFO @ 14:19:53] Energy consumed for all CPUs : 0.001595 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:19:53] 0.007417 kWh of electricity used since the beginning.
 32%|â–ˆâ–ˆâ–ˆâ–      | 999/3114 [02:15<04:18,  8.17it/s][codecarbon INFO @ 14:20:08] Energy consumed for RAM : 0.000737 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:20:08] Energy consumed for all GPUs : 0.005805 kWh. Total GPU Power : 155.05854261499667 W
[codecarbon INFO @ 14:20:08] Energy consumed for all CPUs : 0.001772 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:20:08] 0.008314 kWh of electricity used since the beginning.
 32%|â–ˆâ–ˆâ–ˆâ–      | 1000/3114 [02:15<04:20,  8.11it/s]{'loss': 0.3585, 'grad_norm': 2.6896345615386963, 'learning_rate': 1.357739242132306e-05, 'epoch': 1.93}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1037/3114 [02:19<04:20,  7.98it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 8299
  Batch size = 64

  0%|          | 0/130 [00:00<?, ?it/s]
  4%|â–         | 5/130 [00:00<00:02, 42.14it/s]
  8%|â–Š         | 10/130 [00:00<00:03, 35.83it/s]
 11%|â–ˆ         | 14/130 [00:00<00:03, 34.45it/s]
 14%|â–ˆâ–        | 18/130 [00:00<00:03, 31.45it/s]
 17%|â–ˆâ–‹        | 22/130 [00:00<00:03, 30.12it/s]
 20%|â–ˆâ–ˆ        | 26/130 [00:00<00:03, 28.91it/s]
 22%|â–ˆâ–ˆâ–       | 29/130 [00:00<00:03, 28.31it/s]
 25%|â–ˆâ–ˆâ–       | 32/130 [00:01<00:03, 28.32it/s]
 27%|â–ˆâ–ˆâ–‹       | 35/130 [00:01<00:03, 28.10it/s]
 29%|â–ˆâ–ˆâ–‰       | 38/130 [00:01<00:03, 28.20it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 41/130 [00:01<00:03, 28.60it/s]
 34%|â–ˆâ–ˆâ–ˆâ–      | 44/130 [00:01<00:03, 28.53it/s]
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 47/130 [00:01<00:02, 28.79it/s]
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 51/130 [00:01<00:02, 29.42it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 55/130 [00:01<00:02, 29.73it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 59/130 [00:01<00:02, 30.19it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 63/130 [00:02<00:02, 30.12it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 67/130 [00:02<00:02, 31.41it/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 71/130 [00:02<00:01, 32.11it/s]
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 75/130 [00:02<00:01, 32.62it/s]
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 79/130 [00:02<00:01, 32.84it/s]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 83/130 [00:02<00:01, 33.20it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 87/130 [00:02<00:01, 33.19it/s]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 91/130 [00:02<00:01, 33.31it/s]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 95/130 [00:03<00:01, 32.99it/s]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 99/130 [00:03<00:00, 32.94it/s]
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 103/130 [00:03<00:00, 32.72it/s]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 107/130 [00:03<00:00, 32.60it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 111/130 [00:03<00:00, 30.69it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 115/130 [00:03<00:00, 28.23it/s]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 118/130 [00:03<00:00, 27.17it/s]
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 121/130 [00:03<00:00, 26.72it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 124/130 [00:04<00:00, 26.33it/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 128/130 [00:04<00:00, 28.01it/s]
{'eval_loss': 0.3917471468448639, 'eval_precision': 0.7952261959294075, 'eval_recall': 0.7856807964528938, 'eval_f1': 0.7899475617199119, 'eval_balanced accuracy': 0.7856807964528938, 'eval_runtime': 4.3145, 'eval_samples_per_second': 1923.493, 'eval_steps_per_second': 30.131, 'epoch': 2.0}
                                                   
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1038/3114 [02:24<04:20,  7.98it/s]
                                                 Saving model checkpoint to model_output_distilbert\mgsd_trained\checkpoint-1038
Configuration saved in model_output_distilbert\mgsd_trained\checkpoint-1038\config.json
Model weights saved in model_output_distilbert\mgsd_trained\checkpoint-1038\model.safetensors
tokenizer config file saved in model_output_distilbert\mgsd_trained\checkpoint-1038\tokenizer_config.json
Special tokens file saved in model_output_distilbert\mgsd_trained\checkpoint-1038\special_tokens_map.json
Deleting older checkpoint [model_output_distilbert\mgsd_trained\checkpoint-519] due to args.save_total_limit
 35%|â–ˆâ–ˆâ–ˆâ–      | 1078/3114 [02:29<04:14,  7.99it/s][codecarbon INFO @ 14:20:23] Energy consumed for RAM : 0.000810 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:20:23] Energy consumed for all GPUs : 0.006458 kWh. Total GPU Power : 156.51140533727298 W
[codecarbon INFO @ 14:20:23] Energy consumed for all CPUs : 0.001949 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:20:23] 0.009217 kWh of electricity used since the beginning.
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1197/3114 [02:45<03:48,  8.40it/s][codecarbon INFO @ 14:20:38] Energy consumed for RAM : 0.000884 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:20:38] Energy consumed for all GPUs : 0.007119 kWh. Total GPU Power : 158.71120401426785 W
[codecarbon INFO @ 14:20:38] Energy consumed for all CPUs : 0.002126 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:20:38] 0.010129 kWh of electricity used since the beginning.
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1316/3114 [03:00<03:48,  7.87it/s][codecarbon INFO @ 14:20:53] Energy consumed for RAM : 0.000957 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:20:53] Energy consumed for all GPUs : 0.007782 kWh. Total GPU Power : 159.05214161337796 W
[codecarbon INFO @ 14:20:53] Energy consumed for all CPUs : 0.002303 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:20:53] 0.011043 kWh of electricity used since the beginning.
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1437/3114 [03:15<03:20,  8.37it/s][codecarbon INFO @ 14:21:08] Energy consumed for RAM : 0.001031 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:21:08] Energy consumed for all GPUs : 0.008452 kWh. Total GPU Power : 160.63476431406724 W
[codecarbon INFO @ 14:21:08] Energy consumed for all CPUs : 0.002481 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:21:08] 0.011964 kWh of electricity used since the beginning.
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1500/3114 [03:22<03:24,  7.90it/s]{'loss': 0.2809, 'grad_norm': 4.881559371948242, 'learning_rate': 1.0366088631984585e-05, 'epoch': 2.89}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1556/3114 [03:29<03:16,  7.91it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 8299
  Batch size = 64

  0%|          | 0/130 [00:00<?, ?it/s]
  4%|â–         | 5/130 [00:00<00:02, 42.88it/s][codecarbon INFO @ 14:21:23] Energy consumed for RAM : 0.001105 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:21:23] Energy consumed for all GPUs : 0.009122 kWh. Total GPU Power : 160.7980272057693 W
[codecarbon INFO @ 14:21:23] Energy consumed for all CPUs : 0.002658 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:21:23] 0.012885 kWh of electricity used since the beginning.

  8%|â–Š         | 10/130 [00:00<00:03, 36.70it/s]
 11%|â–ˆ         | 14/130 [00:00<00:03, 35.06it/s]
 14%|â–ˆâ–        | 18/130 [00:00<00:03, 33.33it/s]
 17%|â–ˆâ–‹        | 22/130 [00:00<00:03, 31.54it/s]
 20%|â–ˆâ–ˆ        | 26/130 [00:00<00:03, 30.79it/s]
 23%|â–ˆâ–ˆâ–Ž       | 30/130 [00:00<00:03, 30.05it/s]
 26%|â–ˆâ–ˆâ–Œ       | 34/130 [00:01<00:03, 29.88it/s]
 28%|â–ˆâ–ˆâ–Š       | 37/130 [00:01<00:03, 29.43it/s]
 31%|â–ˆâ–ˆâ–ˆ       | 40/130 [00:01<00:03, 29.38it/s]
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 43/130 [00:01<00:02, 29.49it/s]
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 46/130 [00:01<00:02, 28.90it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 50/130 [00:01<00:02, 29.43it/s]
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 53/130 [00:01<00:02, 29.45it/s]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 57/130 [00:01<00:02, 29.89it/s]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 60/130 [00:01<00:02, 29.88it/s]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 64/130 [00:02<00:02, 30.36it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 68/130 [00:02<00:01, 31.11it/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 72/130 [00:02<00:01, 32.07it/s]
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 76/130 [00:02<00:01, 32.30it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 80/130 [00:02<00:01, 32.86it/s]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 84/130 [00:02<00:01, 32.94it/s]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 88/130 [00:02<00:01, 33.18it/s]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 92/130 [00:02<00:01, 33.07it/s]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 96/130 [00:03<00:01, 33.00it/s]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 100/130 [00:03<00:00, 32.76it/s]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 104/130 [00:03<00:00, 32.63it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 108/130 [00:03<00:00, 32.44it/s]
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 112/130 [00:03<00:00, 30.83it/s]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 116/130 [00:03<00:00, 29.15it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 119/130 [00:03<00:00, 28.34it/s]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 122/130 [00:03<00:00, 27.60it/s]
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 125/130 [00:04<00:00, 27.25it/s]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 129/130 [00:04<00:00, 29.08it/s]
                                                   
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1557/3114 [03:34<03:16,  7.91it/s]
                                                 Saving model checkpoint to model_output_distilbert\mgsd_trained\checkpoint-1557
Configuration saved in model_output_distilbert\mgsd_trained\checkpoint-1557\config.json
{'eval_loss': 0.4208846092224121, 'eval_precision': 0.8007372654155496, 'eval_recall': 0.7925371119066724, 'eval_f1': 0.7962687642419601, 'eval_balanced accuracy': 0.7925371119066724, 'eval_runtime': 4.2398, 'eval_samples_per_second': 1957.405, 'eval_steps_per_second': 30.662, 'epoch': 3.0}
Model weights saved in model_output_distilbert\mgsd_trained\checkpoint-1557\model.safetensors
tokenizer config file saved in model_output_distilbert\mgsd_trained\checkpoint-1557\tokenizer_config.json
Special tokens file saved in model_output_distilbert\mgsd_trained\checkpoint-1557\special_tokens_map.json
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1638/3114 [03:45<03:01,  8.11it/s][codecarbon INFO @ 14:21:38] Energy consumed for RAM : 0.001178 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:21:38] Energy consumed for all GPUs : 0.009778 kWh. Total GPU Power : 157.2978295483376 W
[codecarbon INFO @ 14:21:38] Energy consumed for all CPUs : 0.002835 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:21:38] 0.013791 kWh of electricity used since the beginning.
[codecarbon INFO @ 14:21:38] 0.014384 g.CO2eq/s mean an estimation of 453.5981157068518 kg.CO2eq/year
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1759/3114 [04:00<02:48,  8.05it/s][codecarbon INFO @ 14:21:53] Energy consumed for RAM : 0.001252 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:21:53] Energy consumed for all GPUs : 0.010449 kWh. Total GPU Power : 161.08199874431483 W
[codecarbon INFO @ 14:21:53] Energy consumed for all CPUs : 0.003012 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:21:53] 0.014714 kWh of electricity used since the beginning.
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1878/3114 [04:15<02:34,  8.01it/s][codecarbon INFO @ 14:22:08] Energy consumed for RAM : 0.001326 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:22:08] Energy consumed for all GPUs : 0.011114 kWh. Total GPU Power : 159.4798748602495 W
[codecarbon INFO @ 14:22:08] Energy consumed for all CPUs : 0.003189 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:22:08] 0.015629 kWh of electricity used since the beginning.
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1998/3114 [04:30<02:16,  8.17it/s][codecarbon INFO @ 14:22:23] Energy consumed for RAM : 0.001399 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:22:23] Energy consumed for all GPUs : 0.011779 kWh. Total GPU Power : 159.42609370182922 W
[codecarbon INFO @ 14:22:23] Energy consumed for all CPUs : 0.003367 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:22:23] 0.016545 kWh of electricity used since the beginning.
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2000/3114 [04:30<02:21,  7.88it/s]{'loss': 0.2173, 'grad_norm': 5.257907867431641, 'learning_rate': 7.154784842646115e-06, 'epoch': 3.85}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2075/3114 [04:39<02:09,  8.02it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 8299
  Batch size = 64

  0%|          | 0/130 [00:00<?, ?it/s]
  4%|â–         | 5/130 [00:00<00:03, 40.53it/s]
  8%|â–Š         | 10/130 [00:00<00:03, 36.81it/s]
 11%|â–ˆ         | 14/130 [00:00<00:03, 35.57it/s]
 14%|â–ˆâ–        | 18/130 [00:00<00:03, 33.51it/s]
 17%|â–ˆâ–‹        | 22/130 [00:00<00:03, 31.54it/s]
 20%|â–ˆâ–ˆ        | 26/130 [00:00<00:03, 30.82it/s]
 23%|â–ˆâ–ˆâ–Ž       | 30/130 [00:00<00:03, 30.00it/s]
 26%|â–ˆâ–ˆâ–Œ       | 34/130 [00:01<00:03, 29.77it/s]
 28%|â–ˆâ–ˆâ–Š       | 37/130 [00:01<00:03, 29.32it/s]
 31%|â–ˆâ–ˆâ–ˆ       | 40/130 [00:01<00:03, 29.31it/s]
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 43/130 [00:01<00:02, 29.32it/s]
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 46/130 [00:01<00:02, 29.03it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 50/130 [00:01<00:02, 29.17it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 54/130 [00:01<00:02, 29.68it/s]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 57/130 [00:01<00:02, 29.69it/s]
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 61/130 [00:01<00:02, 30.09it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 65/130 [00:02<00:02, 30.55it/s]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 69/130 [00:02<00:01, 31.50it/s]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 73/130 [00:02<00:01, 32.02it/s]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 77/130 [00:02<00:01, 32.67it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 81/130 [00:02<00:01, 32.76it/s]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 85/130 [00:02<00:01, 33.03it/s]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 89/130 [00:02<00:01, 33.06it/s]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 93/130 [00:02<00:01, 33.28it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 97/130 [00:03<00:01, 32.92it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 101/130 [00:03<00:00, 32.29it/s]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 105/130 [00:03<00:00, 31.87it/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 109/130 [00:03<00:00, 32.13it/s]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 113/130 [00:03<00:00, 29.87it/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 117/130 [00:03<00:00, 28.66it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 120/130 [00:03<00:00, 27.80it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 123/130 [00:03<00:00, 27.33it/s]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 126/130 [00:04<00:00, 27.49it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 130/130 [00:04<00:00, 30.00it/s]
                                                   
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2076/3114 [04:44<02:09,  8.02it/s]
                                                 Saving model checkpoint to model_output_distilbert\mgsd_trained\checkpoint-2076
Configuration saved in model_output_distilbert\mgsd_trained\checkpoint-2076\config.json
{'eval_loss': 0.4510074257850647, 'eval_precision': 0.8082526775209425, 'eval_recall': 0.8009291157438841, 'eval_f1': 0.8043048748089812, 'eval_balanced accuracy': 0.8009291157438841, 'eval_runtime': 4.2532, 'eval_samples_per_second': 1951.229, 'eval_steps_per_second': 30.565, 'epoch': 4.0}
Model weights saved in model_output_distilbert\mgsd_trained\checkpoint-2076\model.safetensors
tokenizer config file saved in model_output_distilbert\mgsd_trained\checkpoint-2076\tokenizer_config.json
Special tokens file saved in model_output_distilbert\mgsd_trained\checkpoint-2076\special_tokens_map.json
Deleting older checkpoint [model_output_distilbert\mgsd_trained\checkpoint-1557] due to args.save_total_limit
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2078/3114 [04:45<17:09,  1.01it/s][codecarbon INFO @ 14:22:38] Energy consumed for RAM : 0.001473 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:22:38] Energy consumed for all GPUs : 0.012440 kWh. Total GPU Power : 158.46130040985804 W
[codecarbon INFO @ 14:22:38] Energy consumed for all CPUs : 0.003544 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:22:38] 0.017457 kWh of electricity used since the beginning.
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2197/3114 [05:00<01:54,  8.03it/s][codecarbon INFO @ 14:22:53] Energy consumed for RAM : 0.001547 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:22:53] Energy consumed for all GPUs : 0.013108 kWh. Total GPU Power : 160.35811486590475 W
[codecarbon INFO @ 14:22:53] Energy consumed for all CPUs : 0.003721 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:22:53] 0.018376 kWh of electricity used since the beginning.
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2319/3114 [05:15<01:37,  8.18it/s][codecarbon INFO @ 14:23:08] Energy consumed for RAM : 0.001620 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:23:08] Energy consumed for all GPUs : 0.013783 kWh. Total GPU Power : 161.90009178119737 W
[codecarbon INFO @ 14:23:08] Energy consumed for all CPUs : 0.003898 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:23:08] 0.019301 kWh of electricity used since the beginning.
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2440/3114 [05:30<01:24,  8.00it/s][codecarbon INFO @ 14:23:23] Energy consumed for RAM : 0.001694 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:23:23] Energy consumed for all GPUs : 0.014462 kWh. Total GPU Power : 162.810814633353 W
[codecarbon INFO @ 14:23:23] Energy consumed for all CPUs : 0.004075 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:23:23] 0.020231 kWh of electricity used since the beginning.
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2500/3114 [05:37<01:18,  7.81it/s]{'loss': 0.1733, 'grad_norm': 7.294335842132568, 'learning_rate': 3.9434810533076434e-06, 'epoch': 4.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2560/3114 [05:45<01:10,  7.88it/s][codecarbon INFO @ 14:23:38] Energy consumed for RAM : 0.001768 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:23:38] Energy consumed for all GPUs : 0.015133 kWh. Total GPU Power : 161.15176569722664 W
[codecarbon INFO @ 14:23:38] Energy consumed for all CPUs : 0.004252 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:23:38] 0.021153 kWh of electricity used since the beginning.
[codecarbon INFO @ 14:23:38] 0.014568 g.CO2eq/s mean an estimation of 459.43027218802547 kg.CO2eq/year
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2594/3114 [05:49<01:06,  7.83it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 8299
  Batch size = 64

  0%|          | 0/130 [00:00<?, ?it/s]
  4%|â–         | 5/130 [00:00<00:03, 40.08it/s]
  8%|â–Š         | 10/130 [00:00<00:03, 35.63it/s]
 11%|â–ˆ         | 14/130 [00:00<00:03, 34.79it/s]
 14%|â–ˆâ–        | 18/130 [00:00<00:03, 32.91it/s]
 17%|â–ˆâ–‹        | 22/130 [00:00<00:03, 30.83it/s]
 20%|â–ˆâ–ˆ        | 26/130 [00:00<00:03, 30.18it/s]
 23%|â–ˆâ–ˆâ–Ž       | 30/130 [00:00<00:03, 29.52it/s]
 25%|â–ˆâ–ˆâ–Œ       | 33/130 [00:01<00:03, 29.37it/s]
 28%|â–ˆâ–ˆâ–Š       | 36/130 [00:01<00:03, 29.28it/s]
 30%|â–ˆâ–ˆâ–ˆ       | 39/130 [00:01<00:03, 28.65it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 42/130 [00:01<00:03, 28.59it/s]
 35%|â–ˆâ–ˆâ–ˆâ–      | 45/130 [00:01<00:02, 28.44it/s]
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 48/130 [00:01<00:02, 28.87it/s]
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 51/130 [00:01<00:02, 28.81it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 55/130 [00:01<00:02, 28.92it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 59/130 [00:01<00:02, 29.30it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 62/130 [00:02<00:02, 29.30it/s]
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 66/130 [00:02<00:02, 30.49it/s]
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 70/130 [00:02<00:01, 31.09it/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 74/130 [00:02<00:01, 31.51it/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 78/130 [00:02<00:01, 31.82it/s]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 82/130 [00:02<00:01, 32.34it/s]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 86/130 [00:02<00:01, 32.55it/s]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 90/130 [00:02<00:01, 32.87it/s]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 94/130 [00:03<00:01, 32.59it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 98/130 [00:03<00:00, 32.51it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 102/130 [00:03<00:00, 32.23it/s]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 106/130 [00:03<00:00, 32.19it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 110/130 [00:03<00:00, 31.35it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 114/130 [00:03<00:00, 29.19it/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 117/130 [00:03<00:00, 27.98it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 120/130 [00:03<00:00, 27.37it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 123/130 [00:04<00:00, 26.37it/s]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 126/130 [00:04<00:00, 26.58it/s]
{'eval_loss': 0.49053579568862915, 'eval_precision': 0.8062541524618039, 'eval_recall': 0.8026000838425611, 'eval_f1': 0.8043524195739067, 'eval_balanced accuracy': 0.8026000838425611, 'eval_runtime': 4.3316, 'eval_samples_per_second': 1915.94, 'eval_steps_per_second': 30.012, 'epoch': 5.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 130/130 [00:04<00:00, 29.35it/s]
                                                   
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2595/3114 [05:53<01:06,  7.83it/s]
                                                 Saving model checkpoint to model_output_distilbert\mgsd_trained\checkpoint-2595
Configuration saved in model_output_distilbert\mgsd_trained\checkpoint-2595\config.json
Model weights saved in model_output_distilbert\mgsd_trained\checkpoint-2595\model.safetensors
tokenizer config file saved in model_output_distilbert\mgsd_trained\checkpoint-2595\tokenizer_config.json
Special tokens file saved in model_output_distilbert\mgsd_trained\checkpoint-2595\special_tokens_map.json
Deleting older checkpoint [model_output_distilbert\mgsd_trained\checkpoint-2076] due to args.save_total_limit
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2640/3114 [06:00<01:00,  7.90it/s][codecarbon INFO @ 14:23:53] Energy consumed for RAM : 0.001841 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:23:53] Energy consumed for all GPUs : 0.015794 kWh. Total GPU Power : 158.4949250783808 W
[codecarbon INFO @ 14:23:53] Energy consumed for all CPUs : 0.004430 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:23:53] 0.022065 kWh of electricity used since the beginning.
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2758/3114 [06:15<00:44,  8.01it/s][codecarbon INFO @ 14:24:08] Energy consumed for RAM : 0.001915 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:24:08] Energy consumed for all GPUs : 0.016460 kWh. Total GPU Power : 159.71346450550237 W
[codecarbon INFO @ 14:24:08] Energy consumed for all CPUs : 0.004607 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:24:08] 0.022982 kWh of electricity used since the beginning.
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2879/3114 [06:30<00:30,  7.81it/s][codecarbon INFO @ 14:24:23] Energy consumed for RAM : 0.001989 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:24:23] Energy consumed for all GPUs : 0.017136 kWh. Total GPU Power : 162.33248736331052 W
[codecarbon INFO @ 14:24:23] Energy consumed for all CPUs : 0.004784 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:24:23] 0.023909 kWh of electricity used since the beginning.
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3000/3114 [06:45<00:14,  8.00it/s]{'loss': 0.1441, 'grad_norm': 7.014316558837891, 'learning_rate': 7.321772639691716e-07, 'epoch': 5.78}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3001/3114 [06:45<00:14,  7.96it/s][codecarbon INFO @ 14:24:38] Energy consumed for RAM : 0.002062 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:24:38] Energy consumed for all GPUs : 0.017813 kWh. Total GPU Power : 162.17734706863592 W
[codecarbon INFO @ 14:24:38] Energy consumed for all CPUs : 0.004961 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:24:38] 0.024836 kWh of electricity used since the beginning.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3113/3114 [06:59<00:00,  7.93it/s]Saving model checkpoint to model_output_distilbert\mgsd_trained\checkpoint-3114
Configuration saved in model_output_distilbert\mgsd_trained\checkpoint-3114\config.json
Model weights saved in model_output_distilbert\mgsd_trained\checkpoint-3114\model.safetensors
tokenizer config file saved in model_output_distilbert\mgsd_trained\checkpoint-3114\tokenizer_config.json
Special tokens file saved in model_output_distilbert\mgsd_trained\checkpoint-3114\special_tokens_map.json
Deleting older checkpoint [model_output_distilbert\mgsd_trained\checkpoint-2595] due to args.save_total_limit
The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 8299
  Batch size = 64

  0%|          | 0/130 [00:00<?, ?it/s]
  4%|â–         | 5/130 [00:00<00:02, 44.58it/s]
  8%|â–Š         | 10/130 [00:00<00:03, 36.17it/s][codecarbon INFO @ 14:24:53] Energy consumed for RAM : 0.002136 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:24:53] Energy consumed for all GPUs : 0.018463 kWh. Total GPU Power : 156.1037416485234 W
[codecarbon INFO @ 14:24:53] Energy consumed for all CPUs : 0.005138 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:24:53] 0.025737 kWh of electricity used since the beginning.

 11%|â–ˆ         | 14/130 [00:00<00:03, 35.72it/s]
 14%|â–ˆâ–        | 18/130 [00:00<00:03, 32.51it/s]
 17%|â–ˆâ–‹        | 22/130 [00:00<00:03, 31.19it/s]
 20%|â–ˆâ–ˆ        | 26/130 [00:00<00:03, 30.05it/s]
 23%|â–ˆâ–ˆâ–Ž       | 30/130 [00:00<00:03, 29.78it/s]
 25%|â–ˆâ–ˆâ–Œ       | 33/130 [00:01<00:03, 29.36it/s]
 28%|â–ˆâ–ˆâ–Š       | 36/130 [00:01<00:03, 29.33it/s]
 30%|â–ˆâ–ˆâ–ˆ       | 39/130 [00:01<00:03, 29.47it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 42/130 [00:01<00:03, 28.86it/s]
 35%|â–ˆâ–ˆâ–ˆâ–      | 45/130 [00:01<00:02, 29.01it/s]
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 48/130 [00:01<00:02, 29.20it/s]
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 51/130 [00:01<00:02, 29.20it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 55/130 [00:01<00:02, 29.74it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 58/130 [00:01<00:02, 29.73it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 62/130 [00:02<00:02, 29.88it/s]
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 66/130 [00:02<00:02, 30.55it/s]
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 70/130 [00:02<00:01, 31.48it/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 74/130 [00:02<00:01, 31.91it/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 78/130 [00:02<00:01, 32.51it/s]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 82/130 [00:02<00:01, 32.59it/s]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 86/130 [00:02<00:01, 32.93it/s]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 90/130 [00:02<00:01, 32.73it/s]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 94/130 [00:03<00:01, 32.86it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 98/130 [00:03<00:00, 32.24it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 102/130 [00:03<00:00, 32.38it/s]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 106/130 [00:03<00:00, 32.25it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 110/130 [00:03<00:00, 31.76it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 114/130 [00:03<00:00, 29.54it/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 117/130 [00:03<00:00, 28.57it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 120/130 [00:03<00:00, 27.63it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 123/130 [00:04<00:00, 27.33it/s]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 126/130 [00:04<00:00, 27.63it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 130/130 [00:04<00:00, 29.97it/s]
                                                   
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3114/3114 [07:04<00:00,  7.93it/s]
                                                 Saving model checkpoint to model_output_distilbert\mgsd_trained\checkpoint-3114
Configuration saved in model_output_distilbert\mgsd_trained\checkpoint-3114\config.json
{'eval_loss': 0.5185672044754028, 'eval_precision': 0.8022240384815345, 'eval_recall': 0.8064870931321112, 'eval_f1': 0.8042412286872787, 'eval_balanced accuracy': 0.8064870931321112, 'eval_runtime': 4.2649, 'eval_samples_per_second': 1945.878, 'eval_steps_per_second': 30.481, 'epoch': 6.0}
Model weights saved in model_output_distilbert\mgsd_trained\checkpoint-3114\model.safetensors
tokenizer config file saved in model_output_distilbert\mgsd_trained\checkpoint-3114\tokenizer_config.json
Special tokens file saved in model_output_distilbert\mgsd_trained\checkpoint-3114\special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from model_output_distilbert\mgsd_trained\checkpoint-1038 (score: 0.3917471468448639).
{'train_runtime': 425.138, 'train_samples_per_second': 468.469, 'train_steps_per_second': 7.325, 'train_loss': 0.2709117196045706, 'epoch': 6.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3114/3114 [07:05<00:00,  7.93it/s]Deleting older checkpoint [model_output_distilbert\mgsd_trained\checkpoint-3114] due to args.save_total_limit
[codecarbon WARNING @ 14:24:58] Another instance of codecarbon is already running. Exiting.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3114/3114 [07:05<00:00,  7.32it/s]
Saving model checkpoint to model_output_distilbert\mgsd_trained
Configuration saved in model_output_distilbert\mgsd_trained\config.json
Model weights saved in model_output_distilbert\mgsd_trained\model.safetensors
tokenizer config file saved in model_output_distilbert\mgsd_trained\tokenizer_config.json
Special tokens file saved in model_output_distilbert\mgsd_trained\special_tokens_map.json
[codecarbon INFO @ 14:24:58] Energy consumed for RAM : 0.002161 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:24:58] Energy consumed for all GPUs : 0.018672 kWh. Total GPU Power : 146.21072468454048 W
[codecarbon INFO @ 14:24:58] Energy consumed for all CPUs : 0.005199 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:24:58] 0.026032 kWh of electricity used since the beginning.
Estimated total emissions: 0.0061849587529763635 kg CO2
Number of unique labels: 2
loading configuration file model_output_distilbert/mgsd_trained\config.json
Model config DistilBertConfig {
  "_name_or_path": "model_output_distilbert/mgsd_trained",
  "activation": "gelu",
  "architectures": [
    "DistilBertForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "problem_type": "single_label_classification",
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "vocab_size": 30522
}

loading weights file model_output_distilbert/mgsd_trained\model.safetensors
All model checkpoint weights were used when initializing DistilBertForSequenceClassification.

All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at model_output_distilbert/mgsd_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 653/653 [00:00<00:00, 18968.76 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 653/653 [00:00<00:00, 4632.91 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3236, 'input_ids': [101, 2087, 2111, 2040, 2024, 5637, 2024, 20497, 2015, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
loading configuration file model_output_distilbert/mgsd_trained\config.json
Model config DistilBertConfig {
  "_name_or_path": "model_output_distilbert/mgsd_trained",
  "activation": "gelu",
  "architectures": [
    "DistilBertForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "problem_type": "single_label_classification",
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "vocab_size": 30522
}

loading weights file model_output_distilbert/mgsd_trained\model.safetensors
Number of unique labels: 2
All model checkpoint weights were used when initializing DistilBertForSequenceClassification.

All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at model_output_distilbert/mgsd_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 414/414 [00:00<00:00, 27596.30 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 414/414 [00:00<00:00, 20632.14 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 840, 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'labels': 0}
loading configuration file model_output_distilbert/mgsd_trained\config.json
Model config DistilBertConfig {
  "_name_or_path": "model_output_distilbert/mgsd_trained",
  "activation": "gelu",
  "architectures": [
    "DistilBertForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "problem_type": "single_label_classification",
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "vocab_size": 30522
}

loading weights file model_output_distilbert/mgsd_trained\model.safetensors
Number of unique labels: 2
All model checkpoint weights were used when initializing DistilBertForSequenceClassification.

All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at model_output_distilbert/mgsd_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10374/10374 [00:00<00:00, 24871.98 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10374/10374 [00:00<00:00, 18642.04 examples/s]
Sample tokenized input from test: {'group': 'race', 'text': 'When i see an arab, i think they are a happy.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 17851, 'input_ids': [101, 2043, 1045, 2156, 2019, 5424, 1010, 1045, 2228, 2027, 2024, 1037, 3407, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
loading configuration file model_output_distilbert/mgsd_trained\config.json
Model config DistilBertConfig {
  "_name_or_path": "model_output_distilbert/mgsd_trained",
  "activation": "gelu",
  "architectures": [
    "DistilBertForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "problem_type": "single_label_classification",
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "vocab_size": 30522
}

loading weights file model_output_distilbert/mgsd_trained\model.safetensors
Number of unique labels: 2
All model checkpoint weights were used when initializing DistilBertForSequenceClassification.

All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at model_output_distilbert/mgsd_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11441/11441 [00:00<00:00, 26818.00 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11441/11441 [00:00<00:00, 22734.60 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
Number of unique labels: 2
[codecarbon INFO @ 14:29:34] [setup] RAM Tracking...
[codecarbon INFO @ 14:29:34] [setup] GPU Tracking...
[codecarbon INFO @ 14:29:34] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 14:29:34] [setup] CPU Tracking...
[codecarbon WARNING @ 14:29:34] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon WARNING @ 14:29:36] We saw that you have a AMD Ryzen 7 7700 8-Core Processor but we don't know it. Please contact us.
[codecarbon INFO @ 14:29:36] CPU Model on constant consumption mode: AMD Ryzen 7 7700 8-Core Processor
[codecarbon INFO @ 14:29:36] >>> Tracker's metadata:
[codecarbon INFO @ 14:29:36]   Platform system: Windows-11-10.0.26100-SP0
[codecarbon INFO @ 14:29:36]   Python version: 3.12.12
[codecarbon INFO @ 14:29:36]   CodeCarbon version: 2.8.0
[codecarbon INFO @ 14:29:36]   Available RAM : 47.116 GB
[codecarbon INFO @ 14:29:36]   CPU count: 16
[codecarbon INFO @ 14:29:36]   CPU model: AMD Ryzen 7 7700 8-Core Processor
[codecarbon INFO @ 14:29:36]   GPU count: 1
[codecarbon INFO @ 14:29:36]   GPU model: 1 x NVIDIA GeForce RTX 5060 Ti
[codecarbon INFO @ 14:29:39] Saving emissions data to file D:\UCL\HEARTS-Text-Stereotype-Detection-main\Model Training and Evaluation\emissions.csv
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--distilbert--distilbert-base-uncased\snapshots\12040accade4e8a0f71eabdb258fecc2e7e948be\config.json
Model config DistilBertConfig {
  "_name_or_path": "distilbert/distilbert-base-uncased",
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.46.3",
  "vocab_size": 30522
}

loading weights file model.safetensors from cache at C:\Users\32618\.cache\huggingface\hub\models--distilbert--distilbert-base-uncased\snapshots\12040accade4e8a0f71eabdb258fecc2e7e948be\model.safetensors
Some weights of the model checkpoint at distilbert/distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--distilbert--distilbert-base-uncased\snapshots\12040accade4e8a0f71eabdb258fecc2e7e948be\config.json
Model config DistilBertConfig {
  "_name_or_path": "distilbert/distilbert-base-uncased",
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.46.3",
  "vocab_size": 30522
}

loading file vocab.txt from cache at C:\Users\32618\.cache\huggingface\hub\models--distilbert--distilbert-base-uncased\snapshots\12040accade4e8a0f71eabdb258fecc2e7e948be\vocab.txt
loading file tokenizer.json from cache at C:\Users\32618\.cache\huggingface\hub\models--distilbert--distilbert-base-uncased\snapshots\12040accade4e8a0f71eabdb258fecc2e7e948be\tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--distilbert--distilbert-base-uncased\snapshots\12040accade4e8a0f71eabdb258fecc2e7e948be\tokenizer_config.json
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--distilbert--distilbert-base-uncased\snapshots\12040accade4e8a0f71eabdb258fecc2e7e948be\config.json
Model config DistilBertConfig {
  "_name_or_path": "distilbert/distilbert-base-uncased",
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.46.3",
  "vocab_size": 30522
}

Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2088/2088 [00:00<00:00, 30133.94 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2088/2088 [00:00<00:00, 22448.64 examples/s]
Sample tokenized input from train: {'text': 'we expect that nonbinary people are out to share their views.', 'label': 0, 'group': 'NB', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 1788, 'input_ids': [101, 2057, 5987, 2008, 2512, 21114, 2854, 2111, 2024, 2041, 2000, 3745, 2037, 5328, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 523/523 [00:00<00:00, 30683.45 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 523/523 [00:00<00:00, 22916.58 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
PyTorch: setting up devices
Sample tokenized input from validation: {'text': 'we expect that nonbinary people are out to share their views.', 'label': 0, 'group': 'NB', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 1788, 'input_ids': [101, 2057, 5987, 2008, 2512, 21114, 2854, 2111, 2024, 2041, 2000, 3745, 2037, 5328, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
D:\UCL\HEARTS-Text-Stereotype-Detection-main\Model Training and Evaluation\BERT_Models_Fine_Tuning.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
[codecarbon ERROR @ 14:29:40] Error: Another instance of codecarbon is probably running as we find `C:\Users\32618\AppData\Local\Temp\.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.
The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running training *****
  Num examples = 2,088
  Num Epochs = 6
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 198
  Number of trainable parameters = 66,955,010
[codecarbon WARNING @ 14:29:40] Another instance of codecarbon is already running. Exiting.
 16%|â–ˆâ–Œ        | 32/198 [00:01<00:10, 16.58it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 523
  Batch size = 64

  0%|          | 0/9 [00:00<?, ?it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:00<00:00, 52.97it/s]
{'eval_loss': 0.2779788374900818, 'eval_precision': 0.908476507713885, 'eval_recall': 0.8944388014220417, 'eval_f1': 0.9009469696969697, 'eval_balanced accuracy': 0.8944388014220417, 'eval_runtime': 0.1914, 'eval_samples_per_second': 2731.969, 'eval_steps_per_second': 47.013, 'epoch': 1.0}
                                                
 17%|â–ˆâ–‹        | 33/198 [00:02<00:09, 16.58it/s]
                                             Saving model checkpoint to model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-33
Configuration saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-33\config.json
Model weights saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-33\model.safetensors
tokenizer config file saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-33\tokenizer_config.json
Special tokens file saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-33\special_tokens_map.json
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/198 [00:05<00:07, 16.79it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 523
  Batch size = 64

  0%|          | 0/9 [00:00<?, ?it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:00<00:00, 59.62it/s]
                                                
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/198 [00:05<00:07, 16.79it/s]
                                             Saving model checkpoint to model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-66
Configuration saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-66\config.json
{'eval_loss': 0.1336636245250702, 'eval_precision': 0.9516096579476861, 'eval_recall': 0.9559674961909599, 'eval_f1': 0.9537419607545834, 'eval_balanced accuracy': 0.9559674961909599, 'eval_runtime': 0.1787, 'eval_samples_per_second': 2927.224, 'eval_steps_per_second': 50.373, 'epoch': 2.0}
Model weights saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-66\model.safetensors
tokenizer config file saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-66\tokenizer_config.json
Special tokens file saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-66\special_tokens_map.json
Deleting older checkpoint [model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-33] due to args.save_total_limit
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/198 [00:08<00:06, 16.41it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 523
  Batch size = 64

  0%|          | 0/9 [00:00<?, ?it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:00<00:00, 59.80it/s]
{'eval_loss': 0.07451874017715454, 'eval_precision': 0.9750526526928092, 'eval_recall': 0.9811325545962417, 'eval_f1': 0.9780074682096481, 'eval_balanced accuracy': 0.9811325545962417, 'eval_runtime': 0.179, 'eval_samples_per_second': 2921.641, 'eval_steps_per_second': 50.277, 'epoch': 3.0}
                                                
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 99/198 [00:08<00:06, 16.41it/s]
                                             Saving model checkpoint to model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-99
Configuration saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-99\config.json
Model weights saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-99\model.safetensors
tokenizer config file saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-99\tokenizer_config.json
Special tokens file saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-99\special_tokens_map.json
Deleting older checkpoint [model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-66] due to args.save_total_limit
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 132/198 [00:11<00:03, 17.24it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 523
  Batch size = 64

  0%|          | 0/9 [00:00<?, ?it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:00<00:00, 55.59it/s]
{'eval_loss': 0.0477914996445179, 'eval_precision': 0.982302146210597, 'eval_recall': 0.9869561537159303, 'eval_f1': 0.9845806535848611, 'eval_balanced accuracy': 0.9869561537159303, 'eval_runtime': 0.1919, 'eval_samples_per_second': 2725.672, 'eval_steps_per_second': 46.904, 'epoch': 4.0}
                                                 
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 132/198 [00:11<00:03, 17.24it/s]
                                             Saving model checkpoint to model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-132
Configuration saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-132\config.json
Model weights saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-132\model.safetensors
tokenizer config file saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-132\tokenizer_config.json
Special tokens file saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-132\special_tokens_map.json
Deleting older checkpoint [model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-99] due to args.save_total_limit
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 162/198 [00:13<00:02, 16.32it/s][codecarbon INFO @ 14:29:54] Energy consumed for RAM : 0.000074 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:29:54] Energy consumed for all GPUs : 0.000445 kWh. Total GPU Power : 106.57002532636514 W
[codecarbon INFO @ 14:29:54] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:29:54] 0.000696 kWh of electricity used since the beginning.
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 164/198 [00:13<00:02, 16.08it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 523
  Batch size = 64

  0%|          | 0/9 [00:00<?, ?it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:00<00:00, 59.65it/s]
{'eval_loss': 0.05475977808237076, 'eval_precision': 0.9765789035160807, 'eval_recall': 0.9841628576265449, 'eval_f1': 0.9802380502550538, 'eval_balanced accuracy': 0.9841628576265449, 'eval_runtime': 0.1835, 'eval_samples_per_second': 2849.95, 'eval_steps_per_second': 49.043, 'epoch': 5.0}
                                                 
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/198 [00:14<00:02, 16.08it/s]
                                             Saving model checkpoint to model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-165
Configuration saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-165\config.json
Model weights saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-165\model.safetensors
tokenizer config file saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-165\tokenizer_config.json
Special tokens file saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-165\special_tokens_map.json
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:16<00:00, 17.17it/s]Saving model checkpoint to model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-198
Configuration saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-198\config.json
Model weights saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-198\model.safetensors
tokenizer config file saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-198\tokenizer_config.json
Special tokens file saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-198\special_tokens_map.json
Deleting older checkpoint [model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-165] due to args.save_total_limit
The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 523
  Batch size = 64

  0%|          | 0/9 [00:00<?, ?it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:00<00:00, 58.62it/s]
                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:17<00:00, 17.17it/s]
                                             Saving model checkpoint to model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-198
Configuration saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-198\config.json
{'eval_loss': 0.0559297576546669, 'eval_precision': 0.9737672780435938, 'eval_recall': 0.9827662095818521, 'eval_f1': 0.9780767940979209, 'eval_balanced accuracy': 0.9827662095818521, 'eval_runtime': 0.1711, 'eval_samples_per_second': 3056.417, 'eval_steps_per_second': 52.596, 'epoch': 6.0}
Model weights saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-198\model.safetensors
tokenizer config file saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-198\tokenizer_config.json
Special tokens file saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-198\special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-132 (score: 0.0477914996445179).
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:18<00:00, 17.17it/s]Deleting older checkpoint [model_output_distilbert\winoqueer_gpt_augmentation_trained\checkpoint-198] due to args.save_total_limit
{'train_runtime': 18.9397, 'train_samples_per_second': 661.467, 'train_steps_per_second': 10.454, 'train_loss': 0.15494624051180753, 'epoch': 6.0}
[codecarbon WARNING @ 14:29:59] Another instance of codecarbon is already running. Exiting.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:19<00:00, 10.36it/s]
Saving model checkpoint to model_output_distilbert\winoqueer_gpt_augmentation_trained
Configuration saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\config.json
Model weights saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\model.safetensors
tokenizer config file saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\tokenizer_config.json
Special tokens file saved in model_output_distilbert\winoqueer_gpt_augmentation_trained\special_tokens_map.json
[codecarbon INFO @ 14:30:00] Energy consumed for RAM : 0.000101 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:30:00] Energy consumed for all GPUs : 0.000569 kWh. Total GPU Power : 81.17539934259355 W
[codecarbon INFO @ 14:30:00] Energy consumed for all CPUs : 0.000242 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:30:00] 0.000912 kWh of electricity used since the beginning.
Estimated total emissions: 0.00021664895123793367 kg CO2
Number of unique labels: 2
loading configuration file model_output_distilbert/winoqueer_gpt_augmentation_trained\config.json
Model config DistilBertConfig {
  "_name_or_path": "model_output_distilbert/winoqueer_gpt_augmentation_trained",
  "activation": "gelu",
  "architectures": [
    "DistilBertForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "problem_type": "single_label_classification",
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "vocab_size": 30522
}

loading weights file model_output_distilbert/winoqueer_gpt_augmentation_trained\model.safetensors
All model checkpoint weights were used when initializing DistilBertForSequenceClassification.

All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at model_output_distilbert/winoqueer_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 653/653 [00:00<00:00, 32945.77 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 653/653 [00:00<00:00, 21753.03 examples/s]
Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3236, 'input_ids': [101, 2087, 2111, 2040, 2024, 5637, 2024, 20497, 2015, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Number of unique labels: 2
loading configuration file model_output_distilbert/winoqueer_gpt_augmentation_trained\config.json
Model config DistilBertConfig {
  "_name_or_path": "model_output_distilbert/winoqueer_gpt_augmentation_trained",
  "activation": "gelu",
  "architectures": [
    "DistilBertForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "problem_type": "single_label_classification",
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "vocab_size": 30522
}

loading weights file model_output_distilbert/winoqueer_gpt_augmentation_trained\model.safetensors
All model checkpoint weights were used when initializing DistilBertForSequenceClassification.

All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at model_output_distilbert/winoqueer_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 414/414 [00:00<00:00, 29542.38 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 414/414 [00:00<00:00, 25064.48 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 840, 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'labels': 0}
loading configuration file model_output_distilbert/winoqueer_gpt_augmentation_trained\config.json
Model config DistilBertConfig {
  "_name_or_path": "model_output_distilbert/winoqueer_gpt_augmentation_trained",
  "activation": "gelu",
  "architectures": [
    "DistilBertForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "problem_type": "single_label_classification",
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "vocab_size": 30522
}

loading weights file model_output_distilbert/winoqueer_gpt_augmentation_trained\model.safetensors
Number of unique labels: 2
All model checkpoint weights were used when initializing DistilBertForSequenceClassification.

All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at model_output_distilbert/winoqueer_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10374/10374 [00:00<00:00, 25975.91 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10374/10374 [00:00<00:00, 26908.45 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Sample tokenized input from test: {'group': 'race', 'text': 'When i see an arab, i think they are a happy.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 17851, 'input_ids': [101, 2043, 1045, 2156, 2019, 5424, 1010, 1045, 2228, 2027, 2024, 1037, 3407, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
Number of unique labels: 2
loading configuration file model_output_distilbert/winoqueer_gpt_augmentation_trained\config.json
Model config DistilBertConfig {
  "_name_or_path": "model_output_distilbert/winoqueer_gpt_augmentation_trained",
  "activation": "gelu",
  "architectures": [
    "DistilBertForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "problem_type": "single_label_classification",
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "vocab_size": 30522
}

loading weights file model_output_distilbert/winoqueer_gpt_augmentation_trained\model.safetensors
All model checkpoint weights were used when initializing DistilBertForSequenceClassification.

All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at model_output_distilbert/winoqueer_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11441/11441 [00:00<00:00, 27049.61 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11441/11441 [00:00<00:00, 31176.32 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
Number of unique labels: 2
[codecarbon INFO @ 14:34:30] [setup] RAM Tracking...
[codecarbon INFO @ 14:34:30] [setup] GPU Tracking...
[codecarbon INFO @ 14:34:30] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 14:34:30] [setup] CPU Tracking...
[codecarbon WARNING @ 14:34:30] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon WARNING @ 14:34:32] We saw that you have a AMD Ryzen 7 7700 8-Core Processor but we don't know it. Please contact us.
[codecarbon INFO @ 14:34:32] CPU Model on constant consumption mode: AMD Ryzen 7 7700 8-Core Processor
[codecarbon INFO @ 14:34:32] >>> Tracker's metadata:
[codecarbon INFO @ 14:34:32]   Platform system: Windows-11-10.0.26100-SP0
[codecarbon INFO @ 14:34:32]   Python version: 3.12.12
[codecarbon INFO @ 14:34:32]   CodeCarbon version: 2.8.0
[codecarbon INFO @ 14:34:32]   Available RAM : 47.116 GB
[codecarbon INFO @ 14:34:32]   CPU count: 16
[codecarbon INFO @ 14:34:32]   CPU model: AMD Ryzen 7 7700 8-Core Processor
[codecarbon INFO @ 14:34:32]   GPU count: 1
[codecarbon INFO @ 14:34:32]   GPU model: 1 x NVIDIA GeForce RTX 5060 Ti
[codecarbon INFO @ 14:34:35] Saving emissions data to file D:\UCL\HEARTS-Text-Stereotype-Detection-main\Model Training and Evaluation\emissions.csv
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--distilbert--distilbert-base-uncased\snapshots\12040accade4e8a0f71eabdb258fecc2e7e948be\config.json
Model config DistilBertConfig {
  "_name_or_path": "distilbert/distilbert-base-uncased",
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.46.3",
  "vocab_size": 30522
}

loading weights file model.safetensors from cache at C:\Users\32618\.cache\huggingface\hub\models--distilbert--distilbert-base-uncased\snapshots\12040accade4e8a0f71eabdb258fecc2e7e948be\model.safetensors
Some weights of the model checkpoint at distilbert/distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--distilbert--distilbert-base-uncased\snapshots\12040accade4e8a0f71eabdb258fecc2e7e948be\config.json
Model config DistilBertConfig {
  "_name_or_path": "distilbert/distilbert-base-uncased",
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.46.3",
  "vocab_size": 30522
}

loading file vocab.txt from cache at C:\Users\32618\.cache\huggingface\hub\models--distilbert--distilbert-base-uncased\snapshots\12040accade4e8a0f71eabdb258fecc2e7e948be\vocab.txt
loading file tokenizer.json from cache at C:\Users\32618\.cache\huggingface\hub\models--distilbert--distilbert-base-uncased\snapshots\12040accade4e8a0f71eabdb258fecc2e7e948be\tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--distilbert--distilbert-base-uncased\snapshots\12040accade4e8a0f71eabdb258fecc2e7e948be\tokenizer_config.json
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--distilbert--distilbert-base-uncased\snapshots\12040accade4e8a0f71eabdb258fecc2e7e948be\config.json
Model config DistilBertConfig {
  "_name_or_path": "distilbert/distilbert-base-uncased",
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.46.3",
  "vocab_size": 30522
}

Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1324/1324 [00:00<00:00, 7129.73 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1324/1324 [00:00<00:00, 24208.28 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 332/332 [00:00<00:00, 27895.37 examples/s]
Sample tokenized input from train: {'text': 'The average Chadian income is modest compared to developed countries', 'label': 0, 'group': 'Chadian', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 1249, 'input_ids': [101, 1996, 2779, 9796, 2937, 3318, 2003, 10754, 4102, 2000, 2764, 3032, 102, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 332/332 [00:00<00:00, 23700.67 examples/s]
Sample tokenized input from validation: {'text': 'The average Chadian income is modest compared to developed countries', 'label': 0, 'group': 'Chadian', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 1249, 'input_ids': [101, 1996, 2779, 9796, 2937, 3318, 2003, 10754, 4102, 2000, 2764, 3032, 102, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
D:\UCL\HEARTS-Text-Stereotype-Detection-main\Model Training and Evaluation\BERT_Models_Fine_Tuning.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
[codecarbon ERROR @ 14:34:36] Error: Another instance of codecarbon is probably running as we find `C:\Users\32618\AppData\Local\Temp\.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.
The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running training *****
  Num examples = 1,324
  Num Epochs = 6
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 126
  Number of trainable parameters = 66,955,010
[codecarbon WARNING @ 14:34:36] Another instance of codecarbon is already running. Exiting.
 15%|â–ˆâ–Œ        | 19/126 [00:00<00:05, 19.87it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 332
  Batch size = 64

  0%|          | 0/6 [00:00<?, ?it/s]E:\Anaconda\envs\cw2\Lib\site-packages\sklearn\metrics\_classification.py:1528: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

                                                
 17%|â–ˆâ–‹        | 21/126 [00:01<00:05, 19.87it/s]
                                              Saving model checkpoint to model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-21
Configuration saved in model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-21\config.json
{'eval_loss': 0.5829784274101257, 'eval_precision': 0.3328313253012048, 'eval_recall': 0.5, 'eval_f1': 0.3996383363471971, 'eval_balanced accuracy': 0.5, 'eval_runtime': 0.0859, 'eval_samples_per_second': 3864.109, 'eval_steps_per_second': 69.833, 'epoch': 1.0}
Model weights saved in model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-21\model.safetensors
tokenizer config file saved in model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-21\tokenizer_config.json
Special tokens file saved in model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-21\special_tokens_map.json
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 41/126 [00:02<00:04, 17.46it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 332
  Batch size = 64

  0%|          | 0/6 [00:00<?, ?it/s]
                                                
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 42/126 [00:03<00:04, 17.46it/s]
                                             Saving model checkpoint to model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-42
Configuration saved in model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-42\config.json
{'eval_loss': 0.3502448797225952, 'eval_precision': 0.8477434223161129, 'eval_recall': 0.8690636337695161, 'eval_f1': 0.8559198327218211, 'eval_balanced accuracy': 0.8690636337695161, 'eval_runtime': 0.0928, 'eval_samples_per_second': 3577.581, 'eval_steps_per_second': 64.655, 'epoch': 2.0}
Model weights saved in model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-42\model.safetensors
tokenizer config file saved in model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-42\tokenizer_config.json
Special tokens file saved in model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-42\special_tokens_map.json
Deleting older checkpoint [model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-21] due to args.save_total_limit
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 61/126 [00:04<00:03, 17.01it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 332
  Batch size = 64

  0%|          | 0/6 [00:00<?, ?it/s]
                                                
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 63/126 [00:04<00:03, 17.01it/s]
                                              Saving model checkpoint to model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-63
Configuration saved in model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-63\config.json
{'eval_loss': 0.29520994424819946, 'eval_precision': 0.8575757575757575, 'eval_recall': 0.8848192083486202, 'eval_f1': 0.8669442733839672, 'eval_balanced accuracy': 0.8848192083486202, 'eval_runtime': 0.0848, 'eval_samples_per_second': 3913.213, 'eval_steps_per_second': 70.721, 'epoch': 3.0}
Model weights saved in model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-63\model.safetensors
tokenizer config file saved in model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-63\tokenizer_config.json
Special tokens file saved in model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-63\special_tokens_map.json
Deleting older checkpoint [model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-42] due to args.save_total_limit
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 82/126 [00:06<00:02, 16.83it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 332
  Batch size = 64

  0%|          | 0/6 [00:00<?, ?it/s]
                                                
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 84/126 [00:06<00:02, 16.83it/s]
                                              Saving model checkpoint to model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-84
Configuration saved in model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-84\config.json
{'eval_loss': 0.3050386309623718, 'eval_precision': 0.8711713730763484, 'eval_recall': 0.9050792874322287, 'eval_f1': 0.8811244979919679, 'eval_balanced accuracy': 0.9050792874322287, 'eval_runtime': 0.0848, 'eval_samples_per_second': 3913.422, 'eval_steps_per_second': 70.724, 'epoch': 4.0}
Model weights saved in model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-84\model.safetensors
tokenizer config file saved in model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-84\tokenizer_config.json
Special tokens file saved in model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-84\special_tokens_map.json
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 104/126 [00:08<00:01, 17.17it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 332
  Batch size = 64

  0%|          | 0/6 [00:00<?, ?it/s]
{'eval_loss': 0.2508264183998108, 'eval_precision': 0.8990917874396135, 'eval_recall': 0.920957156251274, 'eval_f1': 0.9079676857278631, 'eval_balanced accuracy': 0.920957156251274, 'eval_runtime': 0.0849, 'eval_samples_per_second': 3911.696, 'eval_steps_per_second': 70.693, 'epoch': 5.0}
                                                 
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 105/126 [00:08<00:01, 17.17it/s]
                                              Saving model checkpoint to model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-105
Configuration saved in model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-105\config.json
Model weights saved in model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-105\model.safetensors
tokenizer config file saved in model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-105\tokenizer_config.json
Special tokens file saved in model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-105\special_tokens_map.json
Deleting older checkpoint [model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-63] due to args.save_total_limit
Deleting older checkpoint [model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-84] due to args.save_total_limit
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 125/126 [00:10<00:00, 17.03it/s]Saving model checkpoint to model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-126
Configuration saved in model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-126\config.json
Model weights saved in model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-126\model.safetensors
tokenizer config file saved in model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-126\tokenizer_config.json
Special tokens file saved in model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-126\special_tokens_map.json
The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 332
  Batch size = 64

  0%|          | 0/6 [00:00<?, ?it/s]
{'eval_loss': 0.25900423526763916, 'eval_precision': 0.89875168043019, 'eval_recall': 0.9231992173168644, 'eval_f1': 0.908312620822977, 'eval_balanced accuracy': 0.9231992173168644, 'eval_runtime': 0.075, 'eval_samples_per_second': 4428.297, 'eval_steps_per_second': 80.029, 'epoch': 6.0}
                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:11<00:00, 17.03it/s]
                                              Saving model checkpoint to model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-126
Configuration saved in model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-126\config.json
Model weights saved in model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-126\model.safetensors
tokenizer config file saved in model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-126\tokenizer_config.json
Special tokens file saved in model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-126\special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-105 (score: 0.2508264183998108).
{'train_runtime': 12.7601, 'train_samples_per_second': 622.566, 'train_steps_per_second': 9.875, 'train_loss': 0.28211415003216456, 'epoch': 6.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:12<00:00, 17.03it/s]Deleting older checkpoint [model_output_distilbert\seegull_gpt_augmentation_trained\checkpoint-126] due to args.save_total_limit
[codecarbon WARNING @ 14:34:49] Another instance of codecarbon is already running. Exiting.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:12<00:00,  9.84it/s]
Saving model checkpoint to model_output_distilbert\seegull_gpt_augmentation_trained
Configuration saved in model_output_distilbert\seegull_gpt_augmentation_trained\config.json
Model weights saved in model_output_distilbert\seegull_gpt_augmentation_trained\model.safetensors
tokenizer config file saved in model_output_distilbert\seegull_gpt_augmentation_trained\tokenizer_config.json
Special tokens file saved in model_output_distilbert\seegull_gpt_augmentation_trained\special_tokens_map.json
[codecarbon INFO @ 14:34:50] Energy consumed for RAM : 0.000071 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:34:50] Energy consumed for all GPUs : 0.000314 kWh. Total GPU Power : 78.04151228005186 W
[codecarbon INFO @ 14:34:50] Energy consumed for all CPUs : 0.000171 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:34:50] 0.000557 kWh of electricity used since the beginning.
loading configuration file model_output_distilbert/seegull_gpt_augmentation_trained\config.json
Model config DistilBertConfig {
  "_name_or_path": "model_output_distilbert/seegull_gpt_augmentation_trained",
  "activation": "gelu",
  "architectures": [
    "DistilBertForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "problem_type": "single_label_classification",
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "vocab_size": 30522
}

loading weights file model_output_distilbert/seegull_gpt_augmentation_trained\model.safetensors
Estimated total emissions: 0.0001322316995192557 kg CO2
Number of unique labels: 2
All model checkpoint weights were used when initializing DistilBertForSequenceClassification.

All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at model_output_distilbert/seegull_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 653/653 [00:00<00:00, 30889.86 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 653/653 [00:00<00:00, 25406.58 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3236, 'input_ids': [101, 2087, 2111, 2040, 2024, 5637, 2024, 20497, 2015, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
loading configuration file model_output_distilbert/seegull_gpt_augmentation_trained\config.json
Model config DistilBertConfig {
  "_name_or_path": "model_output_distilbert/seegull_gpt_augmentation_trained",
  "activation": "gelu",
  "architectures": [
    "DistilBertForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "problem_type": "single_label_classification",
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "vocab_size": 30522
}

loading weights file model_output_distilbert/seegull_gpt_augmentation_trained\model.safetensors
Number of unique labels: 2
All model checkpoint weights were used when initializing DistilBertForSequenceClassification.

All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at model_output_distilbert/seegull_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 414/414 [00:00<00:00, 29550.92 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 414/414 [00:00<00:00, 23306.69 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 840, 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'labels': 0}
loading configuration file model_output_distilbert/seegull_gpt_augmentation_trained\config.json
Model config DistilBertConfig {
  "_name_or_path": "model_output_distilbert/seegull_gpt_augmentation_trained",
  "activation": "gelu",
  "architectures": [
    "DistilBertForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "problem_type": "single_label_classification",
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "vocab_size": 30522
}

loading weights file model_output_distilbert/seegull_gpt_augmentation_trained\model.safetensors
Number of unique labels: 2
All model checkpoint weights were used when initializing DistilBertForSequenceClassification.

All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at model_output_distilbert/seegull_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10374/10374 [00:00<00:00, 25614.21 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10374/10374 [00:00<00:00, 27605.06 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Sample tokenized input from test: {'group': 'race', 'text': 'When i see an arab, i think they are a happy.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 17851, 'input_ids': [101, 2043, 1045, 2156, 2019, 5424, 1010, 1045, 2228, 2027, 2024, 1037, 3407, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
Number of unique labels: 2
loading configuration file model_output_distilbert/seegull_gpt_augmentation_trained\config.json
Model config DistilBertConfig {
  "_name_or_path": "model_output_distilbert/seegull_gpt_augmentation_trained",
  "activation": "gelu",
  "architectures": [
    "DistilBertForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "problem_type": "single_label_classification",
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "vocab_size": 30522
}

loading weights file model_output_distilbert/seegull_gpt_augmentation_trained\model.safetensors
All model checkpoint weights were used when initializing DistilBertForSequenceClassification.

All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at model_output_distilbert/seegull_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11441/11441 [00:00<00:00, 26923.72 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11441/11441 [00:00<00:00, 31539.35 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
Number of unique labels: 2
[codecarbon INFO @ 14:39:22] [setup] RAM Tracking...
[codecarbon INFO @ 14:39:22] [setup] GPU Tracking...
[codecarbon INFO @ 14:39:22] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 14:39:22] [setup] CPU Tracking...
[codecarbon WARNING @ 14:39:22] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon WARNING @ 14:39:24] We saw that you have a AMD Ryzen 7 7700 8-Core Processor but we don't know it. Please contact us.
[codecarbon INFO @ 14:39:24] CPU Model on constant consumption mode: AMD Ryzen 7 7700 8-Core Processor
[codecarbon INFO @ 14:39:24] >>> Tracker's metadata:
[codecarbon INFO @ 14:39:24]   Platform system: Windows-11-10.0.26100-SP0
[codecarbon INFO @ 14:39:24]   Python version: 3.12.12
[codecarbon INFO @ 14:39:24]   CodeCarbon version: 2.8.0
[codecarbon INFO @ 14:39:24]   Available RAM : 47.116 GB
[codecarbon INFO @ 14:39:24]   CPU count: 16
[codecarbon INFO @ 14:39:24]   CPU model: AMD Ryzen 7 7700 8-Core Processor
[codecarbon INFO @ 14:39:24]   GPU count: 1
[codecarbon INFO @ 14:39:24]   GPU model: 1 x NVIDIA GeForce RTX 5060 Ti
[codecarbon INFO @ 14:39:27] Saving emissions data to file D:\UCL\HEARTS-Text-Stereotype-Detection-main\Model Training and Evaluation\emissions.csv
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--distilbert--distilbert-base-uncased\snapshots\12040accade4e8a0f71eabdb258fecc2e7e948be\config.json
Model config DistilBertConfig {
  "_name_or_path": "distilbert/distilbert-base-uncased",
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.46.3",
  "vocab_size": 30522
}

loading weights file model.safetensors from cache at C:\Users\32618\.cache\huggingface\hub\models--distilbert--distilbert-base-uncased\snapshots\12040accade4e8a0f71eabdb258fecc2e7e948be\model.safetensors
Some weights of the model checkpoint at distilbert/distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--distilbert--distilbert-base-uncased\snapshots\12040accade4e8a0f71eabdb258fecc2e7e948be\config.json
Model config DistilBertConfig {
  "_name_or_path": "distilbert/distilbert-base-uncased",
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.46.3",
  "vocab_size": 30522
}

loading file vocab.txt from cache at C:\Users\32618\.cache\huggingface\hub\models--distilbert--distilbert-base-uncased\snapshots\12040accade4e8a0f71eabdb258fecc2e7e948be\vocab.txt
loading file tokenizer.json from cache at C:\Users\32618\.cache\huggingface\hub\models--distilbert--distilbert-base-uncased\snapshots\12040accade4e8a0f71eabdb258fecc2e7e948be\tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--distilbert--distilbert-base-uncased\snapshots\12040accade4e8a0f71eabdb258fecc2e7e948be\tokenizer_config.json
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--distilbert--distilbert-base-uncased\snapshots\12040accade4e8a0f71eabdb258fecc2e7e948be\config.json
Model config DistilBertConfig {
  "_name_or_path": "distilbert/distilbert-base-uncased",
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.46.3",
  "vocab_size": 30522
}

Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36608/36608 [00:02<00:00, 17681.54 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36608/36608 [00:01<00:00, 22817.36 examples/s]
Map:   0%|          | 0/9152 [00:00<?, ? examples/s]Sample tokenized input from train: {'text': 'The commander could tell the soldier was disciplined.', 'label': 1, 'group': 'profession', 'data_name': 'MGSD', '__index_level_0__': 7380, 'input_ids': [101, 1996, 3474, 2071, 2425, 1996, 5268, 2001, 28675, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1}
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9152/9152 [00:00<00:00, 25906.40 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9152/9152 [00:00<00:00, 19626.83 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
PyTorch: setting up devices
Sample tokenized input from validation: {'text': 'The commander could tell the soldier was disciplined.', 'label': 1, 'group': 'profession', 'data_name': 'MGSD', '__index_level_0__': 7380, 'input_ids': [101, 1996, 3474, 2071, 2425, 1996, 5268, 2001, 28675, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1}
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
D:\UCL\HEARTS-Text-Stereotype-Detection-main\Model Training and Evaluation\BERT_Models_Fine_Tuning.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
[codecarbon ERROR @ 14:39:33] Error: Another instance of codecarbon is probably running as we find `C:\Users\32618\AppData\Local\Temp\.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.
The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running training *****
  Num examples = 36,608
  Num Epochs = 6
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 3,432
  Number of trainable parameters = 66,955,010
[codecarbon WARNING @ 14:39:33] Another instance of codecarbon is already running. Exiting.
  2%|â–         | 81/3432 [00:08<06:09,  9.07it/s][codecarbon INFO @ 14:39:42] Energy consumed for RAM : 0.000074 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:39:42] Energy consumed for all GPUs : 0.000408 kWh. Total GPU Power : 97.84415354565115 W
[codecarbon INFO @ 14:39:42] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:39:42] 0.000659 kWh of electricity used since the beginning.
  6%|â–‹         | 215/3432 [00:23<05:55,  9.05it/s][codecarbon INFO @ 14:39:57] Energy consumed for RAM : 0.000147 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:39:57] Energy consumed for all GPUs : 0.001074 kWh. Total GPU Power : 159.77743612723202 W
[codecarbon INFO @ 14:39:57] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:39:57] 0.001575 kWh of electricity used since the beginning.
 10%|â–ˆ         | 350/3432 [00:38<05:47,  8.86it/s][codecarbon INFO @ 14:40:12] Energy consumed for RAM : 0.000221 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:40:12] Energy consumed for all GPUs : 0.001755 kWh. Total GPU Power : 163.40415134743392 W
[codecarbon INFO @ 14:40:12] Energy consumed for all CPUs : 0.000531 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:40:12] 0.002507 kWh of electricity used since the beginning.
 14%|â–ˆâ–        | 486/3432 [00:53<05:29,  8.94it/s][codecarbon INFO @ 14:40:28] Energy consumed for RAM : 0.000295 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:40:28] Energy consumed for all GPUs : 0.002440 kWh. Total GPU Power : 164.51573981682046 W
[codecarbon INFO @ 14:40:28] Energy consumed for all CPUs : 0.000709 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:40:28] 0.003444 kWh of electricity used since the beginning.
 15%|â–ˆâ–        | 500/3432 [00:55<05:25,  9.00it/s]{'loss': 0.4731, 'grad_norm': 2.8645565509796143, 'learning_rate': 1.7086247086247088e-05, 'epoch': 0.87}
 17%|â–ˆâ–‹        | 572/3432 [01:03<05:13,  9.12it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 9152
  Batch size = 64

  0%|          | 0/143 [00:00<?, ?it/s]
  3%|â–Ž         | 5/143 [00:00<00:03, 37.81it/s]
  6%|â–‹         | 9/143 [00:00<00:04, 33.20it/s]
  9%|â–‰         | 13/143 [00:00<00:03, 32.76it/s]
 12%|â–ˆâ–        | 17/143 [00:00<00:04, 30.17it/s]
 15%|â–ˆâ–        | 21/143 [00:00<00:04, 27.51it/s]
 17%|â–ˆâ–‹        | 24/143 [00:00<00:04, 26.26it/s]
 19%|â–ˆâ–‰        | 27/143 [00:00<00:04, 25.47it/s]
 21%|â–ˆâ–ˆ        | 30/143 [00:01<00:04, 24.85it/s]
 23%|â–ˆâ–ˆâ–Ž       | 33/143 [00:01<00:04, 25.01it/s]
 25%|â–ˆâ–ˆâ–Œ       | 36/143 [00:01<00:04, 26.14it/s]
 27%|â–ˆâ–ˆâ–‹       | 39/143 [00:01<00:03, 26.96it/s]
 29%|â–ˆâ–ˆâ–‰       | 42/143 [00:01<00:03, 27.64it/s]
 31%|â–ˆâ–ˆâ–ˆâ–      | 45/143 [00:01<00:03, 27.91it/s]
 34%|â–ˆâ–ˆâ–ˆâ–      | 49/143 [00:01<00:03, 29.15it/s]
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 53/143 [00:01<00:02, 30.46it/s]
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 57/143 [00:01<00:02, 31.75it/s]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 61/143 [00:02<00:02, 32.52it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 65/143 [00:02<00:02, 31.92it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 69/143 [00:02<00:02, 30.95it/s]
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 73/143 [00:02<00:02, 30.64it/s]
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 77/143 [00:02<00:02, 30.54it/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 81/143 [00:02<00:02, 30.57it/s]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 85/143 [00:02<00:01, 30.27it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 89/143 [00:03<00:01, 30.01it/s]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 93/143 [00:03<00:01, 29.94it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 96/143 [00:03<00:01, 29.34it/s]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 99/143 [00:03<00:01, 28.98it/s]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 102/143 [00:03<00:01, 28.59it/s]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 105/143 [00:03<00:01, 28.33it/s]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 108/143 [00:03<00:01, 27.53it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 112/143 [00:03<00:01, 28.79it/s]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 116/143 [00:03<00:00, 30.50it/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 120/143 [00:04<00:00, 31.84it/s]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 124/143 [00:04<00:00, 32.87it/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 128/143 [00:04<00:00, 33.20it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 132/143 [00:04<00:00, 33.79it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 136/143 [00:04<00:00, 33.68it/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 140/143 [00:04<00:00, 34.15it/s]
                                                  
 17%|â–ˆâ–‹        | 572/3432 [01:08<05:13,  9.12it/s]
                                                 Saving model checkpoint to model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-572
Configuration saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-572\config.json
{'eval_loss': 0.39745089411735535, 'eval_precision': 0.780507442106942, 'eval_recall': 0.7890342858975592, 'eval_f1': 0.7842337811462705, 'eval_balanced accuracy': 0.7890342858975592, 'eval_runtime': 4.781, 'eval_samples_per_second': 1914.234, 'eval_steps_per_second': 29.91, 'epoch': 1.0}
Model weights saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-572\model.safetensors
tokenizer config file saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-572\tokenizer_config.json
Special tokens file saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-572\special_tokens_map.json
[codecarbon INFO @ 14:40:43] Energy consumed for RAM : 0.000368 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:40:43] Energy consumed for all GPUs : 0.003116 kWh. Total GPU Power : 162.06524512898972 W
[codecarbon INFO @ 14:40:43] Energy consumed for all CPUs : 0.000886 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:40:43] 0.004369 kWh of electricity used since the beginning.
 21%|â–ˆâ–ˆ        | 705/3432 [01:23<05:06,  8.90it/s][codecarbon INFO @ 14:40:58] Energy consumed for RAM : 0.000442 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:40:58] Energy consumed for all GPUs : 0.003794 kWh. Total GPU Power : 162.60145057223835 W
[codecarbon INFO @ 14:40:58] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:40:58] 0.005298 kWh of electricity used since the beginning.
 24%|â–ˆâ–ˆâ–       | 839/3432 [01:38<04:51,  8.89it/s][codecarbon INFO @ 14:41:13] Energy consumed for RAM : 0.000515 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:41:13] Energy consumed for all GPUs : 0.004488 kWh. Total GPU Power : 166.63282041918356 W
[codecarbon INFO @ 14:41:13] Energy consumed for all CPUs : 0.001240 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:41:13] 0.006243 kWh of electricity used since the beginning.
 28%|â–ˆâ–ˆâ–Š       | 973/3432 [01:53<04:34,  8.95it/s][codecarbon INFO @ 14:41:28] Energy consumed for RAM : 0.000589 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:41:28] Energy consumed for all GPUs : 0.005177 kWh. Total GPU Power : 165.13137448890015 W
[codecarbon INFO @ 14:41:28] Energy consumed for all CPUs : 0.001417 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:41:28] 0.007183 kWh of electricity used since the beginning.
[codecarbon INFO @ 14:41:28] 0.014216 g.CO2eq/s mean an estimation of 448.3150140566058 kg.CO2eq/year
 29%|â–ˆâ–ˆâ–‰       | 1000/3432 [01:57<04:31,  8.94it/s]{'loss': 0.3442, 'grad_norm': 4.500096797943115, 'learning_rate': 1.4172494172494174e-05, 'epoch': 1.75}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1107/3432 [02:08<04:22,  8.86it/s][codecarbon INFO @ 14:41:43] Energy consumed for RAM : 0.000663 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:41:43] Energy consumed for all GPUs : 0.005867 kWh. Total GPU Power : 165.4864538037189 W
[codecarbon INFO @ 14:41:43] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:41:43] 0.008124 kWh of electricity used since the beginning.
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1144/3432 [02:13<04:12,  9.05it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 9152
  Batch size = 64

  0%|          | 0/143 [00:00<?, ?it/s]
  3%|â–Ž         | 5/143 [00:00<00:03, 38.59it/s]
  6%|â–‹         | 9/143 [00:00<00:04, 33.36it/s]
  9%|â–‰         | 13/143 [00:00<00:04, 31.79it/s]
 12%|â–ˆâ–        | 17/143 [00:00<00:04, 29.16it/s]
 14%|â–ˆâ–        | 20/143 [00:00<00:04, 27.25it/s]
 16%|â–ˆâ–Œ        | 23/143 [00:00<00:04, 25.90it/s]
 18%|â–ˆâ–Š        | 26/143 [00:00<00:04, 25.22it/s]
 20%|â–ˆâ–ˆ        | 29/143 [00:01<00:04, 24.48it/s]
 22%|â–ˆâ–ˆâ–       | 32/143 [00:01<00:04, 24.20it/s]
 24%|â–ˆâ–ˆâ–       | 35/143 [00:01<00:04, 25.28it/s]
 27%|â–ˆâ–ˆâ–‹       | 38/143 [00:01<00:03, 26.30it/s]
 29%|â–ˆâ–ˆâ–Š       | 41/143 [00:01<00:03, 27.25it/s]
 31%|â–ˆâ–ˆâ–ˆ       | 44/143 [00:01<00:03, 27.21it/s]
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 47/143 [00:01<00:03, 27.42it/s]
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 51/143 [00:01<00:03, 29.42it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 55/143 [00:01<00:02, 30.75it/s]
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 59/143 [00:02<00:02, 31.56it/s]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 63/143 [00:02<00:02, 31.73it/s]
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 67/143 [00:02<00:02, 30.71it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 71/143 [00:02<00:02, 30.29it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 75/143 [00:02<00:02, 29.70it/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78/143 [00:02<00:02, 29.68it/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 81/143 [00:02<00:02, 29.04it/s]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 84/143 [00:02<00:02, 29.07it/s]
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 87/143 [00:03<00:01, 29.05it/s]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 90/143 [00:03<00:01, 29.15it/s]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 93/143 [00:03<00:01, 29.14it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 96/143 [00:03<00:01, 28.21it/s]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 99/143 [00:03<00:01, 27.70it/s]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 102/143 [00:03<00:01, 27.39it/s]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 105/143 [00:03<00:01, 27.52it/s]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 108/143 [00:03<00:01, 27.17it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 111/143 [00:03<00:01, 27.85it/s]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 115/143 [00:04<00:00, 29.53it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 119/143 [00:04<00:00, 31.03it/s]
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 123/143 [00:04<00:00, 32.03it/s]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 127/143 [00:04<00:00, 32.13it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 131/143 [00:04<00:00, 32.66it/s]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 135/143 [00:04<00:00, 32.87it/s]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 139/143 [00:04<00:00, 33.14it/s]
{'eval_loss': 0.36988553404808044, 'eval_precision': 0.8021366155157715, 'eval_recall': 0.8078249054579276, 'eval_f1': 0.8047784004469933, 'eval_balanced accuracy': 0.8078249054579276, 'eval_runtime': 4.8969, 'eval_samples_per_second': 1868.944, 'eval_steps_per_second': 29.202, 'epoch': 2.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:04<00:00, 34.06it/s]
                                                   
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1144/3432 [02:18<04:12,  9.05it/s]
                                                 Saving model checkpoint to model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1144
Configuration saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1144\config.json
Model weights saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1144\model.safetensors
tokenizer config file saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1144\tokenizer_config.json
Special tokens file saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1144\special_tokens_map.json
Deleting older checkpoint [model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-572] due to args.save_total_limit
 35%|â–ˆâ–ˆâ–ˆâ–      | 1189/3432 [02:23<04:17,  8.71it/s][codecarbon INFO @ 14:41:58] Energy consumed for RAM : 0.000736 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:41:58] Energy consumed for all GPUs : 0.006531 kWh. Total GPU Power : 159.35961599233505 W
[codecarbon INFO @ 14:41:58] Energy consumed for all CPUs : 0.001772 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:41:58] 0.009039 kWh of electricity used since the beginning.
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1321/3432 [02:39<03:57,  8.89it/s][codecarbon INFO @ 14:42:13] Energy consumed for RAM : 0.000810 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:42:13] Energy consumed for all GPUs : 0.007213 kWh. Total GPU Power : 163.58118601650423 W
[codecarbon INFO @ 14:42:13] Energy consumed for all CPUs : 0.001949 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:42:13] 0.009972 kWh of electricity used since the beginning.
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1454/3432 [02:54<03:44,  8.82it/s][codecarbon INFO @ 14:42:28] Energy consumed for RAM : 0.000884 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:42:28] Energy consumed for all GPUs : 0.007900 kWh. Total GPU Power : 164.86357848001853 W
[codecarbon INFO @ 14:42:28] Energy consumed for all CPUs : 0.002126 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:42:28] 0.010910 kWh of electricity used since the beginning.
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1500/3432 [02:59<03:40,  8.75it/s]{'loss': 0.2763, 'grad_norm': 4.206209659576416, 'learning_rate': 1.1258741258741259e-05, 'epoch': 2.62}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1586/3432 [03:08<03:28,  8.87it/s][codecarbon INFO @ 14:42:43] Energy consumed for RAM : 0.000957 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:42:43] Energy consumed for all GPUs : 0.008587 kWh. Total GPU Power : 164.83180968580626 W
[codecarbon INFO @ 14:42:43] Energy consumed for all CPUs : 0.002303 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:42:43] 0.011847 kWh of electricity used since the beginning.
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1716/3432 [03:23<03:10,  9.01it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 9152
  Batch size = 64

  0%|          | 0/143 [00:00<?, ?it/s]
  3%|â–Ž         | 5/143 [00:00<00:03, 38.85it/s]
  6%|â–‹         | 9/143 [00:00<00:03, 33.87it/s]
  9%|â–‰         | 13/143 [00:00<00:03, 33.05it/s][codecarbon INFO @ 14:42:58] Energy consumed for RAM : 0.001031 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:42:58] Energy consumed for all GPUs : 0.009282 kWh. Total GPU Power : 166.569657092022 W
[codecarbon INFO @ 14:42:58] Energy consumed for all CPUs : 0.002480 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:42:58] 0.012793 kWh of electricity used since the beginning.

 12%|â–ˆâ–        | 17/143 [00:00<00:04, 30.09it/s]
 15%|â–ˆâ–        | 21/143 [00:00<00:04, 27.37it/s]
 17%|â–ˆâ–‹        | 24/143 [00:00<00:04, 26.05it/s]
 19%|â–ˆâ–‰        | 27/143 [00:00<00:04, 25.30it/s]
 21%|â–ˆâ–ˆ        | 30/143 [00:01<00:04, 24.63it/s]
 23%|â–ˆâ–ˆâ–Ž       | 33/143 [00:01<00:04, 24.79it/s]
 25%|â–ˆâ–ˆâ–Œ       | 36/143 [00:01<00:04, 25.94it/s]
 27%|â–ˆâ–ˆâ–‹       | 39/143 [00:01<00:03, 26.62it/s]
 29%|â–ˆâ–ˆâ–‰       | 42/143 [00:01<00:03, 27.41it/s]
 31%|â–ˆâ–ˆâ–ˆâ–      | 45/143 [00:01<00:03, 27.62it/s]
 34%|â–ˆâ–ˆâ–ˆâ–      | 49/143 [00:01<00:03, 28.98it/s]
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 53/143 [00:01<00:02, 30.41it/s]
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 57/143 [00:01<00:02, 31.72it/s]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 61/143 [00:02<00:02, 32.58it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 65/143 [00:02<00:02, 32.07it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 69/143 [00:02<00:02, 31.47it/s]
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 73/143 [00:02<00:02, 31.13it/s]
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 77/143 [00:02<00:02, 30.49it/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 81/143 [00:02<00:02, 30.34it/s]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 85/143 [00:02<00:01, 30.17it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 89/143 [00:03<00:01, 30.19it/s]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 93/143 [00:03<00:01, 30.13it/s]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 97/143 [00:03<00:01, 29.31it/s]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 100/143 [00:03<00:01, 28.71it/s]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 103/143 [00:03<00:01, 28.46it/s]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 106/143 [00:03<00:01, 28.24it/s]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 109/143 [00:03<00:01, 27.90it/s]
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 113/143 [00:03<00:01, 29.45it/s]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 117/143 [00:03<00:00, 30.91it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 121/143 [00:04<00:00, 32.08it/s]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 125/143 [00:04<00:00, 32.91it/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 129/143 [00:04<00:00, 33.55it/s]
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 133/143 [00:04<00:00, 33.84it/s]
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 137/143 [00:04<00:00, 34.01it/s]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 141/143 [00:04<00:00, 34.08it/s]
                                                   
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1716/3432 [03:28<03:10,  9.01it/s]
                                                 Saving model checkpoint to model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1716
Configuration saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1716\config.json
{'eval_loss': 0.38594505190849304, 'eval_precision': 0.807370813860024, 'eval_recall': 0.8227938968244943, 'eval_f1': 0.8133910157776021, 'eval_balanced accuracy': 0.8227938968244943, 'eval_runtime': 4.7799, 'eval_samples_per_second': 1914.701, 'eval_steps_per_second': 29.917, 'epoch': 3.0}
Model weights saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1716\model.safetensors
tokenizer config file saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1716\tokenizer_config.json
Special tokens file saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1716\special_tokens_map.json
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1803/3432 [03:39<03:03,  8.86it/s][codecarbon INFO @ 14:43:13] Energy consumed for RAM : 0.001105 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:43:13] Energy consumed for all GPUs : 0.009954 kWh. Total GPU Power : 161.40005886282472 W
[codecarbon INFO @ 14:43:13] Energy consumed for all CPUs : 0.002657 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:43:13] 0.013716 kWh of electricity used since the beginning.
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1937/3432 [03:54<02:46,  8.99it/s][codecarbon INFO @ 14:43:28] Energy consumed for RAM : 0.001178 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:43:28] Energy consumed for all GPUs : 0.010648 kWh. Total GPU Power : 166.27981848553807 W
[codecarbon INFO @ 14:43:28] Energy consumed for all CPUs : 0.002835 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:43:28] 0.014661 kWh of electricity used since the beginning.
[codecarbon INFO @ 14:43:28] 0.014797 g.CO2eq/s mean an estimation of 466.6316521487052 kg.CO2eq/year
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2000/3432 [04:01<02:38,  9.05it/s]{'loss': 0.215, 'grad_norm': 4.988974571228027, 'learning_rate': 8.344988344988347e-06, 'epoch': 3.5}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2072/3432 [04:09<02:29,  9.11it/s][codecarbon INFO @ 14:43:43] Energy consumed for RAM : 0.001252 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:43:43] Energy consumed for all GPUs : 0.011354 kWh. Total GPU Power : 169.3006943804669 W
[codecarbon INFO @ 14:43:43] Energy consumed for all CPUs : 0.003012 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:43:43] 0.015618 kWh of electricity used since the beginning.
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2207/3432 [04:24<02:15,  9.04it/s][codecarbon INFO @ 14:43:58] Energy consumed for RAM : 0.001326 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:43:58] Energy consumed for all GPUs : 0.012055 kWh. Total GPU Power : 168.5245357428226 W
[codecarbon INFO @ 14:43:58] Energy consumed for all CPUs : 0.003189 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:43:58] 0.016570 kWh of electricity used since the beginning.
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2288/3432 [04:33<02:06,  9.02it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 9152
  Batch size = 64

  0%|          | 0/143 [00:00<?, ?it/s]
  3%|â–Ž         | 5/143 [00:00<00:03, 37.78it/s]
  6%|â–‹         | 9/143 [00:00<00:03, 34.42it/s]
  9%|â–‰         | 13/143 [00:00<00:03, 33.14it/s]
 12%|â–ˆâ–        | 17/143 [00:00<00:04, 30.49it/s]
 15%|â–ˆâ–        | 21/143 [00:00<00:04, 27.42it/s]
 17%|â–ˆâ–‹        | 24/143 [00:00<00:04, 26.31it/s]
 19%|â–ˆâ–‰        | 27/143 [00:00<00:04, 25.36it/s]
 21%|â–ˆâ–ˆ        | 30/143 [00:01<00:04, 24.86it/s]
 23%|â–ˆâ–ˆâ–Ž       | 33/143 [00:01<00:04, 24.89it/s]
 25%|â–ˆâ–ˆâ–Œ       | 36/143 [00:01<00:04, 25.96it/s]
 27%|â–ˆâ–ˆâ–‹       | 39/143 [00:01<00:03, 26.86it/s]
 29%|â–ˆâ–ˆâ–‰       | 42/143 [00:01<00:03, 27.29it/s]
 31%|â–ˆâ–ˆâ–ˆâ–      | 45/143 [00:01<00:03, 27.85it/s]
 34%|â–ˆâ–ˆâ–ˆâ–      | 49/143 [00:01<00:03, 29.14it/s]
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 53/143 [00:01<00:02, 30.84it/s]
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 57/143 [00:01<00:02, 31.87it/s]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 61/143 [00:02<00:02, 32.69it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 65/143 [00:02<00:02, 32.27it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 69/143 [00:02<00:02, 31.69it/s]
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 73/143 [00:02<00:02, 31.27it/s]
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 77/143 [00:02<00:02, 31.07it/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 81/143 [00:02<00:02, 30.74it/s]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 85/143 [00:02<00:01, 30.73it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 89/143 [00:03<00:01, 30.38it/s]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 93/143 [00:03<00:01, 30.39it/s]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 97/143 [00:03<00:01, 29.48it/s]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 100/143 [00:03<00:01, 29.10it/s]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 103/143 [00:03<00:01, 28.77it/s]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 106/143 [00:03<00:01, 28.21it/s]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 109/143 [00:03<00:01, 28.13it/s]
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 113/143 [00:03<00:01, 29.64it/s]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 117/143 [00:03<00:00, 31.10it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 121/143 [00:04<00:00, 32.37it/s]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 125/143 [00:04<00:00, 33.05it/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 129/143 [00:04<00:00, 33.60it/s]
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 133/143 [00:04<00:00, 33.25it/s]
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 137/143 [00:04<00:00, 33.24it/s]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 141/143 [00:04<00:00, 33.74it/s]
                                                   
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2288/3432 [04:37<02:06,  9.02it/s]
                                                 Saving model checkpoint to model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2288
Configuration saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2288\config.json
{'eval_loss': 0.42641767859458923, 'eval_precision': 0.8154930882677562, 'eval_recall': 0.8212570232295047, 'eval_f1': 0.8181810709320243, 'eval_balanced accuracy': 0.8212570232295047, 'eval_runtime': 4.7582, 'eval_samples_per_second': 1923.416, 'eval_steps_per_second': 30.053, 'epoch': 4.0}
Model weights saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2288\model.safetensors
tokenizer config file saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2288\tokenizer_config.json
Special tokens file saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2288\special_tokens_map.json
Deleting older checkpoint [model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1716] due to args.save_total_limit
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2293/3432 [04:39<09:40,  1.96it/s][codecarbon INFO @ 14:44:13] Energy consumed for RAM : 0.001399 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:44:13] Energy consumed for all GPUs : 0.012738 kWh. Total GPU Power : 163.7515106405808 W
[codecarbon INFO @ 14:44:13] Energy consumed for all CPUs : 0.003366 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:44:13] 0.017504 kWh of electricity used since the beginning.
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2427/3432 [04:54<01:51,  8.97it/s][codecarbon INFO @ 14:44:28] Energy consumed for RAM : 0.001473 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:44:28] Energy consumed for all GPUs : 0.013434 kWh. Total GPU Power : 166.90365674487032 W
[codecarbon INFO @ 14:44:28] Energy consumed for all CPUs : 0.003543 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:44:28] 0.018450 kWh of electricity used since the beginning.
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2500/3432 [05:02<01:43,  8.99it/s]{'loss': 0.1716, 'grad_norm': 8.174759864807129, 'learning_rate': 5.431235431235432e-06, 'epoch': 4.37}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2562/3432 [05:09<01:35,  9.08it/s][codecarbon INFO @ 14:44:43] Energy consumed for RAM : 0.001547 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:44:43] Energy consumed for all GPUs : 0.014137 kWh. Total GPU Power : 168.60568457537056 W
[codecarbon INFO @ 14:44:43] Energy consumed for all CPUs : 0.003721 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:44:43] 0.019404 kWh of electricity used since the beginning.
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2698/3432 [05:24<01:21,  8.96it/s][codecarbon INFO @ 14:44:58] Energy consumed for RAM : 0.001620 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:44:58] Energy consumed for all GPUs : 0.014841 kWh. Total GPU Power : 168.90437636365354 W
[codecarbon INFO @ 14:44:58] Energy consumed for all CPUs : 0.003898 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:44:58] 0.020360 kWh of electricity used since the beginning.
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2833/3432 [05:39<01:06,  9.02it/s][codecarbon INFO @ 14:45:13] Energy consumed for RAM : 0.001694 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:45:13] Energy consumed for all GPUs : 0.015545 kWh. Total GPU Power : 168.73621436834125 W
[codecarbon INFO @ 14:45:13] Energy consumed for all CPUs : 0.004075 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:45:13] 0.021314 kWh of electricity used since the beginning.
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2860/3432 [05:42<01:02,  9.15it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 9152
  Batch size = 64

  0%|          | 0/143 [00:00<?, ?it/s]
  3%|â–Ž         | 5/143 [00:00<00:03, 38.26it/s]
  6%|â–‹         | 9/143 [00:00<00:03, 34.31it/s]
  9%|â–‰         | 13/143 [00:00<00:03, 33.29it/s]
 12%|â–ˆâ–        | 17/143 [00:00<00:04, 30.39it/s]
 15%|â–ˆâ–        | 21/143 [00:00<00:04, 27.56it/s]
 17%|â–ˆâ–‹        | 24/143 [00:00<00:04, 26.26it/s]
 19%|â–ˆâ–‰        | 27/143 [00:00<00:04, 25.46it/s]
 21%|â–ˆâ–ˆ        | 30/143 [00:01<00:04, 24.79it/s]
 23%|â–ˆâ–ˆâ–Ž       | 33/143 [00:01<00:04, 24.86it/s]
 25%|â–ˆâ–ˆâ–Œ       | 36/143 [00:01<00:04, 26.04it/s]
 27%|â–ˆâ–ˆâ–‹       | 39/143 [00:01<00:03, 26.78it/s]
 29%|â–ˆâ–ˆâ–‰       | 42/143 [00:01<00:03, 27.46it/s]
 31%|â–ˆâ–ˆâ–ˆâ–      | 45/143 [00:01<00:03, 27.67it/s]
 34%|â–ˆâ–ˆâ–ˆâ–      | 49/143 [00:01<00:03, 29.08it/s]
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 53/143 [00:01<00:02, 30.81it/s]
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 57/143 [00:01<00:02, 32.01it/s]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 61/143 [00:02<00:02, 32.66it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 65/143 [00:02<00:02, 32.28it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 69/143 [00:02<00:02, 31.62it/s]
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 73/143 [00:02<00:02, 31.40it/s]
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 77/143 [00:02<00:02, 31.07it/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 81/143 [00:02<00:02, 30.78it/s]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 85/143 [00:02<00:01, 30.54it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 89/143 [00:03<00:01, 30.45it/s]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 93/143 [00:03<00:01, 30.13it/s]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 97/143 [00:03<00:01, 29.12it/s]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 100/143 [00:03<00:01, 28.67it/s]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 103/143 [00:03<00:01, 28.32it/s]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 106/143 [00:03<00:01, 28.18it/s]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 109/143 [00:03<00:01, 27.88it/s]
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 113/143 [00:03<00:01, 29.54it/s]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 117/143 [00:03<00:00, 31.01it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 121/143 [00:04<00:00, 32.23it/s]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 125/143 [00:04<00:00, 32.91it/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 129/143 [00:04<00:00, 33.59it/s]
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 133/143 [00:04<00:00, 33.89it/s]
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 137/143 [00:04<00:00, 34.36it/s]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 141/143 [00:04<00:00, 34.57it/s]
{'eval_loss': 0.46842390298843384, 'eval_precision': 0.8164651737537908, 'eval_recall': 0.827143269508323, 'eval_f1': 0.8210940807462491, 'eval_balanced accuracy': 0.827143269508323, 'eval_runtime': 4.757, 'eval_samples_per_second': 1923.891, 'eval_steps_per_second': 30.061, 'epoch': 5.0}
                                                   
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2860/3432 [05:46<01:02,  9.15it/s]
                                                 Saving model checkpoint to model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2860
Configuration saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2860\config.json
Model weights saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2860\model.safetensors
tokenizer config file saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2860\tokenizer_config.json
Special tokens file saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2860\special_tokens_map.json
Deleting older checkpoint [model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2288] due to args.save_total_limit
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2918/3432 [05:54<00:55,  9.21it/s][codecarbon INFO @ 14:45:28] Energy consumed for RAM : 0.001768 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:45:28] Energy consumed for all GPUs : 0.016229 kWh. Total GPU Power : 164.3025519659559 W
[codecarbon INFO @ 14:45:28] Energy consumed for all CPUs : 0.004252 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:45:28] 0.022249 kWh of electricity used since the beginning.
[codecarbon INFO @ 14:45:28] 0.015015 g.CO2eq/s mean an estimation of 473.52838082382743 kg.CO2eq/year
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3000/3432 [06:03<00:48,  8.97it/s]{'loss': 0.14, 'grad_norm': 5.166245460510254, 'learning_rate': 2.517482517482518e-06, 'epoch': 5.24}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3052/3432 [06:09<00:42,  9.02it/s][codecarbon INFO @ 14:45:43] Energy consumed for RAM : 0.001841 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:45:43] Energy consumed for all GPUs : 0.016929 kWh. Total GPU Power : 167.81933509063407 W
[codecarbon INFO @ 14:45:43] Energy consumed for all CPUs : 0.004429 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:45:43] 0.023199 kWh of electricity used since the beginning.
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3187/3432 [06:24<00:26,  9.10it/s][codecarbon INFO @ 14:45:58] Energy consumed for RAM : 0.001915 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:45:58] Energy consumed for all GPUs : 0.017634 kWh. Total GPU Power : 169.17697675644823 W
[codecarbon INFO @ 14:45:58] Energy consumed for all CPUs : 0.004607 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:45:58] 0.024156 kWh of electricity used since the beginning.
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3322/3432 [06:39<00:12,  9.05it/s][codecarbon INFO @ 14:46:13] Energy consumed for RAM : 0.001989 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:46:13] Energy consumed for all GPUs : 0.018345 kWh. Total GPU Power : 170.29675631386806 W
[codecarbon INFO @ 14:46:13] Energy consumed for all CPUs : 0.004784 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:46:13] 0.025117 kWh of electricity used since the beginning.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3432/3432 [06:51<00:00,  9.00it/s]Saving model checkpoint to model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432
Configuration saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432\config.json
Model weights saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432\model.safetensors
tokenizer config file saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432\tokenizer_config.json
Special tokens file saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432\special_tokens_map.json
Deleting older checkpoint [model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2860] due to args.save_total_limit
The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 9152
  Batch size = 64

  0%|          | 0/143 [00:00<?, ?it/s]
  3%|â–Ž         | 5/143 [00:00<00:03, 40.30it/s]
  7%|â–‹         | 10/143 [00:00<00:03, 34.15it/s]
 10%|â–‰         | 14/143 [00:00<00:03, 33.32it/s]
 13%|â–ˆâ–Ž        | 18/143 [00:00<00:04, 29.71it/s]
 15%|â–ˆâ–Œ        | 22/143 [00:00<00:04, 27.33it/s]
 17%|â–ˆâ–‹        | 25/143 [00:00<00:04, 26.08it/s]
 20%|â–ˆâ–‰        | 28/143 [00:00<00:04, 25.35it/s]
 22%|â–ˆâ–ˆâ–       | 31/143 [00:01<00:04, 24.77it/s]
 24%|â–ˆâ–ˆâ–       | 34/143 [00:01<00:04, 25.45it/s]
 26%|â–ˆâ–ˆâ–Œ       | 37/143 [00:01<00:04, 26.45it/s]
 28%|â–ˆâ–ˆâ–Š       | 40/143 [00:01<00:03, 26.99it/s]
 30%|â–ˆâ–ˆâ–ˆ       | 43/143 [00:01<00:03, 27.70it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 46/143 [00:01<00:03, 28.03it/s]
 35%|â–ˆâ–ˆâ–ˆâ–      | 50/143 [00:01<00:03, 29.52it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 54/143 [00:01<00:02, 31.11it/s]
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 58/143 [00:02<00:02, 32.17it/s][codecarbon INFO @ 14:46:28] Energy consumed for RAM : 0.002062 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:46:28] Energy consumed for all GPUs : 0.019026 kWh. Total GPU Power : 163.40364097834538 W
[codecarbon INFO @ 14:46:28] Energy consumed for all CPUs : 0.004961 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:46:28] 0.026049 kWh of electricity used since the beginning.

 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 62/143 [00:02<00:02, 32.93it/s]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 66/143 [00:02<00:02, 32.01it/s]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 70/143 [00:02<00:02, 31.33it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 74/143 [00:02<00:02, 31.02it/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78/143 [00:02<00:02, 30.96it/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 82/143 [00:02<00:01, 30.70it/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 86/143 [00:02<00:01, 30.33it/s]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 90/143 [00:03<00:01, 30.08it/s]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 94/143 [00:03<00:01, 29.91it/s]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 97/143 [00:03<00:01, 29.22it/s]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 100/143 [00:03<00:01, 28.79it/s]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 103/143 [00:03<00:01, 28.50it/s]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 106/143 [00:03<00:01, 27.37it/s]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 109/143 [00:03<00:01, 26.77it/s]
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 113/143 [00:03<00:01, 28.33it/s]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 117/143 [00:04<00:00, 27.72it/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 120/143 [00:04<00:00, 28.27it/s]
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 123/143 [00:04<00:00, 24.02it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 126/143 [00:04<00:00, 25.44it/s]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 130/143 [00:04<00:00, 27.51it/s]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 134/143 [00:04<00:00, 29.24it/s]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 138/143 [00:04<00:00, 30.92it/s]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 142/143 [00:04<00:00, 32.25it/s]
{'eval_loss': 0.5027534365653992, 'eval_precision': 0.816606678874781, 'eval_recall': 0.8252334759118171, 'eval_f1': 0.8204707430600661, 'eval_balanced accuracy': 0.8252334759118171, 'eval_runtime': 4.9344, 'eval_samples_per_second': 1854.742, 'eval_steps_per_second': 28.98, 'epoch': 6.0}
                                                   
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3432/3432 [06:57<00:00,  9.00it/s]
                                                 Saving model checkpoint to model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432
Configuration saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432\config.json
Model weights saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432\model.safetensors
tokenizer config file saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432\tokenizer_config.json
Special tokens file saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432\special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1144 (score: 0.36988553404808044).
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3432/3432 [06:57<00:00,  9.00it/s]Deleting older checkpoint [model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432] due to args.save_total_limit
{'train_runtime': 417.9259, 'train_samples_per_second': 525.567, 'train_steps_per_second': 8.212, 'train_loss': 0.2509405162784603, 'epoch': 6.0}
[codecarbon WARNING @ 14:46:31] Another instance of codecarbon is already running. Exiting.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3432/3432 [06:57<00:00,  8.21it/s]
Saving model checkpoint to model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained
Configuration saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\config.json
Model weights saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\model.safetensors
tokenizer config file saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\tokenizer_config.json
Special tokens file saved in model_output_distilbert\merged_winoqueer_seegull_gpt_augmentation_trained\special_tokens_map.json
[codecarbon INFO @ 14:46:32] Energy consumed for RAM : 0.002082 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:46:32] Energy consumed for all GPUs : 0.019166 kWh. Total GPU Power : 127.39046923695405 W
[codecarbon INFO @ 14:46:32] Energy consumed for all CPUs : 0.005008 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:46:32] 0.026256 kWh of electricity used since the beginning.
loading configuration file model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained\config.json
Model config DistilBertConfig {
  "_name_or_path": "model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained",
  "activation": "gelu",
  "architectures": [
    "DistilBertForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "problem_type": "single_label_classification",
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "vocab_size": 30522
}

loading weights file model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained\model.safetensors
Estimated total emissions: 0.006238102149749824 kg CO2
Number of unique labels: 2
All model checkpoint weights were used when initializing DistilBertForSequenceClassification.

All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 653/653 [00:00<00:00, 29809.00 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 653/653 [00:00<00:00, 24357.51 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3236, 'input_ids': [101, 2087, 2111, 2040, 2024, 5637, 2024, 20497, 2015, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
loading configuration file model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained\config.json
Model config DistilBertConfig {
  "_name_or_path": "model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained",
  "activation": "gelu",
  "architectures": [
    "DistilBertForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "problem_type": "single_label_classification",
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "vocab_size": 30522
}

loading weights file model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained\model.safetensors
Number of unique labels: 2
All model checkpoint weights were used when initializing DistilBertForSequenceClassification.

All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 414/414 [00:00<00:00, 29276.89 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 414/414 [00:00<00:00, 24613.62 examples/s]
Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 840, 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'labels': 0}
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
loading configuration file model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained\config.json
Model config DistilBertConfig {
  "_name_or_path": "model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained",
  "activation": "gelu",
  "architectures": [
    "DistilBertForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "problem_type": "single_label_classification",
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "vocab_size": 30522
}

loading weights file model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained\model.safetensors
Number of unique labels: 2
All model checkpoint weights were used when initializing DistilBertForSequenceClassification.

All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10374/10374 [00:00<00:00, 14727.49 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10374/10374 [00:00<00:00, 16962.99 examples/s]
Sample tokenized input from test: {'group': 'race', 'text': 'When i see an arab, i think they are a happy.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 17851, 'input_ids': [101, 2043, 1045, 2156, 2019, 5424, 1010, 1045, 2228, 2027, 2024, 1037, 3407, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Number of unique labels: 2
loading configuration file model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained\config.json
Model config DistilBertConfig {
  "_name_or_path": "model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained",
  "activation": "gelu",
  "architectures": [
    "DistilBertForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "problem_type": "single_label_classification",
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "vocab_size": 30522
}

loading weights file model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained\model.safetensors
All model checkpoint weights were used when initializing DistilBertForSequenceClassification.

All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11441/11441 [00:00<00:00, 26016.56 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11441/11441 [00:00<00:00, 22387.09 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
Number of unique labels: 2
[codecarbon INFO @ 14:51:06] [setup] RAM Tracking...
[codecarbon INFO @ 14:51:06] [setup] GPU Tracking...
[codecarbon INFO @ 14:51:06] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 14:51:06] [setup] CPU Tracking...
[codecarbon WARNING @ 14:51:06] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon WARNING @ 14:51:08] We saw that you have a AMD Ryzen 7 7700 8-Core Processor but we don't know it. Please contact us.
[codecarbon INFO @ 14:51:08] CPU Model on constant consumption mode: AMD Ryzen 7 7700 8-Core Processor
[codecarbon INFO @ 14:51:08] >>> Tracker's metadata:
[codecarbon INFO @ 14:51:08]   Platform system: Windows-11-10.0.26100-SP0
[codecarbon INFO @ 14:51:08]   Python version: 3.12.12
[codecarbon INFO @ 14:51:08]   CodeCarbon version: 2.8.0
[codecarbon INFO @ 14:51:08]   Available RAM : 47.116 GB
[codecarbon INFO @ 14:51:08]   CPU count: 16
[codecarbon INFO @ 14:51:08]   CPU model: AMD Ryzen 7 7700 8-Core Processor
[codecarbon INFO @ 14:51:08]   GPU count: 1
[codecarbon INFO @ 14:51:08]   GPU model: 1 x NVIDIA GeForce RTX 5060 Ti
[codecarbon INFO @ 14:51:11] Saving emissions data to file D:\UCL\HEARTS-Text-Stereotype-Detection-main\Model Training and Evaluation\emissions.csv
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--google-bert--bert-base-uncased\snapshots\86b5e0934494bd15c9632b12f734a8a67f723594\config.json
Model config BertConfig {
  "_name_or_path": "google-bert/bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[codecarbon INFO @ 14:51:26] Energy consumed for RAM : 0.000074 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:51:26] Energy consumed for all GPUs : 0.000037 kWh. Total GPU Power : 8.96177352557283 W
[codecarbon INFO @ 14:51:26] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:51:26] 0.000288 kWh of electricity used since the beginning.
[codecarbon INFO @ 14:51:41] Energy consumed for RAM : 0.000147 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:51:41] Energy consumed for all GPUs : 0.000077 kWh. Total GPU Power : 9.468711167179402 W
[codecarbon INFO @ 14:51:41] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:51:41] 0.000579 kWh of electricity used since the beginning.
loading weights file model.safetensors from cache at C:\Users\32618\.cache\huggingface\hub\models--google-bert--bert-base-uncased\snapshots\86b5e0934494bd15c9632b12f734a8a67f723594\model.safetensors
A pretrained model of type `BertForSequenceClassification` contains parameters that have been renamed internally (a few are listed below but more are present in the model):
* `bert.embeddings.LayerNorm.gamma` -> `bert.embeddings.LayerNorm.weight`
* `bert.encoder.layer.0.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`
* `bert.encoder.layer.0.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`
* `bert.encoder.layer.1.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`
* `bert.encoder.layer.1.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`
* `bert.encoder.layer.10.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`
* `bert.encoder.layer.10.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`
* `bert.encoder.layer.11.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`
* `bert.encoder.layer.11.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`
* `bert.encoder.layer.2.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`
* `bert.encoder.layer.2.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`
* `bert.encoder.layer.3.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`
* `bert.encoder.layer.3.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`
* `bert.encoder.layer.4.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`
* `bert.encoder.layer.4.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`
* `bert.encoder.layer.5.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`
* `bert.encoder.layer.5.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`
* `bert.encoder.layer.6.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`
* `bert.encoder.layer.6.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`
* `bert.encoder.layer.7.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`
* `bert.encoder.layer.7.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`
* `bert.encoder.layer.8.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`
* `bert.encoder.layer.8.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`
* `bert.encoder.layer.9.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`
* `bert.encoder.layer.9.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`
* `cls.predictions.transform.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`
* `bert.embeddings.LayerNorm.beta` -> `bert.embeddings.LayerNorm.bias`
* `bert.encoder.layer.0.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`
* `bert.encoder.layer.0.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`
* `bert.encoder.layer.1.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`
* `bert.encoder.layer.1.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`
* `bert.encoder.layer.10.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`
* `bert.encoder.layer.10.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`
* `bert.encoder.layer.11.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`
* `bert.encoder.layer.11.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`
* `bert.encoder.layer.2.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`
* `bert.encoder.layer.2.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`
* `bert.encoder.layer.3.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`
* `bert.encoder.layer.3.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`
* `bert.encoder.layer.4.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`
* `bert.encoder.layer.4.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`
* `bert.encoder.layer.5.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`
* `bert.encoder.layer.5.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`
* `bert.encoder.layer.6.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`
* `bert.encoder.layer.6.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`
* `bert.encoder.layer.7.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`
* `bert.encoder.layer.7.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`
* `bert.encoder.layer.8.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`
* `bert.encoder.layer.8.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`
* `bert.encoder.layer.9.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`
* `bert.encoder.layer.9.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`
* `cls.predictions.transform.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`
If you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.
Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--google-bert--bert-base-uncased\snapshots\86b5e0934494bd15c9632b12f734a8a67f723594\config.json
Model config BertConfig {
  "_name_or_path": "google-bert/bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at C:\Users\32618\.cache\huggingface\hub\models--google-bert--bert-base-uncased\snapshots\86b5e0934494bd15c9632b12f734a8a67f723594\vocab.txt
loading file tokenizer.json from cache at C:\Users\32618\.cache\huggingface\hub\models--google-bert--bert-base-uncased\snapshots\86b5e0934494bd15c9632b12f734a8a67f723594\tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--google-bert--bert-base-uncased\snapshots\86b5e0934494bd15c9632b12f734a8a67f723594\tokenizer_config.json
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--google-bert--bert-base-uncased\snapshots\86b5e0934494bd15c9632b12f734a8a67f723594\config.json
Model config BertConfig {
  "_name_or_path": "google-bert/bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33194/33194 [00:01<00:00, 20780.48 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33194/33194 [00:01<00:00, 18400.09 examples/s]
Map:   0%|          | 0/8299 [00:00<?, ? examples/s]Sample tokenized input from train: {'group': 'religion', 'text': 'Have you ever met someone named Mohammed? I need a new pair of pajamas.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 36953, 'input_ids': [101, 2031, 2017, 2412, 2777, 2619, 2315, 12619, 1029, 1045, 2342, 1037, 2047, 3940, 1997, 27621, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8299/8299 [00:00<00:00, 21845.25 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8299/8299 [00:00<00:00, 15861.97 examples/s]
Sample tokenized input from validation: {'group': 'religion', 'text': 'Have you ever met someone named Mohammed? I need a new pair of pajamas.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 36953, 'input_ids': [101, 2031, 2017, 2412, 2777, 2619, 2315, 12619, 1029, 1045, 2342, 1037, 2047, 3940, 1997, 27621, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
D:\UCL\HEARTS-Text-Stereotype-Detection-main\Model Training and Evaluation\BERT_Models_Fine_Tuning.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
[codecarbon ERROR @ 14:51:51] Error: Another instance of codecarbon is probably running as we find `C:\Users\32618\AppData\Local\Temp\.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.
The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running training *****
  Num examples = 33,194
  Num Epochs = 6
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 3,114
  Number of trainable parameters = 109,483,778
[codecarbon WARNING @ 14:51:51] Another instance of codecarbon is already running. Exiting.
  1%|          | 20/3114 [00:04<12:24,  4.15it/s][codecarbon INFO @ 14:51:56] Energy consumed for RAM : 0.000221 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:51:56] Energy consumed for all GPUs : 0.000309 kWh. Total GPU Power : 55.72678793662123 W
[codecarbon INFO @ 14:51:56] Energy consumed for all CPUs : 0.000532 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:51:56] 0.001062 kWh of electricity used since the beginning.
  3%|â–Ž         | 82/3114 [00:19<12:28,  4.05it/s][codecarbon INFO @ 14:52:11] Energy consumed for RAM : 0.000295 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:52:11] Energy consumed for all GPUs : 0.000960 kWh. Total GPU Power : 156.05039364802218 W
[codecarbon INFO @ 14:52:11] Energy consumed for all CPUs : 0.000709 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:52:11] 0.001963 kWh of electricity used since the beginning.
  5%|â–         | 144/3114 [00:34<12:31,  3.95it/s][codecarbon INFO @ 14:52:26] Energy consumed for RAM : 0.000368 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:52:26] Energy consumed for all GPUs : 0.001610 kWh. Total GPU Power : 156.18801334308523 W
[codecarbon INFO @ 14:52:26] Energy consumed for all CPUs : 0.000886 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:52:26] 0.002865 kWh of electricity used since the beginning.
  7%|â–‹         | 205/3114 [00:49<12:15,  3.95it/s][codecarbon INFO @ 14:52:41] Energy consumed for RAM : 0.000442 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:52:41] Energy consumed for all GPUs : 0.002270 kWh. Total GPU Power : 158.1981649600305 W
[codecarbon INFO @ 14:52:41] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:52:41] 0.003775 kWh of electricity used since the beginning.
  9%|â–Š         | 266/3114 [01:04<11:42,  4.05it/s][codecarbon INFO @ 14:52:56] Energy consumed for RAM : 0.000516 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:52:56] Energy consumed for all GPUs : 0.002931 kWh. Total GPU Power : 158.7285709644456 W
[codecarbon INFO @ 14:52:56] Energy consumed for all CPUs : 0.001240 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:52:56] 0.004687 kWh of electricity used since the beginning.
 11%|â–ˆ         | 328/3114 [01:19<11:25,  4.07it/s][codecarbon INFO @ 14:53:11] Energy consumed for RAM : 0.000589 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:53:11] Energy consumed for all GPUs : 0.003601 kWh. Total GPU Power : 160.66411015973125 W
[codecarbon INFO @ 14:53:11] Energy consumed for all CPUs : 0.001417 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:53:11] 0.005607 kWh of electricity used since the beginning.
[codecarbon INFO @ 14:53:11] 0.011097 g.CO2eq/s mean an estimation of 349.94228931673285 kg.CO2eq/year
 13%|â–ˆâ–Ž        | 390/3114 [01:34<11:02,  4.11it/s][codecarbon INFO @ 14:53:26] Energy consumed for RAM : 0.000663 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:53:26] Energy consumed for all GPUs : 0.004274 kWh. Total GPU Power : 161.4638301126932 W
[codecarbon INFO @ 14:53:26] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:53:26] 0.006531 kWh of electricity used since the beginning.
 15%|â–ˆâ–        | 452/3114 [01:49<10:42,  4.14it/s][codecarbon INFO @ 14:53:41] Energy consumed for RAM : 0.000736 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:53:41] Energy consumed for all GPUs : 0.004948 kWh. Total GPU Power : 161.72499926215946 W
[codecarbon INFO @ 14:53:41] Energy consumed for all CPUs : 0.001772 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:53:41] 0.007456 kWh of electricity used since the beginning.
 16%|â–ˆâ–Œ        | 500/3114 [02:01<10:40,  4.08it/s]{'loss': 0.478, 'grad_norm': 3.044327974319458, 'learning_rate': 1.678869621066153e-05, 'epoch': 0.96}
 17%|â–ˆâ–‹        | 514/3114 [02:04<10:41,  4.05it/s][codecarbon INFO @ 14:53:56] Energy consumed for RAM : 0.000810 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:53:56] Energy consumed for all GPUs : 0.005623 kWh. Total GPU Power : 161.8086274910516 W
[codecarbon INFO @ 14:53:56] Energy consumed for all CPUs : 0.001949 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:53:56] 0.008382 kWh of electricity used since the beginning.
 17%|â–ˆâ–‹        | 519/3114 [02:05<09:44,  4.44it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 8299
  Batch size = 64

  0%|          | 0/130 [00:00<?, ?it/s]
  2%|â–         | 3/130 [00:00<00:04, 27.24it/s]
  5%|â–         | 6/130 [00:00<00:05, 20.86it/s]
  7%|â–‹         | 9/130 [00:00<00:06, 19.43it/s]
  9%|â–‰         | 12/130 [00:00<00:06, 18.75it/s]
 11%|â–ˆ         | 14/130 [00:00<00:06, 18.38it/s]
 12%|â–ˆâ–        | 16/130 [00:00<00:06, 17.58it/s]
 14%|â–ˆâ–        | 18/130 [00:00<00:06, 16.73it/s]
 15%|â–ˆâ–Œ        | 20/130 [00:01<00:06, 16.06it/s]
 17%|â–ˆâ–‹        | 22/130 [00:01<00:06, 15.70it/s]
 18%|â–ˆâ–Š        | 24/130 [00:01<00:06, 15.27it/s]
 20%|â–ˆâ–ˆ        | 26/130 [00:01<00:06, 15.15it/s]
 22%|â–ˆâ–ˆâ–       | 28/130 [00:01<00:06, 14.92it/s]
 23%|â–ˆâ–ˆâ–Ž       | 30/130 [00:01<00:06, 14.92it/s]
 25%|â–ˆâ–ˆâ–       | 32/130 [00:01<00:06, 14.73it/s]
 26%|â–ˆâ–ˆâ–Œ       | 34/130 [00:02<00:06, 14.81it/s]
 28%|â–ˆâ–ˆâ–Š       | 36/130 [00:02<00:06, 14.68it/s]
 29%|â–ˆâ–ˆâ–‰       | 38/130 [00:02<00:06, 14.76it/s]
 31%|â–ˆâ–ˆâ–ˆ       | 40/130 [00:02<00:06, 14.65it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 42/130 [00:02<00:05, 14.73it/s]
 34%|â–ˆâ–ˆâ–ˆâ–      | 44/130 [00:02<00:05, 14.56it/s]
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 46/130 [00:02<00:05, 14.70it/s]
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 48/130 [00:03<00:05, 14.71it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 50/130 [00:03<00:05, 14.98it/s]
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 52/130 [00:03<00:05, 15.03it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 54/130 [00:03<00:04, 15.22it/s]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 56/130 [00:03<00:04, 15.22it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 58/130 [00:03<00:04, 15.36it/s]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 60/130 [00:03<00:04, 15.30it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 62/130 [00:03<00:04, 15.36it/s]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 64/130 [00:04<00:04, 15.58it/s]
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 66/130 [00:04<00:03, 16.05it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 68/130 [00:04<00:03, 16.34it/s]
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 70/130 [00:04<00:03, 16.65it/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 72/130 [00:04<00:03, 16.78it/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 74/130 [00:04<00:03, 17.02it/s]
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 76/130 [00:04<00:03, 17.04it/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 78/130 [00:04<00:03, 17.06it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 80/130 [00:04<00:02, 17.02it/s]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 82/130 [00:05<00:02, 17.11it/s]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 84/130 [00:05<00:02, 16.99it/s]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 86/130 [00:05<00:02, 17.07it/s]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 88/130 [00:05<00:02, 17.13it/s]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 90/130 [00:05<00:02, 17.15it/s]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 92/130 [00:05<00:02, 17.10it/s]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 94/130 [00:05<00:02, 16.92it/s]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 96/130 [00:05<00:02, 16.63it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 98/130 [00:06<00:01, 16.66it/s]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 100/130 [00:06<00:01, 16.48it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 102/130 [00:06<00:01, 16.54it/s]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 104/130 [00:06<00:01, 16.39it/s]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 106/130 [00:06<00:01, 16.46it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 108/130 [00:06<00:01, 16.35it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 110/130 [00:06<00:01, 15.85it/s]
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 112/130 [00:06<00:01, 14.86it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 114/130 [00:07<00:01, 14.38it/s]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 116/130 [00:07<00:01, 13.96it/s]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 118/130 [00:07<00:00, 13.77it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 120/130 [00:07<00:00, 13.44it/s]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 122/130 [00:07<00:00, 13.43it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 124/130 [00:07<00:00, 13.31it/s]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 126/130 [00:08<00:00, 13.81it/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 128/130 [00:08<00:00, 14.61it/s]
                                                  
 17%|â–ˆâ–‹        | 519/3114 [02:14<09:44,  4.44it/s]
                                                 Saving model checkpoint to model_output_bert\mgsd_trained\checkpoint-519
Configuration saved in model_output_bert\mgsd_trained\checkpoint-519\config.json
{'eval_loss': 0.4089339077472687, 'eval_precision': 0.7785364590476328, 'eval_recall': 0.7977297890562312, 'eval_f1': 0.7845172074011446, 'eval_balanced accuracy': 0.7977297890562312, 'eval_runtime': 8.3062, 'eval_samples_per_second': 999.133, 'eval_steps_per_second': 15.651, 'epoch': 1.0}
Model weights saved in model_output_bert\mgsd_trained\checkpoint-519\model.safetensors
tokenizer config file saved in model_output_bert\mgsd_trained\checkpoint-519\tokenizer_config.json
Special tokens file saved in model_output_bert\mgsd_trained\checkpoint-519\special_tokens_map.json
 17%|â–ˆâ–‹        | 537/3114 [02:19<10:35,  4.05it/s][codecarbon INFO @ 14:54:11] Energy consumed for RAM : 0.000884 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:54:11] Energy consumed for all GPUs : 0.006298 kWh. Total GPU Power : 161.9924740724525 W
[codecarbon INFO @ 14:54:11] Energy consumed for all CPUs : 0.002126 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:54:11] 0.009308 kWh of electricity used since the beginning.
 19%|â–ˆâ–‰        | 599/3114 [02:34<10:03,  4.16it/s][codecarbon INFO @ 14:54:26] Energy consumed for RAM : 0.000957 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:54:26] Energy consumed for all GPUs : 0.006973 kWh. Total GPU Power : 161.86081778987287 W
[codecarbon INFO @ 14:54:26] Energy consumed for all CPUs : 0.002303 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:54:26] 0.010234 kWh of electricity used since the beginning.
 21%|â–ˆâ–ˆ        | 660/3114 [02:49<09:49,  4.16it/s][codecarbon INFO @ 14:54:41] Energy consumed for RAM : 0.001031 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:54:41] Energy consumed for all GPUs : 0.007650 kWh. Total GPU Power : 162.4213726027724 W
[codecarbon INFO @ 14:54:41] Energy consumed for all CPUs : 0.002481 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:54:41] 0.011162 kWh of electricity used since the beginning.
 23%|â–ˆâ–ˆâ–Ž       | 722/3114 [03:04<09:39,  4.13it/s][codecarbon INFO @ 14:54:56] Energy consumed for RAM : 0.001105 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:54:56] Energy consumed for all GPUs : 0.008331 kWh. Total GPU Power : 163.22288226115157 W
[codecarbon INFO @ 14:54:56] Energy consumed for all CPUs : 0.002658 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:54:56] 0.012093 kWh of electricity used since the beginning.
 25%|â–ˆâ–ˆâ–Œ       | 785/3114 [03:19<09:19,  4.17it/s][codecarbon INFO @ 14:55:11] Energy consumed for RAM : 0.001178 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:55:11] Energy consumed for all GPUs : 0.009012 kWh. Total GPU Power : 163.49338377610218 W
[codecarbon INFO @ 14:55:11] Energy consumed for all CPUs : 0.002835 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:55:11] 0.013025 kWh of electricity used since the beginning.
[codecarbon INFO @ 14:55:11] 0.014678 g.CO2eq/s mean an estimation of 462.89570317016035 kg.CO2eq/year
 27%|â–ˆâ–ˆâ–‹       | 847/3114 [03:34<09:17,  4.06it/s][codecarbon INFO @ 14:55:26] Energy consumed for RAM : 0.001252 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:55:26] Energy consumed for all GPUs : 0.009696 kWh. Total GPU Power : 164.0868201849974 W
[codecarbon INFO @ 14:55:26] Energy consumed for all CPUs : 0.003012 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:55:26] 0.013960 kWh of electricity used since the beginning.
 29%|â–ˆâ–ˆâ–‰       | 909/3114 [03:49<09:03,  4.06it/s][codecarbon INFO @ 14:55:41] Energy consumed for RAM : 0.001326 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:55:41] Energy consumed for all GPUs : 0.010380 kWh. Total GPU Power : 164.02045095005585 W
[codecarbon INFO @ 14:55:41] Energy consumed for all CPUs : 0.003189 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:55:41] 0.014895 kWh of electricity used since the beginning.
 31%|â–ˆâ–ˆâ–ˆ       | 970/3114 [04:04<08:46,  4.07it/s][codecarbon INFO @ 14:55:56] Energy consumed for RAM : 0.001399 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:55:56] Energy consumed for all GPUs : 0.011060 kWh. Total GPU Power : 163.0359970469817 W
[codecarbon INFO @ 14:55:56] Energy consumed for all CPUs : 0.003366 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:55:56] 0.015826 kWh of electricity used since the beginning.
 32%|â–ˆâ–ˆâ–ˆâ–      | 1000/3114 [04:12<08:27,  4.16it/s]{'loss': 0.3338, 'grad_norm': 4.601315975189209, 'learning_rate': 1.357739242132306e-05, 'epoch': 1.93}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1031/3114 [04:19<08:35,  4.04it/s][codecarbon INFO @ 14:56:11] Energy consumed for RAM : 0.001473 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:56:11] Energy consumed for all GPUs : 0.011744 kWh. Total GPU Power : 164.10970304577418 W
[codecarbon INFO @ 14:56:11] Energy consumed for all CPUs : 0.003544 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:56:11] 0.016761 kWh of electricity used since the beginning.
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1038/3114 [04:21<07:20,  4.71it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 8299
  Batch size = 64

  0%|          | 0/130 [00:00<?, ?it/s]
  2%|â–         | 3/130 [00:00<00:04, 25.68it/s]
  5%|â–         | 6/130 [00:00<00:06, 20.33it/s]
  7%|â–‹         | 9/130 [00:00<00:06, 19.22it/s]
  8%|â–Š         | 11/130 [00:00<00:06, 18.67it/s]
 10%|â–ˆ         | 13/130 [00:00<00:06, 18.39it/s]
 12%|â–ˆâ–        | 15/130 [00:00<00:06, 18.23it/s]
 13%|â–ˆâ–Ž        | 17/130 [00:00<00:06, 17.10it/s]
 15%|â–ˆâ–        | 19/130 [00:01<00:06, 16.17it/s]
 16%|â–ˆâ–Œ        | 21/130 [00:01<00:06, 15.78it/s]
 18%|â–ˆâ–Š        | 23/130 [00:01<00:07, 15.26it/s]
 19%|â–ˆâ–‰        | 25/130 [00:01<00:06, 15.16it/s]
 21%|â–ˆâ–ˆ        | 27/130 [00:01<00:06, 14.86it/s]
 22%|â–ˆâ–ˆâ–       | 29/130 [00:01<00:06, 14.85it/s]
 24%|â–ˆâ–ˆâ–       | 31/130 [00:01<00:06, 14.63it/s]
 25%|â–ˆâ–ˆâ–Œ       | 33/130 [00:02<00:06, 14.69it/s]
 27%|â–ˆâ–ˆâ–‹       | 35/130 [00:02<00:06, 14.57it/s]
 28%|â–ˆâ–ˆâ–Š       | 37/130 [00:02<00:06, 14.69it/s]
 30%|â–ˆâ–ˆâ–ˆ       | 39/130 [00:02<00:06, 14.56it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 41/130 [00:02<00:06, 14.63it/s]
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 43/130 [00:02<00:05, 14.51it/s]
 35%|â–ˆâ–ˆâ–ˆâ–      | 45/130 [00:02<00:05, 14.63it/s]
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 47/130 [00:02<00:05, 14.48it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 49/130 [00:03<00:05, 14.74it/s]
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 51/130 [00:03<00:05, 14.80it/s]
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 53/130 [00:03<00:05, 15.02it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 55/130 [00:03<00:04, 15.03it/s]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 57/130 [00:03<00:04, 15.20it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 59/130 [00:03<00:04, 15.18it/s]
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 61/130 [00:03<00:04, 15.29it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 63/130 [00:04<00:04, 15.30it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 65/130 [00:04<00:04, 15.89it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 67/130 [00:04<00:03, 16.16it/s]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 69/130 [00:04<00:03, 16.52it/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 71/130 [00:04<00:03, 16.63it/s]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 73/130 [00:04<00:03, 16.82it/s]
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 75/130 [00:04<00:03, 16.69it/s]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 77/130 [00:04<00:03, 16.88it/s]
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 79/130 [00:04<00:03, 16.95it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 81/130 [00:05<00:02, 16.99it/s]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 83/130 [00:05<00:02, 16.96it/s]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 85/130 [00:05<00:02, 17.03it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 87/130 [00:05<00:02, 16.94it/s]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 89/130 [00:05<00:02, 17.09it/s]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 91/130 [00:05<00:02, 16.95it/s]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 93/130 [00:05<00:02, 17.02it/s]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 95/130 [00:05<00:02, 16.74it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 97/130 [00:06<00:01, 16.71it/s]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 99/130 [00:06<00:01, 16.46it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 101/130 [00:06<00:01, 16.28it/s]
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 103/130 [00:06<00:01, 16.21it/s]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 105/130 [00:06<00:01, 16.29it/s]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 107/130 [00:06<00:01, 16.23it/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 109/130 [00:06<00:01, 16.14it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 111/130 [00:06<00:01, 14.93it/s]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 113/130 [00:07<00:01, 14.40it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 115/130 [00:07<00:01, 13.95it/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 117/130 [00:07<00:00, 13.76it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 119/130 [00:07<00:00, 13.52it/s]
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 121/130 [00:07<00:00, 13.45it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 123/130 [00:07<00:00, 13.29it/s]
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 125/130 [00:08<00:00, 13.31it/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 127/130 [00:08<00:00, 14.14it/s]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 129/130 [00:08<00:00, 15.02it/s]
{'eval_loss': 0.3841915428638458, 'eval_precision': 0.8212420921322285, 'eval_recall': 0.7990322742423307, 'eval_f1': 0.8079474058130076, 'eval_balanced accuracy': 0.7990322742423307, 'eval_runtime': 8.3573, 'eval_samples_per_second': 993.019, 'eval_steps_per_second': 15.555, 'epoch': 2.0}
                                                   
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1038/3114 [04:29<07:20,  4.71it/s]
                                                 Saving model checkpoint to model_output_bert\mgsd_trained\checkpoint-1038
Configuration saved in model_output_bert\mgsd_trained\checkpoint-1038\config.json
Model weights saved in model_output_bert\mgsd_trained\checkpoint-1038\model.safetensors
tokenizer config file saved in model_output_bert\mgsd_trained\checkpoint-1038\tokenizer_config.json
Special tokens file saved in model_output_bert\mgsd_trained\checkpoint-1038\special_tokens_map.json
Deleting older checkpoint [model_output_bert\mgsd_trained\checkpoint-519] due to args.save_total_limit
 34%|â–ˆâ–ˆâ–ˆâ–      | 1054/3114 [04:34<08:45,  3.92it/s][codecarbon INFO @ 14:56:26] Energy consumed for RAM : 0.001547 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:56:26] Energy consumed for all GPUs : 0.012412 kWh. Total GPU Power : 160.28096299531487 W
[codecarbon INFO @ 14:56:26] Energy consumed for all CPUs : 0.003721 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:56:26] 0.017680 kWh of electricity used since the beginning.
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1115/3114 [04:49<07:37,  4.37it/s][codecarbon INFO @ 14:56:41] Energy consumed for RAM : 0.001620 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:56:41] Energy consumed for all GPUs : 0.013091 kWh. Total GPU Power : 162.7769923594103 W
[codecarbon INFO @ 14:56:41] Energy consumed for all CPUs : 0.003898 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:56:41] 0.018610 kWh of electricity used since the beginning.
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1177/3114 [05:04<07:57,  4.06it/s][codecarbon INFO @ 14:56:56] Energy consumed for RAM : 0.001694 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:56:56] Energy consumed for all GPUs : 0.013772 kWh. Total GPU Power : 163.30129722266364 W
[codecarbon INFO @ 14:56:56] Energy consumed for all CPUs : 0.004075 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:56:56] 0.019541 kWh of electricity used since the beginning.
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1239/3114 [05:19<07:28,  4.18it/s][codecarbon INFO @ 14:57:11] Energy consumed for RAM : 0.001768 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:57:11] Energy consumed for all GPUs : 0.014456 kWh. Total GPU Power : 164.18497187918456 W
[codecarbon INFO @ 14:57:11] Energy consumed for all CPUs : 0.004252 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:57:11] 0.020476 kWh of electricity used since the beginning.
[codecarbon INFO @ 14:57:11] 0.014743 g.CO2eq/s mean an estimation of 464.92489470085394 kg.CO2eq/year
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1301/3114 [05:34<07:28,  4.04it/s][codecarbon INFO @ 14:57:26] Energy consumed for RAM : 0.001841 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:57:26] Energy consumed for all GPUs : 0.015138 kWh. Total GPU Power : 163.59329628771286 W
[codecarbon INFO @ 14:57:26] Energy consumed for all CPUs : 0.004430 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:57:26] 0.021409 kWh of electricity used since the beginning.
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1362/3114 [05:49<06:55,  4.21it/s][codecarbon INFO @ 14:57:41] Energy consumed for RAM : 0.001915 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:57:41] Energy consumed for all GPUs : 0.015824 kWh. Total GPU Power : 164.59588628139755 W
[codecarbon INFO @ 14:57:41] Energy consumed for all CPUs : 0.004607 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:57:41] 0.022346 kWh of electricity used since the beginning.
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1424/3114 [06:04<06:40,  4.22it/s][codecarbon INFO @ 14:57:56] Energy consumed for RAM : 0.001989 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:57:56] Energy consumed for all GPUs : 0.016509 kWh. Total GPU Power : 164.25448505373632 W
[codecarbon INFO @ 14:57:56] Energy consumed for all CPUs : 0.004784 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:57:56] 0.023281 kWh of electricity used since the beginning.
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1486/3114 [06:19<06:29,  4.18it/s][codecarbon INFO @ 14:58:11] Energy consumed for RAM : 0.002062 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:58:11] Energy consumed for all GPUs : 0.017200 kWh. Total GPU Power : 165.8557469384465 W
[codecarbon INFO @ 14:58:11] Energy consumed for all CPUs : 0.004961 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:58:11] 0.024224 kWh of electricity used since the beginning.
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1500/3114 [06:23<06:40,  4.03it/s]{'loss': 0.2406, 'grad_norm': 6.541388511657715, 'learning_rate': 1.0366088631984585e-05, 'epoch': 2.89}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1548/3114 [06:34<06:25,  4.07it/s][codecarbon INFO @ 14:58:26] Energy consumed for RAM : 0.002136 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:58:26] Energy consumed for all GPUs : 0.017887 kWh. Total GPU Power : 164.83795180259503 W
[codecarbon INFO @ 14:58:26] Energy consumed for all CPUs : 0.005138 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:58:26] 0.025161 kWh of electricity used since the beginning.
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1557/3114 [06:37<05:35,  4.65it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 8299
  Batch size = 64

  0%|          | 0/130 [00:00<?, ?it/s]
  2%|â–         | 3/130 [00:00<00:04, 25.46it/s]
  5%|â–         | 6/130 [00:00<00:06, 20.27it/s]
  7%|â–‹         | 9/130 [00:00<00:06, 19.14it/s]
  8%|â–Š         | 11/130 [00:00<00:06, 18.61it/s]
 10%|â–ˆ         | 13/130 [00:00<00:06, 18.45it/s]
 12%|â–ˆâ–        | 15/130 [00:00<00:06, 18.16it/s]
 13%|â–ˆâ–Ž        | 17/130 [00:00<00:06, 16.93it/s]
 15%|â–ˆâ–        | 19/130 [00:01<00:06, 16.07it/s]
 16%|â–ˆâ–Œ        | 21/130 [00:01<00:07, 15.24it/s]
 18%|â–ˆâ–Š        | 23/130 [00:01<00:07, 14.79it/s]
 19%|â–ˆâ–‰        | 25/130 [00:01<00:07, 14.75it/s]
 21%|â–ˆâ–ˆ        | 27/130 [00:01<00:07, 14.59it/s]
 22%|â–ˆâ–ˆâ–       | 29/130 [00:01<00:06, 14.66it/s]
 24%|â–ˆâ–ˆâ–       | 31/130 [00:01<00:06, 14.56it/s]
 25%|â–ˆâ–ˆâ–Œ       | 33/130 [00:02<00:06, 14.57it/s]
 27%|â–ˆâ–ˆâ–‹       | 35/130 [00:02<00:06, 14.42it/s]
 28%|â–ˆâ–ˆâ–Š       | 37/130 [00:02<00:06, 14.51it/s]
 30%|â–ˆâ–ˆâ–ˆ       | 39/130 [00:02<00:06, 14.38it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 41/130 [00:02<00:06, 14.54it/s]
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 43/130 [00:02<00:05, 14.52it/s]
 35%|â–ˆâ–ˆâ–ˆâ–      | 45/130 [00:02<00:05, 14.58it/s]
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 47/130 [00:03<00:05, 14.48it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 49/130 [00:03<00:05, 14.78it/s]
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 51/130 [00:03<00:05, 14.86it/s]
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 53/130 [00:03<00:05, 15.07it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 55/130 [00:03<00:04, 15.08it/s]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 57/130 [00:03<00:04, 15.23it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 59/130 [00:03<00:04, 15.17it/s]
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 61/130 [00:03<00:04, 15.27it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 63/130 [00:04<00:04, 15.24it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 65/130 [00:04<00:04, 15.81it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 67/130 [00:04<00:03, 16.12it/s]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 69/130 [00:04<00:03, 16.45it/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 71/130 [00:04<00:03, 16.62it/s]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 73/130 [00:04<00:03, 16.82it/s]
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 75/130 [00:04<00:03, 16.84it/s]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 77/130 [00:04<00:03, 16.96it/s]
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 79/130 [00:04<00:03, 16.97it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 81/130 [00:05<00:02, 17.03it/s]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 83/130 [00:05<00:02, 16.82it/s]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 85/130 [00:05<00:02, 16.75it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 87/130 [00:05<00:02, 16.64it/s]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 89/130 [00:05<00:02, 16.80it/s]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 91/130 [00:05<00:02, 16.82it/s]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 93/130 [00:05<00:02, 16.90it/s]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 95/130 [00:05<00:02, 16.49it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 97/130 [00:06<00:02, 16.39it/s]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 99/130 [00:06<00:01, 16.18it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 101/130 [00:06<00:01, 16.18it/s]
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 103/130 [00:06<00:01, 16.01it/s]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 105/130 [00:06<00:01, 16.03it/s]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 107/130 [00:06<00:01, 15.86it/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 109/130 [00:06<00:01, 15.85it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 111/130 [00:07<00:01, 14.45it/s]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 113/130 [00:07<00:01, 13.86it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 115/130 [00:07<00:01, 13.29it/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 117/130 [00:07<00:00, 13.11it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 119/130 [00:07<00:00, 12.99it/s]
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 121/130 [00:07<00:00, 13.05it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 123/130 [00:07<00:00, 12.86it/s]
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 125/130 [00:08<00:00, 12.87it/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 127/130 [00:08<00:00, 13.71it/s]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 129/130 [00:08<00:00, 14.52it/s]
                                                   
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1557/3114 [06:45<05:35,  4.65it/s]
                                                 Saving model checkpoint to model_output_bert\mgsd_trained\checkpoint-1557
Configuration saved in model_output_bert\mgsd_trained\checkpoint-1557\config.json
{'eval_loss': 0.4016133248806, 'eval_precision': 0.8243736547070939, 'eval_recall': 0.8180246075345143, 'eval_f1': 0.8209975807327881, 'eval_balanced accuracy': 0.8180246075345143, 'eval_runtime': 8.4744, 'eval_samples_per_second': 979.304, 'eval_steps_per_second': 15.34, 'epoch': 3.0}
Model weights saved in model_output_bert\mgsd_trained\checkpoint-1557\model.safetensors
tokenizer config file saved in model_output_bert\mgsd_trained\checkpoint-1557\tokenizer_config.json
Special tokens file saved in model_output_bert\mgsd_trained\checkpoint-1557\special_tokens_map.json
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1570/3114 [06:49<07:28,  3.45it/s][codecarbon INFO @ 14:58:41] Energy consumed for RAM : 0.002210 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:58:41] Energy consumed for all GPUs : 0.018554 kWh. Total GPU Power : 159.93640656007526 W
[codecarbon INFO @ 14:58:41] Energy consumed for all CPUs : 0.005315 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:58:41] 0.026079 kWh of electricity used since the beginning.
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1630/3114 [07:04<05:59,  4.13it/s][codecarbon INFO @ 14:58:56] Energy consumed for RAM : 0.002283 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:58:56] Energy consumed for all GPUs : 0.019219 kWh. Total GPU Power : 159.64812479885188 W
[codecarbon INFO @ 14:58:56] Energy consumed for all CPUs : 0.005493 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:58:56] 0.026995 kWh of electricity used since the beginning.
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1691/3114 [07:20<05:51,  4.04it/s][codecarbon INFO @ 14:59:11] Energy consumed for RAM : 0.002357 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:59:11] Energy consumed for all GPUs : 0.019898 kWh. Total GPU Power : 162.73707067953563 W
[codecarbon INFO @ 14:59:11] Energy consumed for all CPUs : 0.005670 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:59:11] 0.027924 kWh of electricity used since the beginning.
[codecarbon INFO @ 14:59:11] 0.014740 g.CO2eq/s mean an estimation of 464.83548340501045 kg.CO2eq/year
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1751/3114 [07:34<05:38,  4.03it/s][codecarbon INFO @ 14:59:26] Energy consumed for RAM : 0.002431 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:59:26] Energy consumed for all GPUs : 0.020574 kWh. Total GPU Power : 162.2267796989028 W
[codecarbon INFO @ 14:59:26] Energy consumed for all CPUs : 0.005847 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:59:26] 0.028851 kWh of electricity used since the beginning.
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1812/3114 [07:50<05:27,  3.98it/s][codecarbon INFO @ 14:59:41] Energy consumed for RAM : 0.002504 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:59:41] Energy consumed for all GPUs : 0.021253 kWh. Total GPU Power : 162.95756970832798 W
[codecarbon INFO @ 14:59:41] Energy consumed for all CPUs : 0.006024 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:59:41] 0.029781 kWh of electricity used since the beginning.
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1872/3114 [08:04<05:07,  4.04it/s][codecarbon INFO @ 14:59:56] Energy consumed for RAM : 0.002578 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 14:59:56] Energy consumed for all GPUs : 0.021932 kWh. Total GPU Power : 162.91022073619212 W
[codecarbon INFO @ 14:59:56] Energy consumed for all CPUs : 0.006201 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 14:59:56] 0.030711 kWh of electricity used since the beginning.
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1933/3114 [08:20<04:46,  4.12it/s][codecarbon INFO @ 15:00:11] Energy consumed for RAM : 0.002651 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:00:11] Energy consumed for all GPUs : 0.022611 kWh. Total GPU Power : 162.82639864508883 W
[codecarbon INFO @ 15:00:11] Energy consumed for all CPUs : 0.006378 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:00:11] 0.031641 kWh of electricity used since the beginning.
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1993/3114 [08:34<04:43,  3.96it/s][codecarbon INFO @ 15:00:26] Energy consumed for RAM : 0.002725 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:00:26] Energy consumed for all GPUs : 0.023285 kWh. Total GPU Power : 161.76774515445268 W
[codecarbon INFO @ 15:00:26] Energy consumed for all CPUs : 0.006555 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:00:26] 0.032566 kWh of electricity used since the beginning.
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2000/3114 [08:36<04:36,  4.03it/s]{'loss': 0.1702, 'grad_norm': 4.5743632316589355, 'learning_rate': 7.154784842646115e-06, 'epoch': 3.85}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2053/3114 [08:49<04:28,  3.95it/s][codecarbon INFO @ 15:00:41] Energy consumed for RAM : 0.002799 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:00:41] Energy consumed for all GPUs : 0.023963 kWh. Total GPU Power : 162.6132812886608 W
[codecarbon INFO @ 15:00:41] Energy consumed for all CPUs : 0.006733 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:00:41] 0.033495 kWh of electricity used since the beginning.
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2076/3114 [08:55<03:55,  4.40it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 8299
  Batch size = 64

  0%|          | 0/130 [00:00<?, ?it/s]
  2%|â–         | 3/130 [00:00<00:04, 26.18it/s]
  5%|â–         | 6/130 [00:00<00:06, 20.08it/s]
  7%|â–‹         | 9/130 [00:00<00:06, 18.75it/s]
  8%|â–Š         | 11/130 [00:00<00:06, 18.11it/s]
 10%|â–ˆ         | 13/130 [00:00<00:06, 17.72it/s]
 12%|â–ˆâ–        | 15/130 [00:00<00:06, 17.25it/s]
 13%|â–ˆâ–Ž        | 17/130 [00:00<00:07, 16.06it/s]
 15%|â–ˆâ–        | 19/130 [00:01<00:07, 15.36it/s]
 16%|â–ˆâ–Œ        | 21/130 [00:01<00:07, 14.71it/s]
 18%|â–ˆâ–Š        | 23/130 [00:01<00:07, 14.42it/s]
 19%|â–ˆâ–‰        | 25/130 [00:01<00:07, 14.44it/s]
 21%|â–ˆâ–ˆ        | 27/130 [00:01<00:07, 14.29it/s]
 22%|â–ˆâ–ˆâ–       | 29/130 [00:01<00:07, 14.39it/s]
 24%|â–ˆâ–ˆâ–       | 31/130 [00:01<00:06, 14.21it/s]
 25%|â–ˆâ–ˆâ–Œ       | 33/130 [00:02<00:06, 14.17it/s]
 27%|â–ˆâ–ˆâ–‹       | 35/130 [00:02<00:06, 14.16it/s]
 28%|â–ˆâ–ˆâ–Š       | 37/130 [00:02<00:06, 14.22it/s]
 30%|â–ˆâ–ˆâ–ˆ       | 39/130 [00:02<00:06, 14.21it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 41/130 [00:02<00:06, 14.27it/s]
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 43/130 [00:02<00:06, 14.21it/s]
 35%|â–ˆâ–ˆâ–ˆâ–      | 45/130 [00:02<00:05, 14.24it/s]
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 47/130 [00:03<00:05, 14.13it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 49/130 [00:03<00:05, 14.47it/s]
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 51/130 [00:03<00:05, 14.49it/s]
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 53/130 [00:03<00:05, 14.45it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 55/130 [00:03<00:05, 14.52it/s]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 57/130 [00:03<00:04, 14.69it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 59/130 [00:03<00:04, 14.63it/s]
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 61/130 [00:04<00:04, 14.68it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 63/130 [00:04<00:04, 14.61it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 65/130 [00:04<00:04, 15.29it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 67/130 [00:04<00:04, 15.59it/s]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 69/130 [00:04<00:03, 15.99it/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 71/130 [00:04<00:03, 16.15it/s]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 73/130 [00:04<00:03, 16.40it/s]
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 75/130 [00:04<00:03, 16.45it/s]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 77/130 [00:05<00:03, 16.57it/s]
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 79/130 [00:05<00:03, 16.49it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 81/130 [00:05<00:02, 16.53it/s]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 83/130 [00:05<00:02, 16.34it/s]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 85/130 [00:05<00:02, 16.45it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 87/130 [00:05<00:02, 16.34it/s]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 89/130 [00:05<00:02, 16.44it/s]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 91/130 [00:05<00:02, 16.39it/s]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 93/130 [00:05<00:02, 16.36it/s]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 95/130 [00:06<00:02, 16.04it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 97/130 [00:06<00:02, 15.78it/s]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 99/130 [00:06<00:01, 15.65it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 101/130 [00:06<00:01, 15.75it/s]
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 103/130 [00:06<00:01, 15.72it/s]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 105/130 [00:06<00:01, 15.75it/s]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 107/130 [00:06<00:01, 15.66it/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 109/130 [00:07<00:01, 15.66it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 111/130 [00:07<00:01, 14.47it/s]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 113/130 [00:07<00:01, 13.77it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 115/130 [00:07<00:01, 13.35it/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 117/130 [00:07<00:01, 13.00it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 119/130 [00:07<00:00, 12.80it/s]
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 121/130 [00:07<00:00, 12.78it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 123/130 [00:08<00:00, 12.65it/s]
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 125/130 [00:08<00:00, 12.75it/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 127/130 [00:08<00:00, 13.49it/s]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 129/130 [00:08<00:00, 14.24it/s]
{'eval_loss': 0.4493636190891266, 'eval_precision': 0.8216165440302452, 'eval_recall': 0.8223596987325885, 'eval_f1': 0.8219851058809536, 'eval_balanced accuracy': 0.8223596987325885, 'eval_runtime': 8.6862, 'eval_samples_per_second': 955.424, 'eval_steps_per_second': 14.966, 'epoch': 4.0}
                                                   
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2076/3114 [09:04<03:55,  4.40it/s]
                                                 Saving model checkpoint to model_output_bert\mgsd_trained\checkpoint-2076
Configuration saved in model_output_bert\mgsd_trained\checkpoint-2076\config.json
Model weights saved in model_output_bert\mgsd_trained\checkpoint-2076\model.safetensors
tokenizer config file saved in model_output_bert\mgsd_trained\checkpoint-2076\tokenizer_config.json
Special tokens file saved in model_output_bert\mgsd_trained\checkpoint-2076\special_tokens_map.json
[codecarbon INFO @ 15:00:56] Energy consumed for RAM : 0.002872 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:00:56] Energy consumed for all GPUs : 0.024640 kWh. Total GPU Power : 162.37337421513087 W
[codecarbon INFO @ 15:00:56] Energy consumed for all CPUs : 0.006910 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:00:56] 0.034422 kWh of electricity used since the beginning.
Deleting older checkpoint [model_output_bert\mgsd_trained\checkpoint-1557] due to args.save_total_limit
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2135/3114 [09:20<03:57,  4.12it/s][codecarbon INFO @ 15:01:11] Energy consumed for RAM : 0.002946 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:01:11] Energy consumed for all GPUs : 0.025302 kWh. Total GPU Power : 158.90040735321818 W
[codecarbon INFO @ 15:01:11] Energy consumed for all CPUs : 0.007087 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:01:11] 0.035335 kWh of electricity used since the beginning.
[codecarbon INFO @ 15:01:11] 0.014667 g.CO2eq/s mean an estimation of 462.5264070220986 kg.CO2eq/year
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2195/3114 [09:34<03:42,  4.13it/s][codecarbon INFO @ 15:01:26] Energy consumed for RAM : 0.003020 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:01:26] Energy consumed for all GPUs : 0.025986 kWh. Total GPU Power : 164.12043847363148 W
[codecarbon INFO @ 15:01:26] Energy consumed for all CPUs : 0.007264 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:01:26] 0.036270 kWh of electricity used since the beginning.
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2256/3114 [09:50<03:40,  3.90it/s][codecarbon INFO @ 15:01:41] Energy consumed for RAM : 0.003093 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:01:41] Energy consumed for all GPUs : 0.026664 kWh. Total GPU Power : 162.5106081471702 W
[codecarbon INFO @ 15:01:41] Energy consumed for all CPUs : 0.007441 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:01:41] 0.037199 kWh of electricity used since the beginning.
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2311/3114 [10:05<04:23,  3.05it/s][codecarbon INFO @ 15:01:56] Energy consumed for RAM : 0.003167 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:01:56] Energy consumed for all GPUs : 0.027295 kWh. Total GPU Power : 151.2865215516044 W
[codecarbon INFO @ 15:01:56] Energy consumed for all CPUs : 0.007619 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:01:56] 0.038081 kWh of electricity used since the beginning.
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2369/3114 [10:20<03:02,  4.07it/s][codecarbon INFO @ 15:02:11] Energy consumed for RAM : 0.003241 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:02:11] Energy consumed for all GPUs : 0.027954 kWh. Total GPU Power : 157.97319335263325 W
[codecarbon INFO @ 15:02:11] Energy consumed for all CPUs : 0.007796 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:02:11] 0.038990 kWh of electricity used since the beginning.
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2431/3114 [10:35<02:44,  4.16it/s][codecarbon INFO @ 15:02:26] Energy consumed for RAM : 0.003314 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:02:26] Energy consumed for all GPUs : 0.028640 kWh. Total GPU Power : 164.65146587304756 W
[codecarbon INFO @ 15:02:26] Energy consumed for all CPUs : 0.007973 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:02:26] 0.039928 kWh of electricity used since the beginning.
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2492/3114 [10:50<02:35,  4.00it/s][codecarbon INFO @ 15:02:41] Energy consumed for RAM : 0.003388 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:02:41] Energy consumed for all GPUs : 0.029321 kWh. Total GPU Power : 163.3002077284003 W
[codecarbon INFO @ 15:02:41] Energy consumed for all CPUs : 0.008150 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:02:41] 0.040859 kWh of electricity used since the beginning.
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2500/3114 [10:52<02:33,  4.00it/s]{'loss': 0.127, 'grad_norm': 4.2262959480285645, 'learning_rate': 3.9434810533076434e-06, 'epoch': 4.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2553/3114 [11:05<02:20,  4.00it/s][codecarbon INFO @ 15:02:56] Energy consumed for RAM : 0.003462 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:02:56] Energy consumed for all GPUs : 0.030003 kWh. Total GPU Power : 163.58160201118974 W
[codecarbon INFO @ 15:02:56] Energy consumed for all CPUs : 0.008327 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:02:56] 0.041792 kWh of electricity used since the beginning.
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2595/3114 [11:15<01:59,  4.36it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 8299
  Batch size = 64

  0%|          | 0/130 [00:00<?, ?it/s]
  2%|â–         | 3/130 [00:00<00:04, 26.34it/s]
  5%|â–         | 6/130 [00:00<00:06, 20.26it/s]
  7%|â–‹         | 9/130 [00:00<00:06, 18.98it/s]
  8%|â–Š         | 11/130 [00:00<00:06, 18.42it/s]
 10%|â–ˆ         | 13/130 [00:00<00:06, 18.14it/s]
 12%|â–ˆâ–        | 15/130 [00:00<00:06, 17.89it/s]
 13%|â–ˆâ–Ž        | 17/130 [00:00<00:06, 16.56it/s]
 15%|â–ˆâ–        | 19/130 [00:01<00:07, 15.73it/s]
 16%|â–ˆâ–Œ        | 21/130 [00:01<00:07, 15.34it/s]
 18%|â–ˆâ–Š        | 23/130 [00:01<00:07, 14.84it/s]
 19%|â–ˆâ–‰        | 25/130 [00:01<00:07, 14.73it/s]
 21%|â–ˆâ–ˆ        | 27/130 [00:01<00:07, 14.57it/s]
 22%|â–ˆâ–ˆâ–       | 29/130 [00:01<00:06, 14.54it/s]
 24%|â–ˆâ–ˆâ–       | 31/130 [00:01<00:06, 14.37it/s]
 25%|â–ˆâ–ˆâ–Œ       | 33/130 [00:02<00:06, 14.35it/s]
 27%|â–ˆâ–ˆâ–‹       | 35/130 [00:02<00:06, 14.26it/s]
 28%|â–ˆâ–ˆâ–Š       | 37/130 [00:02<00:06, 14.27it/s]
 30%|â–ˆâ–ˆâ–ˆ       | 39/130 [00:02<00:06, 14.22it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 41/130 [00:02<00:06, 14.30it/s]
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 43/130 [00:02<00:06, 14.20it/s]
 35%|â–ˆâ–ˆâ–ˆâ–      | 45/130 [00:02<00:05, 14.17it/s]
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 47/130 [00:03<00:05, 14.15it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 49/130 [00:03<00:05, 14.46it/s]
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 51/130 [00:03<00:05, 14.50it/s]
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 53/130 [00:03<00:05, 14.73it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 55/130 [00:03<00:05, 14.72it/s]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 57/130 [00:03<00:04, 14.60it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 59/130 [00:03<00:04, 14.61it/s]
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 61/130 [00:03<00:04, 14.84it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 63/130 [00:04<00:04, 14.86it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 65/130 [00:04<00:04, 15.40it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 67/130 [00:04<00:04, 15.68it/s]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 69/130 [00:04<00:03, 16.01it/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 71/130 [00:04<00:03, 16.24it/s]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 73/130 [00:04<00:03, 16.33it/s][codecarbon INFO @ 15:03:12] Energy consumed for RAM : 0.003535 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:03:12] Energy consumed for all GPUs : 0.030695 kWh. Total GPU Power : 166.07743629886988 W
[codecarbon INFO @ 15:03:12] Energy consumed for all CPUs : 0.008505 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:03:12] 0.042735 kWh of electricity used since the beginning.
[codecarbon INFO @ 15:03:12] 0.014641 g.CO2eq/s mean an estimation of 461.712880825824 kg.CO2eq/year

 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 75/130 [00:04<00:03, 16.14it/s]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 77/130 [00:04<00:03, 16.18it/s]
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 79/130 [00:05<00:03, 16.24it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 81/130 [00:05<00:02, 16.37it/s]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 83/130 [00:05<00:02, 16.26it/s]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 85/130 [00:05<00:02, 16.42it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 87/130 [00:05<00:02, 16.37it/s]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 89/130 [00:05<00:02, 16.48it/s]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 91/130 [00:05<00:02, 16.32it/s]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 93/130 [00:05<00:02, 16.32it/s]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 95/130 [00:06<00:02, 16.08it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 97/130 [00:06<00:02, 16.06it/s]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 99/130 [00:06<00:01, 15.87it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 101/130 [00:06<00:01, 15.91it/s]
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 103/130 [00:06<00:01, 15.77it/s]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 105/130 [00:06<00:01, 15.87it/s]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 107/130 [00:06<00:01, 15.76it/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 109/130 [00:06<00:01, 15.77it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 111/130 [00:07<00:01, 14.58it/s]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 113/130 [00:07<00:01, 13.95it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 115/130 [00:07<00:01, 13.56it/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 117/130 [00:07<00:00, 13.25it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 119/130 [00:07<00:00, 13.01it/s]
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 121/130 [00:07<00:00, 12.87it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 123/130 [00:08<00:00, 12.72it/s]
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 125/130 [00:08<00:00, 12.79it/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 127/130 [00:08<00:00, 13.70it/s]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 129/130 [00:08<00:00, 14.60it/s]
                                                   
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2595/3114 [11:23<01:59,  4.36it/s]
                                                 Saving model checkpoint to model_output_bert\mgsd_trained\checkpoint-2595
Configuration saved in model_output_bert\mgsd_trained\checkpoint-2595\config.json
{'eval_loss': 0.5258792638778687, 'eval_precision': 0.817634655361115, 'eval_recall': 0.8281551228833611, 'eval_f1': 0.8221913503936948, 'eval_balanced accuracy': 0.8281551228833611, 'eval_runtime': 8.6123, 'eval_samples_per_second': 963.626, 'eval_steps_per_second': 15.095, 'epoch': 5.0}
Model weights saved in model_output_bert\mgsd_trained\checkpoint-2595\model.safetensors
tokenizer config file saved in model_output_bert\mgsd_trained\checkpoint-2595\tokenizer_config.json
Special tokens file saved in model_output_bert\mgsd_trained\checkpoint-2595\special_tokens_map.json
Deleting older checkpoint [model_output_bert\mgsd_trained\checkpoint-2076] due to args.save_total_limit
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2635/3114 [11:35<01:57,  4.07it/s][codecarbon INFO @ 15:03:27] Energy consumed for RAM : 0.003609 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:03:27] Energy consumed for all GPUs : 0.031339 kWh. Total GPU Power : 154.49003396556105 W
[codecarbon INFO @ 15:03:27] Energy consumed for all CPUs : 0.008682 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:03:27] 0.043629 kWh of electricity used since the beginning.
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2694/3114 [11:50<01:48,  3.88it/s][codecarbon INFO @ 15:03:42] Energy consumed for RAM : 0.003683 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:03:42] Energy consumed for all GPUs : 0.032009 kWh. Total GPU Power : 160.8322438110743 W
[codecarbon INFO @ 15:03:42] Energy consumed for all CPUs : 0.008859 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:03:42] 0.044551 kWh of electricity used since the beginning.
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2755/3114 [12:05<01:28,  4.06it/s][codecarbon INFO @ 15:03:57] Energy consumed for RAM : 0.003756 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:03:57] Energy consumed for all GPUs : 0.032692 kWh. Total GPU Power : 163.71333722389457 W
[codecarbon INFO @ 15:03:57] Energy consumed for all CPUs : 0.009036 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:03:57] 0.045484 kWh of electricity used since the beginning.
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2815/3114 [12:20<01:15,  3.97it/s][codecarbon INFO @ 15:04:12] Energy consumed for RAM : 0.003830 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:04:12] Energy consumed for all GPUs : 0.033371 kWh. Total GPU Power : 162.99952341258432 W
[codecarbon INFO @ 15:04:12] Energy consumed for all CPUs : 0.009213 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:04:12] 0.046415 kWh of electricity used since the beginning.
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2877/3114 [12:35<01:00,  3.94it/s][codecarbon INFO @ 15:04:27] Energy consumed for RAM : 0.003904 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:04:27] Energy consumed for all GPUs : 0.034052 kWh. Total GPU Power : 163.35795227577847 W
[codecarbon INFO @ 15:04:27] Energy consumed for all CPUs : 0.009391 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:04:27] 0.047347 kWh of electricity used since the beginning.
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2937/3114 [12:50<00:44,  3.97it/s][codecarbon INFO @ 15:04:42] Energy consumed for RAM : 0.003977 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:04:42] Energy consumed for all GPUs : 0.034731 kWh. Total GPU Power : 162.89367045781077 W
[codecarbon INFO @ 15:04:42] Energy consumed for all CPUs : 0.009568 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:04:42] 0.048276 kWh of electricity used since the beginning.
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2998/3114 [13:05<00:29,  3.97it/s][codecarbon INFO @ 15:04:57] Energy consumed for RAM : 0.004051 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:04:57] Energy consumed for all GPUs : 0.035409 kWh. Total GPU Power : 162.53440284934842 W
[codecarbon INFO @ 15:04:57] Energy consumed for all CPUs : 0.009745 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:04:57] 0.049205 kWh of electricity used since the beginning.
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3000/3114 [13:05<00:29,  3.92it/s]{'loss': 0.0963, 'grad_norm': 5.459228038787842, 'learning_rate': 7.321772639691716e-07, 'epoch': 5.78}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3058/3114 [13:20<00:14,  3.93it/s][codecarbon INFO @ 15:05:12] Energy consumed for RAM : 0.004125 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:05:12] Energy consumed for all GPUs : 0.036085 kWh. Total GPU Power : 161.9973708222103 W
[codecarbon INFO @ 15:05:12] Energy consumed for all CPUs : 0.009922 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:05:12] 0.050132 kWh of electricity used since the beginning.
[codecarbon INFO @ 15:05:12] 0.014635 g.CO2eq/s mean an estimation of 461.5330704784508 kg.CO2eq/year
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3114/3114 [13:34<00:00,  4.41it/s]Saving model checkpoint to model_output_bert\mgsd_trained\checkpoint-3114
Configuration saved in model_output_bert\mgsd_trained\checkpoint-3114\config.json
Model weights saved in model_output_bert\mgsd_trained\checkpoint-3114\model.safetensors
tokenizer config file saved in model_output_bert\mgsd_trained\checkpoint-3114\tokenizer_config.json
Special tokens file saved in model_output_bert\mgsd_trained\checkpoint-3114\special_tokens_map.json
[codecarbon INFO @ 15:05:27] Energy consumed for RAM : 0.004198 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:05:27] Energy consumed for all GPUs : 0.036727 kWh. Total GPU Power : 154.01518904511104 W
[codecarbon INFO @ 15:05:27] Energy consumed for all CPUs : 0.010099 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:05:27] 0.051025 kWh of electricity used since the beginning.
Deleting older checkpoint [model_output_bert\mgsd_trained\checkpoint-2595] due to args.save_total_limit
The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 8299
  Batch size = 64

  0%|          | 0/130 [00:00<?, ?it/s]
  2%|â–         | 3/130 [00:00<00:05, 24.57it/s]
  5%|â–         | 6/130 [00:00<00:06, 19.02it/s]
  6%|â–Œ         | 8/130 [00:00<00:06, 18.04it/s]
  8%|â–Š         | 10/130 [00:00<00:06, 17.97it/s]
  9%|â–‰         | 12/130 [00:00<00:06, 17.74it/s]
 11%|â–ˆ         | 14/130 [00:00<00:06, 17.79it/s]
 12%|â–ˆâ–        | 16/130 [00:00<00:06, 17.02it/s]
 14%|â–ˆâ–        | 18/130 [00:01<00:06, 16.19it/s]
 15%|â–ˆâ–Œ        | 20/130 [00:01<00:07, 15.53it/s]
 17%|â–ˆâ–‹        | 22/130 [00:01<00:07, 15.32it/s]
 18%|â–ˆâ–Š        | 24/130 [00:01<00:07, 14.90it/s]
 20%|â–ˆâ–ˆ        | 26/130 [00:01<00:07, 14.85it/s]
 22%|â–ˆâ–ˆâ–       | 28/130 [00:01<00:07, 14.51it/s]
 23%|â–ˆâ–ˆâ–Ž       | 30/130 [00:01<00:06, 14.54it/s]
 25%|â–ˆâ–ˆâ–       | 32/130 [00:02<00:06, 14.40it/s]
 26%|â–ˆâ–ˆâ–Œ       | 34/130 [00:02<00:06, 14.49it/s]
 28%|â–ˆâ–ˆâ–Š       | 36/130 [00:02<00:06, 14.35it/s]
 29%|â–ˆâ–ˆâ–‰       | 38/130 [00:02<00:06, 14.44it/s]
 31%|â–ˆâ–ˆâ–ˆ       | 40/130 [00:02<00:06, 14.36it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 42/130 [00:02<00:06, 14.49it/s]
 34%|â–ˆâ–ˆâ–ˆâ–      | 44/130 [00:02<00:06, 14.31it/s]
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 46/130 [00:02<00:05, 14.31it/s]
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 48/130 [00:03<00:05, 14.18it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 50/130 [00:03<00:05, 14.46it/s]
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 52/130 [00:03<00:05, 14.48it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 54/130 [00:03<00:05, 13.12it/s]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 56/130 [00:03<00:06, 11.91it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 58/130 [00:03<00:05, 12.60it/s]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 60/130 [00:04<00:05, 13.12it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 62/130 [00:04<00:05, 13.57it/s]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 64/130 [00:04<00:04, 14.05it/s]
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 66/130 [00:04<00:04, 14.84it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 68/130 [00:04<00:04, 15.29it/s]
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 70/130 [00:04<00:03, 15.73it/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 72/130 [00:04<00:03, 16.01it/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 74/130 [00:04<00:03, 16.30it/s]
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 76/130 [00:05<00:03, 16.27it/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 78/130 [00:05<00:03, 16.45it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 80/130 [00:05<00:03, 16.35it/s]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 82/130 [00:05<00:02, 16.23it/s]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 84/130 [00:05<00:02, 16.00it/s]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 86/130 [00:05<00:02, 16.01it/s]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 88/130 [00:05<00:02, 15.96it/s]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 90/130 [00:05<00:02, 16.03it/s]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 92/130 [00:06<00:02, 15.94it/s]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 94/130 [00:06<00:02, 15.98it/s]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 96/130 [00:06<00:02, 15.70it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 98/130 [00:06<00:02, 15.65it/s]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 100/130 [00:06<00:01, 15.62it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 102/130 [00:06<00:01, 15.68it/s]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 104/130 [00:06<00:01, 15.70it/s]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 106/130 [00:06<00:01, 15.83it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 108/130 [00:07<00:01, 15.74it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 110/130 [00:07<00:01, 15.29it/s]
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 112/130 [00:07<00:01, 14.32it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 114/130 [00:07<00:01, 13.85it/s]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 116/130 [00:07<00:01, 13.47it/s]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 118/130 [00:07<00:00, 13.31it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 120/130 [00:07<00:00, 13.12it/s]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 122/130 [00:08<00:00, 12.94it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 124/130 [00:08<00:00, 12.82it/s]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 126/130 [00:08<00:00, 13.35it/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 128/130 [00:08<00:00, 14.14it/s]
{'eval_loss': 0.5763684511184692, 'eval_precision': 0.8170586202920986, 'eval_recall': 0.8299448143633109, 'eval_f1': 0.8224007017213473, 'eval_balanced accuracy': 0.8299448143633109, 'eval_runtime': 8.727, 'eval_samples_per_second': 950.957, 'eval_steps_per_second': 14.896, 'epoch': 6.0}
                                                   
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3114/3114 [13:44<00:00,  4.41it/s]
                                                 Saving model checkpoint to model_output_bert\mgsd_trained\checkpoint-3114
Configuration saved in model_output_bert\mgsd_trained\checkpoint-3114\config.json
Model weights saved in model_output_bert\mgsd_trained\checkpoint-3114\model.safetensors
tokenizer config file saved in model_output_bert\mgsd_trained\checkpoint-3114\tokenizer_config.json
Special tokens file saved in model_output_bert\mgsd_trained\checkpoint-3114\special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from model_output_bert\mgsd_trained\checkpoint-1038 (score: 0.3841915428638458).
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3114/3114 [13:45<00:00,  4.41it/s]Deleting older checkpoint [model_output_bert\mgsd_trained\checkpoint-3114] due to args.save_total_limit
{'train_runtime': 825.6032, 'train_samples_per_second': 241.235, 'train_steps_per_second': 3.772, 'train_loss': 0.23568227954146642, 'epoch': 6.0}
[codecarbon WARNING @ 15:05:37] Another instance of codecarbon is already running. Exiting.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3114/3114 [13:45<00:00,  3.77it/s]
Saving model checkpoint to model_output_bert\mgsd_trained
Configuration saved in model_output_bert\mgsd_trained\config.json
Model weights saved in model_output_bert\mgsd_trained\model.safetensors
tokenizer config file saved in model_output_bert\mgsd_trained\tokenizer_config.json
Special tokens file saved in model_output_bert\mgsd_trained\special_tokens_map.json
[codecarbon INFO @ 15:05:37] Energy consumed for RAM : 0.004251 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:05:37] Energy consumed for all GPUs : 0.037165 kWh. Total GPU Power : 146.60670366479312 W
[codecarbon INFO @ 15:05:37] Energy consumed for all CPUs : 0.010226 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:05:37] 0.051642 kWh of electricity used since the beginning.
loading configuration file model_output_bert/mgsd_trained\config.json
Model config BertConfig {
  "_name_or_path": "model_output_bert/mgsd_trained",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file model_output_bert/mgsd_trained\model.safetensors
Estimated total emissions: 0.012269651511220232 kg CO2
Number of unique labels: 2
All model checkpoint weights were used when initializing BertForSequenceClassification.

All the weights of BertForSequenceClassification were initialized from the model checkpoint at model_output_bert/mgsd_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 653/653 [00:00<00:00, 28381.01 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 653/653 [00:00<00:00, 20712.69 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3236, 'input_ids': [101, 2087, 2111, 2040, 2024, 5637, 2024, 20497, 2015, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
Number of unique labels: 2
loading configuration file model_output_bert/mgsd_trained\config.json
Model config BertConfig {
  "_name_or_path": "model_output_bert/mgsd_trained",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file model_output_bert/mgsd_trained\model.safetensors
All model checkpoint weights were used when initializing BertForSequenceClassification.

All the weights of BertForSequenceClassification were initialized from the model checkpoint at model_output_bert/mgsd_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 414/414 [00:00<00:00, 24743.04 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 414/414 [00:00<00:00, 19743.51 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 840, 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'labels': 0}
loading configuration file model_output_bert/mgsd_trained\config.json
Model config BertConfig {
  "_name_or_path": "model_output_bert/mgsd_trained",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file model_output_bert/mgsd_trained\model.safetensors
Number of unique labels: 2
All model checkpoint weights were used when initializing BertForSequenceClassification.

All the weights of BertForSequenceClassification were initialized from the model checkpoint at model_output_bert/mgsd_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10374/10374 [00:00<00:00, 21525.87 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10374/10374 [00:00<00:00, 16610.85 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Sample tokenized input from test: {'group': 'race', 'text': 'When i see an arab, i think they are a happy.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 17851, 'input_ids': [101, 2043, 1045, 2156, 2019, 5424, 1010, 1045, 2228, 2027, 2024, 1037, 3407, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
Number of unique labels: 2
loading configuration file model_output_bert/mgsd_trained\config.json
Model config BertConfig {
  "_name_or_path": "model_output_bert/mgsd_trained",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file model_output_bert/mgsd_trained\model.safetensors
All model checkpoint weights were used when initializing BertForSequenceClassification.

All the weights of BertForSequenceClassification were initialized from the model checkpoint at model_output_bert/mgsd_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11441/11441 [00:00<00:00, 17877.49 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11441/11441 [00:00<00:00, 25284.27 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
Number of unique labels: 2
[codecarbon INFO @ 15:14:43] [setup] RAM Tracking...
[codecarbon INFO @ 15:14:43] [setup] GPU Tracking...
[codecarbon INFO @ 15:14:43] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 15:14:43] [setup] CPU Tracking...
[codecarbon WARNING @ 15:14:43] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon WARNING @ 15:14:45] We saw that you have a AMD Ryzen 7 7700 8-Core Processor but we don't know it. Please contact us.
[codecarbon INFO @ 15:14:45] CPU Model on constant consumption mode: AMD Ryzen 7 7700 8-Core Processor
[codecarbon INFO @ 15:14:45] >>> Tracker's metadata:
[codecarbon INFO @ 15:14:45]   Platform system: Windows-11-10.0.26100-SP0
[codecarbon INFO @ 15:14:45]   Python version: 3.12.12
[codecarbon INFO @ 15:14:45]   CodeCarbon version: 2.8.0
[codecarbon INFO @ 15:14:45]   Available RAM : 47.116 GB
[codecarbon INFO @ 15:14:45]   CPU count: 16
[codecarbon INFO @ 15:14:45]   CPU model: AMD Ryzen 7 7700 8-Core Processor
[codecarbon INFO @ 15:14:45]   GPU count: 1
[codecarbon INFO @ 15:14:45]   GPU model: 1 x NVIDIA GeForce RTX 5060 Ti
[codecarbon INFO @ 15:14:48] Saving emissions data to file D:\UCL\HEARTS-Text-Stereotype-Detection-main\Model Training and Evaluation\emissions.csv
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--google-bert--bert-base-uncased\snapshots\86b5e0934494bd15c9632b12f734a8a67f723594\config.json
Model config BertConfig {
  "_name_or_path": "google-bert/bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file model.safetensors from cache at C:\Users\32618\.cache\huggingface\hub\models--google-bert--bert-base-uncased\snapshots\86b5e0934494bd15c9632b12f734a8a67f723594\model.safetensors
Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--google-bert--bert-base-uncased\snapshots\86b5e0934494bd15c9632b12f734a8a67f723594\config.json
Model config BertConfig {
  "_name_or_path": "google-bert/bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at C:\Users\32618\.cache\huggingface\hub\models--google-bert--bert-base-uncased\snapshots\86b5e0934494bd15c9632b12f734a8a67f723594\vocab.txt
loading file tokenizer.json from cache at C:\Users\32618\.cache\huggingface\hub\models--google-bert--bert-base-uncased\snapshots\86b5e0934494bd15c9632b12f734a8a67f723594\tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--google-bert--bert-base-uncased\snapshots\86b5e0934494bd15c9632b12f734a8a67f723594\tokenizer_config.json
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--google-bert--bert-base-uncased\snapshots\86b5e0934494bd15c9632b12f734a8a67f723594\config.json
Model config BertConfig {
  "_name_or_path": "google-bert/bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2088/2088 [00:00<00:00, 9493.01 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2088/2088 [00:00<00:00, 22334.37 examples/s]
Map:   0%|          | 0/523 [00:00<?, ? examples/s]Sample tokenized input from train: {'text': 'we expect that nonbinary people are out to share their views.', 'label': 0, 'group': 'NB', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 1788, 'input_ids': [101, 2057, 5987, 2008, 2512, 21114, 2854, 2111, 2024, 2041, 2000, 3745, 2037, 5328, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 523/523 [00:00<00:00, 26743.65 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 523/523 [00:00<00:00, 20057.61 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
PyTorch: setting up devices
Sample tokenized input from validation: {'text': 'we expect that nonbinary people are out to share their views.', 'label': 0, 'group': 'NB', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 1788, 'input_ids': [101, 2057, 5987, 2008, 2512, 21114, 2854, 2111, 2024, 2041, 2000, 3745, 2037, 5328, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
D:\UCL\HEARTS-Text-Stereotype-Detection-main\Model Training and Evaluation\BERT_Models_Fine_Tuning.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
[codecarbon ERROR @ 15:14:50] Error: Another instance of codecarbon is probably running as we find `C:\Users\32618\AppData\Local\Temp\.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.
The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running training *****
  Num examples = 2,088
  Num Epochs = 6
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 198
  Number of trainable parameters = 109,483,778
[codecarbon WARNING @ 15:14:50] Another instance of codecarbon is already running. Exiting.
 16%|â–ˆâ–Œ        | 32/198 [00:03<00:18,  8.87it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 523
  Batch size = 64

  0%|          | 0/9 [00:00<?, ?it/s]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:00<00:00, 34.70it/s]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:00<00:00, 29.12it/s]
                                                
 17%|â–ˆâ–‹        | 33/198 [00:04<00:18,  8.87it/s]
                                             Saving model checkpoint to model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-33
Configuration saved in model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-33\config.json
{'eval_loss': 0.20255735516548157, 'eval_precision': 0.9392893660531698, 'eval_recall': 0.936389029964449, 'eval_f1': 0.9378184659283919, 'eval_balanced accuracy': 0.936389029964449, 'eval_runtime': 0.3391, 'eval_samples_per_second': 1542.5, 'eval_steps_per_second': 26.544, 'epoch': 1.0}
Model weights saved in model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-33\model.safetensors
tokenizer config file saved in model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-33\tokenizer_config.json
Special tokens file saved in model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-33\special_tokens_map.json
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/198 [00:08<00:15,  8.85it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 523
  Batch size = 64

  0%|          | 0/9 [00:00<?, ?it/s]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:00<00:00, 34.60it/s]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:00<00:00, 28.93it/s]
                                                
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/198 [00:09<00:14,  8.85it/s]
                                             Saving model checkpoint to model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-66
Configuration saved in model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-66\config.json
{'eval_loss': 0.12119560688734055, 'eval_precision': 0.9536622937785728, 'eval_recall': 0.9636617572371762, 'eval_f1': 0.9584109352831129, 'eval_balanced accuracy': 0.9636617572371762, 'eval_runtime': 0.3438, 'eval_samples_per_second': 1521.026, 'eval_steps_per_second': 26.174, 'epoch': 2.0}
Model weights saved in model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-66\model.safetensors
tokenizer config file saved in model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-66\tokenizer_config.json
Special tokens file saved in model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-66\special_tokens_map.json
Deleting older checkpoint [model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-33] due to args.save_total_limit
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/198 [00:13<00:12,  8.85it/s][codecarbon INFO @ 15:15:03] Energy consumed for RAM : 0.000074 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:15:03] Energy consumed for all GPUs : 0.000479 kWh. Total GPU Power : 114.95257425560294 W
[codecarbon INFO @ 15:15:03] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:15:03] 0.000730 kWh of electricity used since the beginning.
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/198 [00:14<00:11,  8.86it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 523
  Batch size = 64

  0%|          | 0/9 [00:00<?, ?it/s]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:00<00:00, 34.38it/s]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:00<00:00, 29.05it/s]
                                                
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 99/198 [00:14<00:11,  8.86it/s]
                                             Saving model checkpoint to model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-99
Configuration saved in model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-99\config.json
{'eval_loss': 0.07565562427043915, 'eval_precision': 0.9796057162534435, 'eval_recall': 0.9715676316234976, 'eval_f1': 0.975448629040862, 'eval_balanced accuracy': 0.9715676316234976, 'eval_runtime': 0.3405, 'eval_samples_per_second': 1536.1, 'eval_steps_per_second': 26.434, 'epoch': 3.0}
Model weights saved in model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-99\model.safetensors
tokenizer config file saved in model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-99\tokenizer_config.json
Special tokens file saved in model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-99\special_tokens_map.json
Deleting older checkpoint [model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-66] due to args.save_total_limit
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/198 [00:19<00:07,  8.85it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 523
  Batch size = 64

  0%|          | 0/9 [00:00<?, ?it/s]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:00<00:00, 33.49it/s]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:00<00:00, 28.90it/s]
{'eval_loss': 0.04295346140861511, 'eval_precision': 0.9881627349735075, 'eval_recall': 0.9897494498053158, 'eval_f1': 0.988950626412862, 'eval_balanced accuracy': 0.9897494498053158, 'eval_runtime': 0.342, 'eval_samples_per_second': 1529.282, 'eval_steps_per_second': 26.317, 'epoch': 4.0}
                                                 
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 132/198 [00:20<00:07,  8.85it/s]
                                             Saving model checkpoint to model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-132
Configuration saved in model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-132\config.json
Model weights saved in model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-132\model.safetensors
tokenizer config file saved in model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-132\tokenizer_config.json
Special tokens file saved in model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-132\special_tokens_map.json
Deleting older checkpoint [model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-99] due to args.save_total_limit
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/198 [00:25<00:04,  7.87it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 523
  Batch size = 64

  0%|          | 0/9 [00:00<?, ?it/s]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:00<00:00, 33.79it/s]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:00<00:00, 22.63it/s]
                                                 
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/198 [00:25<00:04,  7.87it/s]
                                             Saving model checkpoint to model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-165
Configuration saved in model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-165\config.json
{'eval_loss': 0.05293940380215645, 'eval_precision': 0.9809365105815697, 'eval_recall': 0.9885898087015406, 'eval_f1': 0.9846295946428196, 'eval_balanced accuracy': 0.9885898087015406, 'eval_runtime': 0.4236, 'eval_samples_per_second': 1234.756, 'eval_steps_per_second': 21.248, 'epoch': 5.0}
Model weights saved in model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-165\model.safetensors
tokenizer config file saved in model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-165\tokenizer_config.json
Special tokens file saved in model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-165\special_tokens_map.json
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 176/198 [00:28<00:02,  7.41it/s][codecarbon INFO @ 15:15:18] Energy consumed for RAM : 0.000147 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:15:18] Energy consumed for all GPUs : 0.000974 kWh. Total GPU Power : 118.74381228950921 W
[codecarbon INFO @ 15:15:18] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:15:18] 0.001476 kWh of electricity used since the beginning.
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 197/198 [00:30<00:00,  5.89it/s]Saving model checkpoint to model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-198
Configuration saved in model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-198\config.json
Model weights saved in model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-198\model.safetensors
tokenizer config file saved in model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-198\tokenizer_config.json
Special tokens file saved in model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-198\special_tokens_map.json
Deleting older checkpoint [model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-165] due to args.save_total_limit
The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 523
  Batch size = 64

  0%|          | 0/9 [00:00<?, ?it/s]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:00<00:00, 33.47it/s]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:00<00:00, 27.44it/s]
                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:32<00:00,  5.89it/s]
                                             Saving model checkpoint to model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-198
Configuration saved in model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-198\config.json
{'eval_loss': 0.046422891318798065, 'eval_precision': 0.9837946712131849, 'eval_recall': 0.9899864567462333, 'eval_f1': 0.9868044809257889, 'eval_balanced accuracy': 0.9899864567462333, 'eval_runtime': 0.3428, 'eval_samples_per_second': 1525.739, 'eval_steps_per_second': 26.256, 'epoch': 6.0}
Model weights saved in model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-198\model.safetensors
tokenizer config file saved in model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-198\tokenizer_config.json
Special tokens file saved in model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-198\special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-132 (score: 0.04295346140861511).
{'train_runtime': 34.0492, 'train_samples_per_second': 367.938, 'train_steps_per_second': 5.815, 'train_loss': 0.1202018911188299, 'epoch': 6.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:34<00:00,  5.89it/s]Deleting older checkpoint [model_output_bert\winoqueer_gpt_augmentation_trained\checkpoint-198] due to args.save_total_limit
[codecarbon WARNING @ 15:15:24] Another instance of codecarbon is already running. Exiting.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:34<00:00,  5.80it/s]
Saving model checkpoint to model_output_bert\winoqueer_gpt_augmentation_trained
Configuration saved in model_output_bert\winoqueer_gpt_augmentation_trained\config.json
Model weights saved in model_output_bert\winoqueer_gpt_augmentation_trained\model.safetensors
tokenizer config file saved in model_output_bert\winoqueer_gpt_augmentation_trained\tokenizer_config.json
Special tokens file saved in model_output_bert\winoqueer_gpt_augmentation_trained\special_tokens_map.json
[codecarbon INFO @ 15:15:24] Energy consumed for RAM : 0.000178 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:15:24] Energy consumed for all GPUs : 0.001123 kWh. Total GPU Power : 87.0515613946421 W
[codecarbon INFO @ 15:15:24] Energy consumed for all CPUs : 0.000427 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:15:24] 0.001728 kWh of electricity used since the beginning.
loading configuration file model_output_bert/winoqueer_gpt_augmentation_trained\config.json
Model config BertConfig {
  "_name_or_path": "model_output_bert/winoqueer_gpt_augmentation_trained",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file model_output_bert/winoqueer_gpt_augmentation_trained\model.safetensors
Estimated total emissions: 0.0004104925222028754 kg CO2
Number of unique labels: 2
All model checkpoint weights were used when initializing BertForSequenceClassification.

All the weights of BertForSequenceClassification were initialized from the model checkpoint at model_output_bert/winoqueer_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 653/653 [00:00<00:00, 26632.70 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 653/653 [00:00<00:00, 21706.14 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3236, 'input_ids': [101, 2087, 2111, 2040, 2024, 5637, 2024, 20497, 2015, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
loading configuration file model_output_bert/winoqueer_gpt_augmentation_trained\config.json
Model config BertConfig {
  "_name_or_path": "model_output_bert/winoqueer_gpt_augmentation_trained",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file model_output_bert/winoqueer_gpt_augmentation_trained\model.safetensors
Number of unique labels: 2
All model checkpoint weights were used when initializing BertForSequenceClassification.

All the weights of BertForSequenceClassification were initialized from the model checkpoint at model_output_bert/winoqueer_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 414/414 [00:00<00:00, 27445.82 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 414/414 [00:00<00:00, 20989.77 examples/s]
Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 840, 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'labels': 0}
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Number of unique labels: 2
loading configuration file model_output_bert/winoqueer_gpt_augmentation_trained\config.json
Model config BertConfig {
  "_name_or_path": "model_output_bert/winoqueer_gpt_augmentation_trained",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file model_output_bert/winoqueer_gpt_augmentation_trained\model.safetensors
All model checkpoint weights were used when initializing BertForSequenceClassification.

All the weights of BertForSequenceClassification were initialized from the model checkpoint at model_output_bert/winoqueer_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10374/10374 [00:00<00:00, 12365.93 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10374/10374 [00:00<00:00, 16643.09 examples/s]
Sample tokenized input from test: {'group': 'race', 'text': 'When i see an arab, i think they are a happy.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 17851, 'input_ids': [101, 2043, 1045, 2156, 2019, 5424, 1010, 1045, 2228, 2027, 2024, 1037, 3407, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
loading configuration file model_output_bert/winoqueer_gpt_augmentation_trained\config.json
Model config BertConfig {
  "_name_or_path": "model_output_bert/winoqueer_gpt_augmentation_trained",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file model_output_bert/winoqueer_gpt_augmentation_trained\model.safetensors
Number of unique labels: 2
All model checkpoint weights were used when initializing BertForSequenceClassification.

All the weights of BertForSequenceClassification were initialized from the model checkpoint at model_output_bert/winoqueer_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11441/11441 [00:00<00:00, 24110.72 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11441/11441 [00:00<00:00, 20533.91 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
Number of unique labels: 2
[codecarbon INFO @ 15:24:18] [setup] RAM Tracking...
[codecarbon INFO @ 15:24:18] [setup] GPU Tracking...
[codecarbon INFO @ 15:24:18] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 15:24:18] [setup] CPU Tracking...
[codecarbon WARNING @ 15:24:18] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon WARNING @ 15:24:20] We saw that you have a AMD Ryzen 7 7700 8-Core Processor but we don't know it. Please contact us.
[codecarbon INFO @ 15:24:20] CPU Model on constant consumption mode: AMD Ryzen 7 7700 8-Core Processor
[codecarbon INFO @ 15:24:20] >>> Tracker's metadata:
[codecarbon INFO @ 15:24:20]   Platform system: Windows-11-10.0.26100-SP0
[codecarbon INFO @ 15:24:20]   Python version: 3.12.12
[codecarbon INFO @ 15:24:20]   CodeCarbon version: 2.8.0
[codecarbon INFO @ 15:24:20]   Available RAM : 47.116 GB
[codecarbon INFO @ 15:24:20]   CPU count: 16
[codecarbon INFO @ 15:24:20]   CPU model: AMD Ryzen 7 7700 8-Core Processor
[codecarbon INFO @ 15:24:20]   GPU count: 1
[codecarbon INFO @ 15:24:20]   GPU model: 1 x NVIDIA GeForce RTX 5060 Ti
[codecarbon INFO @ 15:24:23] Saving emissions data to file D:\UCL\HEARTS-Text-Stereotype-Detection-main\Model Training and Evaluation\emissions.csv
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--google-bert--bert-base-uncased\snapshots\86b5e0934494bd15c9632b12f734a8a67f723594\config.json
Model config BertConfig {
  "_name_or_path": "google-bert/bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file model.safetensors from cache at C:\Users\32618\.cache\huggingface\hub\models--google-bert--bert-base-uncased\snapshots\86b5e0934494bd15c9632b12f734a8a67f723594\model.safetensors
Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--google-bert--bert-base-uncased\snapshots\86b5e0934494bd15c9632b12f734a8a67f723594\config.json
Model config BertConfig {
  "_name_or_path": "google-bert/bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at C:\Users\32618\.cache\huggingface\hub\models--google-bert--bert-base-uncased\snapshots\86b5e0934494bd15c9632b12f734a8a67f723594\vocab.txt
loading file tokenizer.json from cache at C:\Users\32618\.cache\huggingface\hub\models--google-bert--bert-base-uncased\snapshots\86b5e0934494bd15c9632b12f734a8a67f723594\tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--google-bert--bert-base-uncased\snapshots\86b5e0934494bd15c9632b12f734a8a67f723594\tokenizer_config.json
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--google-bert--bert-base-uncased\snapshots\86b5e0934494bd15c9632b12f734a8a67f723594\config.json
Model config BertConfig {
  "_name_or_path": "google-bert/bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1324/1324 [00:00<00:00, 29051.53 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1324/1324 [00:00<00:00, 21040.89 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 332/332 [00:00<00:00, 25531.88 examples/s]
Sample tokenized input from train: {'text': 'The average Chadian income is modest compared to developed countries', 'label': 0, 'group': 'Chadian', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 1249, 'input_ids': [101, 1996, 2779, 9796, 2937, 3318, 2003, 10754, 4102, 2000, 2764, 3032, 102, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 332/332 [00:00<00:00, 21415.97 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
PyTorch: setting up devices
Sample tokenized input from validation: {'text': 'The average Chadian income is modest compared to developed countries', 'label': 0, 'group': 'Chadian', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 1249, 'input_ids': [101, 1996, 2779, 9796, 2937, 3318, 2003, 10754, 4102, 2000, 2764, 3032, 102, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
D:\UCL\HEARTS-Text-Stereotype-Detection-main\Model Training and Evaluation\BERT_Models_Fine_Tuning.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
[codecarbon ERROR @ 15:24:24] Error: Another instance of codecarbon is probably running as we find `C:\Users\32618\AppData\Local\Temp\.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.
The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running training *****
  Num examples = 1,324
  Num Epochs = 6
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 126
  Number of trainable parameters = 109,483,778
[codecarbon WARNING @ 15:24:25] Another instance of codecarbon is already running. Exiting.
 16%|â–ˆâ–Œ        | 20/126 [00:01<00:09, 10.82it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 332
  Batch size = 64

  0%|          | 0/6 [00:00<?, ?it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:00<00:00, 49.68it/s]
{'eval_loss': 0.4905111491680145, 'eval_precision': 0.8058879322924266, 'eval_recall': 0.84181240063593, 'eval_f1': 0.8084645576204663, 'eval_balanced accuracy': 0.84181240063593, 'eval_runtime': 0.1575, 'eval_samples_per_second': 2108.278, 'eval_steps_per_second': 38.101, 'epoch': 1.0}
                                                
 17%|â–ˆâ–‹        | 21/126 [00:02<00:09, 10.82it/s]
                                             Saving model checkpoint to model_output_bert\seegull_gpt_augmentation_trained\checkpoint-21
Configuration saved in model_output_bert\seegull_gpt_augmentation_trained\checkpoint-21\config.json
Model weights saved in model_output_bert\seegull_gpt_augmentation_trained\checkpoint-21\model.safetensors
tokenizer config file saved in model_output_bert\seegull_gpt_augmentation_trained\checkpoint-21\tokenizer_config.json
Special tokens file saved in model_output_bert\seegull_gpt_augmentation_trained\checkpoint-21\special_tokens_map.json
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 42/126 [00:05<00:07, 10.57it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 332
  Batch size = 64

  0%|          | 0/6 [00:00<?, ?it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 55.93it/s]
                                                
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 42/126 [00:05<00:07, 10.57it/s]
                                             Saving model checkpoint to model_output_bert\seegull_gpt_augmentation_trained\checkpoint-42
Configuration saved in model_output_bert\seegull_gpt_augmentation_trained\checkpoint-42\config.json
{'eval_loss': 0.29089948534965515, 'eval_precision': 0.8703829338157696, 'eval_recall': 0.9005951653010477, 'eval_f1': 0.8803370512882958, 'eval_balanced accuracy': 0.9005951653010477, 'eval_runtime': 0.1552, 'eval_samples_per_second': 2138.728, 'eval_steps_per_second': 38.652, 'epoch': 2.0}
Model weights saved in model_output_bert\seegull_gpt_augmentation_trained\checkpoint-42\model.safetensors
tokenizer config file saved in model_output_bert\seegull_gpt_augmentation_trained\checkpoint-42\tokenizer_config.json
Special tokens file saved in model_output_bert\seegull_gpt_augmentation_trained\checkpoint-42\special_tokens_map.json
Deleting older checkpoint [model_output_bert\seegull_gpt_augmentation_trained\checkpoint-21] due to args.save_total_limit
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 62/126 [00:08<00:06,  9.98it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 332
  Batch size = 64

  0%|          | 0/6 [00:00<?, ?it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:00<00:00, 49.95it/s]
                                                
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 63/126 [00:08<00:06,  9.98it/s]
                                             Saving model checkpoint to model_output_bert\seegull_gpt_augmentation_trained\checkpoint-63
Configuration saved in model_output_bert\seegull_gpt_augmentation_trained\checkpoint-63\config.json
{'eval_loss': 0.21322953701019287, 'eval_precision': 0.8954503676470589, 'eval_recall': 0.9209367738779504, 'eval_f1': 0.9052128968742308, 'eval_balanced accuracy': 0.9209367738779504, 'eval_runtime': 0.1569, 'eval_samples_per_second': 2115.523, 'eval_steps_per_second': 38.232, 'epoch': 3.0}
Model weights saved in model_output_bert\seegull_gpt_augmentation_trained\checkpoint-63\model.safetensors
tokenizer config file saved in model_output_bert\seegull_gpt_augmentation_trained\checkpoint-63\tokenizer_config.json
Special tokens file saved in model_output_bert\seegull_gpt_augmentation_trained\checkpoint-63\special_tokens_map.json
Deleting older checkpoint [model_output_bert\seegull_gpt_augmentation_trained\checkpoint-42] due to args.save_total_limit
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 84/126 [00:11<00:03, 10.51it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 332
  Batch size = 64

  0%|          | 0/6 [00:00<?, ?it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:00<00:00, 46.81it/s]
                                                
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 84/126 [00:11<00:03, 10.51it/s]
                                             Saving model checkpoint to model_output_bert\seegull_gpt_augmentation_trained\checkpoint-84
Configuration saved in model_output_bert\seegull_gpt_augmentation_trained\checkpoint-84\config.json
{'eval_loss': 0.22995805740356445, 'eval_precision': 0.8986275615717241, 'eval_recall': 0.9321674615792263, 'eval_f1': 0.9095966079278017, 'eval_balanced accuracy': 0.9321674615792263, 'eval_runtime': 0.1644, 'eval_samples_per_second': 2019.39, 'eval_steps_per_second': 36.495, 'epoch': 4.0}
Model weights saved in model_output_bert\seegull_gpt_augmentation_trained\checkpoint-84\model.safetensors
tokenizer config file saved in model_output_bert\seegull_gpt_augmentation_trained\checkpoint-84\tokenizer_config.json
Special tokens file saved in model_output_bert\seegull_gpt_augmentation_trained\checkpoint-84\special_tokens_map.json
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 92/126 [00:13<00:05,  6.31it/s][codecarbon INFO @ 15:24:38] Energy consumed for RAM : 0.000074 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:24:38] Energy consumed for all GPUs : 0.000406 kWh. Total GPU Power : 97.4186091743504 W
[codecarbon INFO @ 15:24:38] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:24:38] 0.000657 kWh of electricity used since the beginning.
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 104/126 [00:14<00:02,  9.95it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 332
  Batch size = 64

  0%|          | 0/6 [00:00<?, ?it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:00<00:00, 48.49it/s]
                                                 
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 105/126 [00:14<00:02,  9.95it/s]
                                             Saving model checkpoint to model_output_bert\seegull_gpt_augmentation_trained\checkpoint-105
Configuration saved in model_output_bert\seegull_gpt_augmentation_trained\checkpoint-105\config.json
{'eval_loss': 0.19810137152671814, 'eval_precision': 0.9081648284313726, 'eval_recall': 0.9344706697647874, 'eval_f1': 0.9182869800639921, 'eval_balanced accuracy': 0.9344706697647874, 'eval_runtime': 0.162, 'eval_samples_per_second': 2049.151, 'eval_steps_per_second': 37.033, 'epoch': 5.0}
Model weights saved in model_output_bert\seegull_gpt_augmentation_trained\checkpoint-105\model.safetensors
tokenizer config file saved in model_output_bert\seegull_gpt_augmentation_trained\checkpoint-105\tokenizer_config.json
Special tokens file saved in model_output_bert\seegull_gpt_augmentation_trained\checkpoint-105\special_tokens_map.json
Deleting older checkpoint [model_output_bert\seegull_gpt_augmentation_trained\checkpoint-63] due to args.save_total_limit
Deleting older checkpoint [model_output_bert\seegull_gpt_augmentation_trained\checkpoint-84] due to args.save_total_limit
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:18<00:00, 10.41it/s]Saving model checkpoint to model_output_bert\seegull_gpt_augmentation_trained\checkpoint-126
Configuration saved in model_output_bert\seegull_gpt_augmentation_trained\checkpoint-126\config.json
Model weights saved in model_output_bert\seegull_gpt_augmentation_trained\checkpoint-126\model.safetensors
tokenizer config file saved in model_output_bert\seegull_gpt_augmentation_trained\checkpoint-126\tokenizer_config.json
Special tokens file saved in model_output_bert\seegull_gpt_augmentation_trained\checkpoint-126\special_tokens_map.json
The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 332
  Batch size = 64

  0%|          | 0/6 [00:00<?, ?it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 55.28it/s]
                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:19<00:00, 10.41it/s]
                                             Saving model checkpoint to model_output_bert\seegull_gpt_augmentation_trained\checkpoint-126
Configuration saved in model_output_bert\seegull_gpt_augmentation_trained\checkpoint-126\config.json
{'eval_loss': 0.20067362487316132, 'eval_precision': 0.907958872810358, 'eval_recall': 0.936712730830378, 'eval_f1': 0.9185819526598198, 'eval_balanced accuracy': 0.936712730830378, 'eval_runtime': 0.1387, 'eval_samples_per_second': 2394.38, 'eval_steps_per_second': 43.272, 'epoch': 6.0}
Model weights saved in model_output_bert\seegull_gpt_augmentation_trained\checkpoint-126\model.safetensors
tokenizer config file saved in model_output_bert\seegull_gpt_augmentation_trained\checkpoint-126\tokenizer_config.json
Special tokens file saved in model_output_bert\seegull_gpt_augmentation_trained\checkpoint-126\special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from model_output_bert\seegull_gpt_augmentation_trained\checkpoint-105 (score: 0.19810137152671814).
{'train_runtime': 20.852, 'train_samples_per_second': 380.97, 'train_steps_per_second': 6.043, 'train_loss': 0.23206059894864522, 'epoch': 6.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:20<00:00, 10.41it/s]Deleting older checkpoint [model_output_bert\seegull_gpt_augmentation_trained\checkpoint-126] due to args.save_total_limit
[codecarbon WARNING @ 15:24:46] Another instance of codecarbon is already running. Exiting.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:20<00:00,  6.02it/s]
Saving model checkpoint to model_output_bert\seegull_gpt_augmentation_trained
Configuration saved in model_output_bert\seegull_gpt_augmentation_trained\config.json
Model weights saved in model_output_bert\seegull_gpt_augmentation_trained\model.safetensors
tokenizer config file saved in model_output_bert\seegull_gpt_augmentation_trained\tokenizer_config.json
Special tokens file saved in model_output_bert\seegull_gpt_augmentation_trained\special_tokens_map.json
[codecarbon INFO @ 15:24:46] Energy consumed for RAM : 0.000111 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:24:46] Energy consumed for all GPUs : 0.000585 kWh. Total GPU Power : 84.01671300818596 W
[codecarbon INFO @ 15:24:46] Energy consumed for all CPUs : 0.000268 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:24:46] 0.000964 kWh of electricity used since the beginning.
loading configuration file model_output_bert/seegull_gpt_augmentation_trained\config.json
Model config BertConfig {
  "_name_or_path": "model_output_bert/seegull_gpt_augmentation_trained",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file model_output_bert/seegull_gpt_augmentation_trained\model.safetensors
Estimated total emissions: 0.0002291020578201154 kg CO2
Number of unique labels: 2
All model checkpoint weights were used when initializing BertForSequenceClassification.

All the weights of BertForSequenceClassification were initialized from the model checkpoint at model_output_bert/seegull_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 653/653 [00:00<00:00, 32627.86 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 653/653 [00:00<00:00, 23286.24 examples/s]
Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3236, 'input_ids': [101, 2087, 2111, 2040, 2024, 5637, 2024, 20497, 2015, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
loading configuration file model_output_bert/seegull_gpt_augmentation_trained\config.json
Model config BertConfig {
  "_name_or_path": "model_output_bert/seegull_gpt_augmentation_trained",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file model_output_bert/seegull_gpt_augmentation_trained\model.safetensors
Number of unique labels: 2
All model checkpoint weights were used when initializing BertForSequenceClassification.

All the weights of BertForSequenceClassification were initialized from the model checkpoint at model_output_bert/seegull_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 414/414 [00:00<00:00, 27561.69 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 414/414 [00:00<00:00, 22365.88 examples/s]
Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 840, 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'labels': 0}
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Number of unique labels: 2
loading configuration file model_output_bert/seegull_gpt_augmentation_trained\config.json
Model config BertConfig {
  "_name_or_path": "model_output_bert/seegull_gpt_augmentation_trained",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file model_output_bert/seegull_gpt_augmentation_trained\model.safetensors
All model checkpoint weights were used when initializing BertForSequenceClassification.

All the weights of BertForSequenceClassification were initialized from the model checkpoint at model_output_bert/seegull_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10374/10374 [00:00<00:00, 23966.67 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10374/10374 [00:00<00:00, 18924.63 examples/s]
Sample tokenized input from test: {'group': 'race', 'text': 'When i see an arab, i think they are a happy.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 17851, 'input_ids': [101, 2043, 1045, 2156, 2019, 5424, 1010, 1045, 2228, 2027, 2024, 1037, 3407, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Number of unique labels: 2
loading configuration file model_output_bert/seegull_gpt_augmentation_trained\config.json
Model config BertConfig {
  "_name_or_path": "model_output_bert/seegull_gpt_augmentation_trained",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file model_output_bert/seegull_gpt_augmentation_trained\model.safetensors
All model checkpoint weights were used when initializing BertForSequenceClassification.

All the weights of BertForSequenceClassification were initialized from the model checkpoint at model_output_bert/seegull_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11441/11441 [00:00<00:00, 24571.03 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11441/11441 [00:00<00:00, 20938.92 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
Number of unique labels: 2
[codecarbon INFO @ 15:33:24] [setup] RAM Tracking...
[codecarbon INFO @ 15:33:24] [setup] GPU Tracking...
[codecarbon INFO @ 15:33:24] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 15:33:24] [setup] CPU Tracking...
[codecarbon WARNING @ 15:33:24] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon WARNING @ 15:33:26] We saw that you have a AMD Ryzen 7 7700 8-Core Processor but we don't know it. Please contact us.
[codecarbon INFO @ 15:33:26] CPU Model on constant consumption mode: AMD Ryzen 7 7700 8-Core Processor
[codecarbon INFO @ 15:33:26] >>> Tracker's metadata:
[codecarbon INFO @ 15:33:26]   Platform system: Windows-11-10.0.26100-SP0
[codecarbon INFO @ 15:33:26]   Python version: 3.12.12
[codecarbon INFO @ 15:33:26]   CodeCarbon version: 2.8.0
[codecarbon INFO @ 15:33:26]   Available RAM : 47.116 GB
[codecarbon INFO @ 15:33:26]   CPU count: 16
[codecarbon INFO @ 15:33:26]   CPU model: AMD Ryzen 7 7700 8-Core Processor
[codecarbon INFO @ 15:33:26]   GPU count: 1
[codecarbon INFO @ 15:33:26]   GPU model: 1 x NVIDIA GeForce RTX 5060 Ti
[codecarbon INFO @ 15:33:29] Saving emissions data to file D:\UCL\HEARTS-Text-Stereotype-Detection-main\Model Training and Evaluation\emissions.csv
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--google-bert--bert-base-uncased\snapshots\86b5e0934494bd15c9632b12f734a8a67f723594\config.json
Model config BertConfig {
  "_name_or_path": "google-bert/bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file model.safetensors from cache at C:\Users\32618\.cache\huggingface\hub\models--google-bert--bert-base-uncased\snapshots\86b5e0934494bd15c9632b12f734a8a67f723594\model.safetensors
Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--google-bert--bert-base-uncased\snapshots\86b5e0934494bd15c9632b12f734a8a67f723594\config.json
Model config BertConfig {
  "_name_or_path": "google-bert/bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading file vocab.txt from cache at C:\Users\32618\.cache\huggingface\hub\models--google-bert--bert-base-uncased\snapshots\86b5e0934494bd15c9632b12f734a8a67f723594\vocab.txt
loading file tokenizer.json from cache at C:\Users\32618\.cache\huggingface\hub\models--google-bert--bert-base-uncased\snapshots\86b5e0934494bd15c9632b12f734a8a67f723594\tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--google-bert--bert-base-uncased\snapshots\86b5e0934494bd15c9632b12f734a8a67f723594\tokenizer_config.json
loading configuration file config.json from cache at C:\Users\32618\.cache\huggingface\hub\models--google-bert--bert-base-uncased\snapshots\86b5e0934494bd15c9632b12f734a8a67f723594\config.json
Model config BertConfig {
  "_name_or_path": "google-bert/bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36608/36608 [00:01<00:00, 20623.24 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36608/36608 [00:01<00:00, 20818.71 examples/s]
Sample tokenized input from train: {'text': 'The commander could tell the soldier was disciplined.', 'label': 1, 'group': 'profession', 'data_name': 'MGSD', '__index_level_0__': 7380, 'input_ids': [101, 1996, 3474, 2071, 2425, 1996, 5268, 2001, 28675, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1}
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9152/9152 [00:00<00:00, 17734.09 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9152/9152 [00:00<00:00, 25224.24 examples/s]
Sample tokenized input from validation: {'text': 'The commander could tell the soldier was disciplined.', 'label': 1, 'group': 'profession', 'data_name': 'MGSD', '__index_level_0__': 7380, 'input_ids': [101, 1996, 3474, 2071, 2425, 1996, 5268, 2001, 28675, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1}
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
D:\UCL\HEARTS-Text-Stereotype-Detection-main\Model Training and Evaluation\BERT_Models_Fine_Tuning.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
[codecarbon ERROR @ 15:33:35] Error: Another instance of codecarbon is probably running as we find `C:\Users\32618\AppData\Local\Temp\.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.
The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running training *****
  Num examples = 36,608
  Num Epochs = 6
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 3,432
  Number of trainable parameters = 109,483,778
[codecarbon WARNING @ 15:33:35] Another instance of codecarbon is already running. Exiting.
  1%|â–         | 43/3432 [00:09<11:55,  4.74it/s][codecarbon INFO @ 15:33:44] Energy consumed for RAM : 0.000074 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:33:44] Energy consumed for all GPUs : 0.000427 kWh. Total GPU Power : 102.53603842551856 W
[codecarbon INFO @ 15:33:44] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:33:44] 0.000678 kWh of electricity used since the beginning.
  3%|â–Ž         | 113/3432 [00:24<11:48,  4.68it/s][codecarbon INFO @ 15:33:59] Energy consumed for RAM : 0.000147 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:33:59] Energy consumed for all GPUs : 0.001122 kWh. Total GPU Power : 166.46856771590527 W
[codecarbon INFO @ 15:33:59] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:33:59] 0.001623 kWh of electricity used since the beginning.
  5%|â–Œ         | 184/3432 [00:39<11:35,  4.67it/s][codecarbon INFO @ 15:34:14] Energy consumed for RAM : 0.000221 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:34:14] Energy consumed for all GPUs : 0.001820 kWh. Total GPU Power : 167.4294324394413 W
[codecarbon INFO @ 15:34:14] Energy consumed for all CPUs : 0.000532 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:34:14] 0.002572 kWh of electricity used since the beginning.
  7%|â–‹         | 254/3432 [00:54<11:18,  4.68it/s][codecarbon INFO @ 15:34:29] Energy consumed for RAM : 0.000295 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:34:29] Energy consumed for all GPUs : 0.002522 kWh. Total GPU Power : 168.31812369297649 W
[codecarbon INFO @ 15:34:29] Energy consumed for all CPUs : 0.000709 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:34:29] 0.003525 kWh of electricity used since the beginning.
  9%|â–‰         | 324/3432 [01:09<11:05,  4.67it/s][codecarbon INFO @ 15:34:44] Energy consumed for RAM : 0.000368 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:34:44] Energy consumed for all GPUs : 0.003223 kWh. Total GPU Power : 168.0711403218791 W
[codecarbon INFO @ 15:34:44] Energy consumed for all CPUs : 0.000886 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:34:44] 0.004477 kWh of electricity used since the beginning.
 11%|â–ˆâ–        | 394/3432 [01:24<10:47,  4.69it/s][codecarbon INFO @ 15:35:00] Energy consumed for RAM : 0.000442 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:35:00] Energy consumed for all GPUs : 0.003930 kWh. Total GPU Power : 169.70383424232492 W
[codecarbon INFO @ 15:35:00] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:35:00] 0.005436 kWh of electricity used since the beginning.
 14%|â–ˆâ–Ž        | 464/3432 [01:39<10:37,  4.65it/s][codecarbon INFO @ 15:35:15] Energy consumed for RAM : 0.000516 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:35:15] Energy consumed for all GPUs : 0.004635 kWh. Total GPU Power : 169.2214604701228 W
[codecarbon INFO @ 15:35:15] Energy consumed for all CPUs : 0.001240 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:35:15] 0.006392 kWh of electricity used since the beginning.
 15%|â–ˆâ–        | 500/3432 [01:46<10:27,  4.67it/s]{'loss': 0.4559, 'grad_norm': 5.60117244720459, 'learning_rate': 1.7086247086247088e-05, 'epoch': 0.87}
 16%|â–ˆâ–Œ        | 535/3432 [01:54<10:19,  4.67it/s][codecarbon INFO @ 15:35:30] Energy consumed for RAM : 0.000589 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:35:30] Energy consumed for all GPUs : 0.005341 kWh. Total GPU Power : 169.27532765748037 W
[codecarbon INFO @ 15:35:30] Energy consumed for all CPUs : 0.001418 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:35:30] 0.007348 kWh of electricity used since the beginning.
[codecarbon INFO @ 15:35:30] 0.014538 g.CO2eq/s mean an estimation of 458.4733513334882 kg.CO2eq/year
 17%|â–ˆâ–‹        | 572/3432 [02:02<10:05,  4.72it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 9152
  Batch size = 64

  0%|          | 0/143 [00:00<?, ?it/s]
  2%|â–         | 3/143 [00:00<00:05, 24.98it/s]
  4%|â–         | 6/143 [00:00<00:07, 18.53it/s]
  6%|â–Œ         | 8/143 [00:00<00:07, 17.46it/s]
  7%|â–‹         | 10/143 [00:00<00:07, 17.21it/s]
  8%|â–Š         | 12/143 [00:00<00:07, 16.88it/s]
 10%|â–‰         | 14/143 [00:00<00:07, 16.78it/s]
 11%|â–ˆ         | 16/143 [00:00<00:08, 15.71it/s]
 13%|â–ˆâ–Ž        | 18/143 [00:01<00:08, 14.48it/s]
 14%|â–ˆâ–        | 20/143 [00:01<00:09, 13.58it/s]
 15%|â–ˆâ–Œ        | 22/143 [00:01<00:09, 13.14it/s]
 17%|â–ˆâ–‹        | 24/143 [00:01<00:09, 12.74it/s]
 18%|â–ˆâ–Š        | 26/143 [00:01<00:09, 12.60it/s]
 20%|â–ˆâ–‰        | 28/143 [00:01<00:09, 12.40it/s]
 21%|â–ˆâ–ˆ        | 30/143 [00:02<00:09, 12.36it/s]
 22%|â–ˆâ–ˆâ–       | 32/143 [00:02<00:09, 12.26it/s]
 24%|â–ˆâ–ˆâ–       | 34/143 [00:02<00:08, 13.04it/s]
 25%|â–ˆâ–ˆâ–Œ       | 36/143 [00:02<00:07, 13.57it/s]
 27%|â–ˆâ–ˆâ–‹       | 38/143 [00:02<00:07, 14.02it/s]
 28%|â–ˆâ–ˆâ–Š       | 40/143 [00:02<00:07, 14.21it/s]
 29%|â–ˆâ–ˆâ–‰       | 42/143 [00:02<00:06, 14.48it/s]
 31%|â–ˆâ–ˆâ–ˆ       | 44/143 [00:03<00:06, 14.57it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 46/143 [00:03<00:06, 14.74it/s]
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 48/143 [00:03<00:06, 15.13it/s]
 35%|â–ˆâ–ˆâ–ˆâ–      | 50/143 [00:03<00:05, 15.88it/s]
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 52/143 [00:03<00:05, 16.40it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 54/143 [00:03<00:05, 16.84it/s]
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 56/143 [00:03<00:05, 17.08it/s]
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 58/143 [00:03<00:04, 17.37it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 60/143 [00:03<00:04, 17.53it/s]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 62/143 [00:04<00:04, 17.65it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 64/143 [00:04<00:04, 16.95it/s]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 66/143 [00:04<00:04, 16.69it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 68/143 [00:04<00:04, 16.32it/s]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 70/143 [00:04<00:04, 16.24it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 72/143 [00:04<00:04, 15.99it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 74/143 [00:04<00:04, 15.95it/s]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 76/143 [00:04<00:04, 15.80it/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78/143 [00:05<00:04, 15.87it/s]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 80/143 [00:05<00:04, 15.69it/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 82/143 [00:05<00:03, 15.76it/s]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 84/143 [00:05<00:03, 15.62it/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 86/143 [00:05<00:03, 15.69it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 88/143 [00:05<00:03, 15.52it/s]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 90/143 [00:05<00:03, 15.62it/s]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 92/143 [00:06<00:03, 15.50it/s]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 94/143 [00:06<00:03, 15.39it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 96/143 [00:06<00:03, 14.91it/s]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 98/143 [00:06<00:03, 14.75it/s]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 100/143 [00:06<00:02, 14.62it/s]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 102/143 [00:06<00:02, 14.54it/s]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 104/143 [00:06<00:02, 14.36it/s]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 106/143 [00:07<00:02, 14.37it/s][codecarbon INFO @ 15:35:45] Energy consumed for RAM : 0.000663 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:35:45] Energy consumed for all GPUs : 0.006065 kWh. Total GPU Power : 173.75145475524332 W
[codecarbon INFO @ 15:35:45] Energy consumed for all CPUs : 0.001595 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:35:45] 0.008323 kWh of electricity used since the beginning.

 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 108/143 [00:07<00:02, 14.05it/s]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 110/143 [00:07<00:02, 14.22it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 112/143 [00:07<00:02, 15.09it/s]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 114/143 [00:07<00:01, 15.95it/s]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 116/143 [00:07<00:01, 16.56it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 118/143 [00:07<00:01, 17.05it/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 120/143 [00:07<00:01, 17.34it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 122/143 [00:07<00:01, 17.53it/s]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 124/143 [00:08<00:01, 17.56it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 126/143 [00:08<00:00, 17.73it/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 128/143 [00:08<00:00, 17.86it/s]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 130/143 [00:08<00:00, 17.97it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 132/143 [00:08<00:00, 17.97it/s]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 134/143 [00:08<00:00, 18.03it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 136/143 [00:08<00:00, 18.18it/s]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 138/143 [00:08<00:00, 18.16it/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 140/143 [00:08<00:00, 18.03it/s]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 142/143 [00:09<00:00, 18.31it/s]
                                                  
 17%|â–ˆâ–‹        | 572/3432 [02:11<10:05,  4.72it/s]
                                                 Saving model checkpoint to model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-572
Configuration saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-572\config.json
{'eval_loss': 0.375211238861084, 'eval_precision': 0.7992637533012455, 'eval_recall': 0.7999912528537565, 'eval_f1': 0.799624351784975, 'eval_balanced accuracy': 0.7999912528537565, 'eval_runtime': 9.1774, 'eval_samples_per_second': 997.235, 'eval_steps_per_second': 15.582, 'epoch': 1.0}
Model weights saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-572\model.safetensors
tokenizer config file saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-572\tokenizer_config.json
Special tokens file saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-572\special_tokens_map.json
 18%|â–ˆâ–Š        | 626/3432 [02:24<09:57,  4.70it/s][codecarbon INFO @ 15:36:00] Energy consumed for RAM : 0.000737 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:36:00] Energy consumed for all GPUs : 0.006727 kWh. Total GPU Power : 158.64080099435267 W
[codecarbon INFO @ 15:36:00] Energy consumed for all CPUs : 0.001772 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:36:00] 0.009235 kWh of electricity used since the beginning.
 20%|â–ˆâ–ˆ        | 696/3432 [02:39<09:46,  4.66it/s][codecarbon INFO @ 15:36:15] Energy consumed for RAM : 0.000810 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:36:15] Energy consumed for all GPUs : 0.007433 kWh. Total GPU Power : 169.3656577653752 W
[codecarbon INFO @ 15:36:15] Energy consumed for all CPUs : 0.001949 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:36:15] 0.010192 kWh of electricity used since the beginning.
 22%|â–ˆâ–ˆâ–       | 767/3432 [02:54<09:30,  4.67it/s][codecarbon INFO @ 15:36:30] Energy consumed for RAM : 0.000884 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:36:30] Energy consumed for all GPUs : 0.008141 kWh. Total GPU Power : 169.96248142217812 W
[codecarbon INFO @ 15:36:30] Energy consumed for all CPUs : 0.002126 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:36:30] 0.011152 kWh of electricity used since the beginning.
 24%|â–ˆâ–ˆâ–       | 837/3432 [03:09<09:15,  4.67it/s][codecarbon INFO @ 15:36:45] Energy consumed for RAM : 0.000958 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:36:45] Energy consumed for all GPUs : 0.008850 kWh. Total GPU Power : 170.04014565768972 W
[codecarbon INFO @ 15:36:45] Energy consumed for all CPUs : 0.002304 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:36:45] 0.012111 kWh of electricity used since the beginning.
 26%|â–ˆâ–ˆâ–‹       | 907/3432 [03:24<09:02,  4.65it/s][codecarbon INFO @ 15:37:00] Energy consumed for RAM : 0.001031 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:37:00] Energy consumed for all GPUs : 0.009560 kWh. Total GPU Power : 170.33849943644628 W
[codecarbon INFO @ 15:37:00] Energy consumed for all CPUs : 0.002481 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:37:00] 0.013072 kWh of electricity used since the beginning.
 28%|â–ˆâ–ˆâ–Š       | 977/3432 [03:39<08:44,  4.68it/s][codecarbon INFO @ 15:37:15] Energy consumed for RAM : 0.001105 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:37:15] Energy consumed for all GPUs : 0.010270 kWh. Total GPU Power : 170.2685033377778 W
[codecarbon INFO @ 15:37:15] Energy consumed for all CPUs : 0.002658 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:37:15] 0.014033 kWh of electricity used since the beginning.
 29%|â–ˆâ–ˆâ–‰       | 1000/3432 [03:44<08:42,  4.66it/s]{'loss': 0.3096, 'grad_norm': 5.336028099060059, 'learning_rate': 1.4172494172494174e-05, 'epoch': 1.75}
 31%|â–ˆâ–ˆâ–ˆ       | 1047/3432 [03:54<08:31,  4.66it/s][codecarbon INFO @ 15:37:30] Energy consumed for RAM : 0.001179 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:37:30] Energy consumed for all GPUs : 0.010982 kWh. Total GPU Power : 170.90225920253548 W
[codecarbon INFO @ 15:37:30] Energy consumed for all CPUs : 0.002835 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:37:30] 0.014996 kWh of electricity used since the beginning.
[codecarbon INFO @ 15:37:30] 0.015134 g.CO2eq/s mean an estimation of 477.26716550322345 kg.CO2eq/year
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1117/3432 [04:09<08:15,  4.67it/s][codecarbon INFO @ 15:37:45] Energy consumed for RAM : 0.001252 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:37:45] Energy consumed for all GPUs : 0.011700 kWh. Total GPU Power : 172.1218673075109 W
[codecarbon INFO @ 15:37:45] Energy consumed for all CPUs : 0.003012 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:37:45] 0.015965 kWh of electricity used since the beginning.
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1144/3432 [04:14<08:01,  4.75it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 9152
  Batch size = 64

  0%|          | 0/143 [00:00<?, ?it/s]
  2%|â–         | 3/143 [00:00<00:05, 25.29it/s]
  4%|â–         | 6/143 [00:00<00:07, 18.51it/s]
  6%|â–Œ         | 8/143 [00:00<00:07, 17.75it/s]
  7%|â–‹         | 10/143 [00:00<00:07, 17.16it/s]
  8%|â–Š         | 12/143 [00:00<00:07, 17.00it/s]
 10%|â–‰         | 14/143 [00:00<00:07, 16.71it/s]
 11%|â–ˆ         | 16/143 [00:00<00:08, 15.72it/s]
 13%|â–ˆâ–Ž        | 18/143 [00:01<00:08, 14.34it/s]
 14%|â–ˆâ–        | 20/143 [00:01<00:09, 13.62it/s]
 15%|â–ˆâ–Œ        | 22/143 [00:01<00:09, 13.06it/s]
 17%|â–ˆâ–‹        | 24/143 [00:01<00:09, 12.80it/s]
 18%|â–ˆâ–Š        | 26/143 [00:01<00:09, 12.53it/s]
 20%|â–ˆâ–‰        | 28/143 [00:01<00:09, 12.42it/s]
 21%|â–ˆâ–ˆ        | 30/143 [00:02<00:09, 12.27it/s]
 22%|â–ˆâ–ˆâ–       | 32/143 [00:02<00:09, 12.30it/s]
 24%|â–ˆâ–ˆâ–       | 34/143 [00:02<00:08, 12.94it/s]
 25%|â–ˆâ–ˆâ–Œ       | 36/143 [00:02<00:07, 13.53it/s]
 27%|â–ˆâ–ˆâ–‹       | 38/143 [00:02<00:07, 13.94it/s]
 28%|â–ˆâ–ˆâ–Š       | 40/143 [00:02<00:07, 14.27it/s]
 29%|â–ˆâ–ˆâ–‰       | 42/143 [00:02<00:07, 14.36it/s]
 31%|â–ˆâ–ˆâ–ˆ       | 44/143 [00:03<00:06, 14.58it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 46/143 [00:03<00:06, 14.68it/s]
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 48/143 [00:03<00:06, 15.19it/s]
 35%|â–ˆâ–ˆâ–ˆâ–      | 50/143 [00:03<00:05, 15.77it/s]
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 52/143 [00:03<00:05, 16.42it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 54/143 [00:03<00:05, 16.77it/s]
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 56/143 [00:03<00:05, 17.17it/s]
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 58/143 [00:03<00:04, 17.45it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 60/143 [00:03<00:04, 17.42it/s]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 62/143 [00:04<00:04, 17.53it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 64/143 [00:04<00:04, 17.08it/s]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 66/143 [00:04<00:04, 16.57it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 68/143 [00:04<00:04, 16.35it/s]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 70/143 [00:04<00:04, 16.08it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 72/143 [00:04<00:04, 16.00it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 74/143 [00:04<00:04, 15.83it/s]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 76/143 [00:05<00:04, 15.84it/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78/143 [00:05<00:04, 15.70it/s]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 80/143 [00:05<00:04, 15.74it/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 82/143 [00:05<00:03, 15.57it/s]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 84/143 [00:05<00:03, 15.62it/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 86/143 [00:05<00:03, 15.48it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 88/143 [00:05<00:03, 15.56it/s]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 90/143 [00:05<00:03, 15.49it/s]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 92/143 [00:06<00:03, 15.56it/s]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 94/143 [00:06<00:03, 15.23it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 96/143 [00:06<00:03, 14.95it/s]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 98/143 [00:06<00:03, 14.63it/s]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 100/143 [00:06<00:02, 14.52it/s]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 102/143 [00:06<00:02, 14.39it/s]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 104/143 [00:06<00:02, 14.39it/s]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 106/143 [00:07<00:02, 14.23it/s]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 108/143 [00:07<00:02, 14.27it/s]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 110/143 [00:07<00:02, 14.23it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 112/143 [00:07<00:02, 15.22it/s]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 114/143 [00:07<00:01, 16.03it/s]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 116/143 [00:07<00:01, 16.47it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 118/143 [00:07<00:01, 16.82it/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 120/143 [00:07<00:01, 17.21it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 122/143 [00:07<00:01, 17.44it/s]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 124/143 [00:08<00:01, 17.58it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 126/143 [00:08<00:00, 17.65it/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 128/143 [00:08<00:00, 17.85it/s]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 130/143 [00:08<00:00, 17.94it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 132/143 [00:08<00:00, 17.97it/s]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 134/143 [00:08<00:00, 17.97it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 136/143 [00:08<00:00, 17.98it/s]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 138/143 [00:08<00:00, 18.00it/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 140/143 [00:08<00:00, 17.97it/s]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 142/143 [00:09<00:00, 18.32it/s]
{'eval_loss': 0.35139015316963196, 'eval_precision': 0.8222243610712763, 'eval_recall': 0.8285145304677682, 'eval_f1': 0.8251445787559246, 'eval_balanced accuracy': 0.8285145304677682, 'eval_runtime': 9.2022, 'eval_samples_per_second': 994.549, 'eval_steps_per_second': 15.54, 'epoch': 2.0}
                                                   
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1144/3432 [04:24<08:01,  4.75it/s]
                                                 Saving model checkpoint to model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1144
Configuration saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1144\config.json
Model weights saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1144\model.safetensors
[codecarbon INFO @ 15:38:00] Energy consumed for RAM : 0.001326 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:38:00] Energy consumed for all GPUs : 0.012422 kWh. Total GPU Power : 172.28482783374068 W
[codecarbon INFO @ 15:38:00] Energy consumed for all CPUs : 0.003191 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:38:00] 0.016939 kWh of electricity used since the beginning.
tokenizer config file saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1144\tokenizer_config.json
Special tokens file saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1144\special_tokens_map.json
Deleting older checkpoint [model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-572] due to args.save_total_limit
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1210/3432 [04:39<07:57,  4.65it/s][codecarbon INFO @ 15:38:15] Energy consumed for RAM : 0.001400 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:38:15] Energy consumed for all GPUs : 0.013101 kWh. Total GPU Power : 162.8695569915321 W
[codecarbon INFO @ 15:38:15] Energy consumed for all CPUs : 0.003368 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:38:15] 0.017869 kWh of electricity used since the beginning.
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1280/3432 [04:54<07:42,  4.65it/s][codecarbon INFO @ 15:38:30] Energy consumed for RAM : 0.001474 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:38:30] Energy consumed for all GPUs : 0.013813 kWh. Total GPU Power : 170.71831005682407 W
[codecarbon INFO @ 15:38:30] Energy consumed for all CPUs : 0.003545 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:38:30] 0.018832 kWh of electricity used since the beginning.
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1350/3432 [05:09<07:25,  4.68it/s][codecarbon INFO @ 15:38:45] Energy consumed for RAM : 0.001547 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:38:45] Energy consumed for all GPUs : 0.014526 kWh. Total GPU Power : 170.95786384148116 W
[codecarbon INFO @ 15:38:45] Energy consumed for all CPUs : 0.003722 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:38:45] 0.019795 kWh of electricity used since the beginning.
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1421/3432 [05:24<06:57,  4.81it/s][codecarbon INFO @ 15:39:00] Energy consumed for RAM : 0.001621 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:39:00] Energy consumed for all GPUs : 0.015238 kWh. Total GPU Power : 170.7488513654522 W
[codecarbon INFO @ 15:39:00] Energy consumed for all CPUs : 0.003899 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:39:00] 0.020758 kWh of electricity used since the beginning.
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1489/3432 [05:39<07:04,  4.58it/s][codecarbon INFO @ 15:39:15] Energy consumed for RAM : 0.001695 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:39:15] Energy consumed for all GPUs : 0.015940 kWh. Total GPU Power : 168.24402403782608 W
[codecarbon INFO @ 15:39:15] Energy consumed for all CPUs : 0.004077 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:39:15] 0.021711 kWh of electricity used since the beginning.
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1500/3432 [05:41<06:53,  4.67it/s]{'loss': 0.2291, 'grad_norm': 7.074445724487305, 'learning_rate': 1.1258741258741259e-05, 'epoch': 2.62}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1558/3432 [05:54<06:51,  4.55it/s][codecarbon INFO @ 15:39:30] Energy consumed for RAM : 0.001768 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:39:30] Energy consumed for all GPUs : 0.016649 kWh. Total GPU Power : 170.226181176564 W
[codecarbon INFO @ 15:39:30] Energy consumed for all CPUs : 0.004254 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:39:30] 0.022671 kWh of electricity used since the beginning.
[codecarbon INFO @ 15:39:30] 0.015175 g.CO2eq/s mean an estimation of 478.5435305744561 kg.CO2eq/year
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1628/3432 [06:09<06:31,  4.61it/s][codecarbon INFO @ 15:39:45] Energy consumed for RAM : 0.001842 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:39:45] Energy consumed for all GPUs : 0.017359 kWh. Total GPU Power : 170.29941055044708 W
[codecarbon INFO @ 15:39:45] Energy consumed for all CPUs : 0.004431 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:39:45] 0.023632 kWh of electricity used since the beginning.
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1698/3432 [06:24<06:14,  4.63it/s][codecarbon INFO @ 15:40:00] Energy consumed for RAM : 0.001916 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:40:00] Energy consumed for all GPUs : 0.018073 kWh. Total GPU Power : 171.02995941654854 W
[codecarbon INFO @ 15:40:00] Energy consumed for all CPUs : 0.004608 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:40:00] 0.024597 kWh of electricity used since the beginning.
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1716/3432 [06:28<06:09,  4.65it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 9152
  Batch size = 64

  0%|          | 0/143 [00:00<?, ?it/s]
  2%|â–         | 3/143 [00:00<00:05, 24.12it/s]
  4%|â–         | 6/143 [00:00<00:07, 17.72it/s]
  6%|â–Œ         | 8/143 [00:00<00:07, 16.88it/s]
  7%|â–‹         | 10/143 [00:00<00:08, 16.51it/s]
  8%|â–Š         | 12/143 [00:00<00:07, 16.47it/s]
 10%|â–‰         | 14/143 [00:00<00:07, 16.15it/s]
 11%|â–ˆ         | 16/143 [00:00<00:08, 15.37it/s]
 13%|â–ˆâ–Ž        | 18/143 [00:01<00:08, 14.04it/s]
 14%|â–ˆâ–        | 20/143 [00:01<00:09, 13.42it/s]
 15%|â–ˆâ–Œ        | 22/143 [00:01<00:09, 12.89it/s]
 17%|â–ˆâ–‹        | 24/143 [00:01<00:09, 12.54it/s]
 18%|â–ˆâ–Š        | 26/143 [00:01<00:09, 12.22it/s]
 20%|â–ˆâ–‰        | 28/143 [00:01<00:09, 12.16it/s]
 21%|â–ˆâ–ˆ        | 30/143 [00:02<00:09, 12.03it/s]
 22%|â–ˆâ–ˆâ–       | 32/143 [00:02<00:09, 12.10it/s]
 24%|â–ˆâ–ˆâ–       | 34/143 [00:02<00:08, 12.78it/s]
 25%|â–ˆâ–ˆâ–Œ       | 36/143 [00:02<00:07, 13.41it/s]
 27%|â–ˆâ–ˆâ–‹       | 38/143 [00:02<00:07, 13.76it/s]
 28%|â–ˆâ–ˆâ–Š       | 40/143 [00:02<00:07, 14.12it/s]
 29%|â–ˆâ–ˆâ–‰       | 42/143 [00:02<00:07, 14.26it/s]
 31%|â–ˆâ–ˆâ–ˆ       | 44/143 [00:03<00:06, 14.30it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 46/143 [00:03<00:06, 14.34it/s]
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 48/143 [00:03<00:06, 14.88it/s]
 35%|â–ˆâ–ˆâ–ˆâ–      | 50/143 [00:03<00:05, 15.52it/s]
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 52/143 [00:03<00:05, 16.24it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 54/143 [00:03<00:05, 16.62it/s]
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 56/143 [00:03<00:05, 17.03it/s]
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 58/143 [00:03<00:04, 17.21it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 60/143 [00:04<00:04, 17.25it/s]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 62/143 [00:04<00:04, 17.18it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 64/143 [00:04<00:04, 16.71it/s]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 66/143 [00:04<00:04, 16.13it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 68/143 [00:04<00:04, 15.95it/s]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 70/143 [00:04<00:04, 15.73it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 72/143 [00:04<00:04, 15.56it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 74/143 [00:04<00:04, 15.44it/s]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 76/143 [00:05<00:04, 15.43it/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78/143 [00:05<00:04, 15.41it/s]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 80/143 [00:05<00:04, 15.54it/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 82/143 [00:05<00:03, 15.46it/s]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 84/143 [00:05<00:03, 15.53it/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 86/143 [00:05<00:03, 15.39it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 88/143 [00:05<00:03, 15.51it/s]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 90/143 [00:06<00:03, 15.41it/s]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 92/143 [00:06<00:03, 15.39it/s]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 94/143 [00:06<00:03, 15.09it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 96/143 [00:06<00:03, 14.81it/s]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 98/143 [00:06<00:03, 14.55it/s]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 100/143 [00:06<00:02, 14.48it/s]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 102/143 [00:06<00:02, 14.30it/s]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 104/143 [00:06<00:02, 14.30it/s]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 106/143 [00:07<00:02, 14.22it/s]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 108/143 [00:07<00:02, 14.21it/s]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 110/143 [00:07<00:02, 14.10it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 112/143 [00:07<00:02, 15.11it/s]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 114/143 [00:07<00:01, 15.60it/s]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 116/143 [00:07<00:01, 16.15it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 118/143 [00:07<00:01, 16.58it/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 120/143 [00:07<00:01, 17.02it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 122/143 [00:08<00:01, 17.22it/s]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 124/143 [00:08<00:01, 17.48it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 126/143 [00:08<00:00, 17.59it/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 128/143 [00:08<00:00, 17.73it/s]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 130/143 [00:08<00:00, 17.75it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 132/143 [00:08<00:00, 17.86it/s]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 134/143 [00:08<00:00, 17.98it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 136/143 [00:08<00:00, 17.89it/s]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 138/143 [00:08<00:00, 17.90it/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 140/143 [00:09<00:00, 17.82it/s]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 142/143 [00:09<00:00, 18.02it/s]
{'eval_loss': 0.3768796920776367, 'eval_precision': 0.822694766075416, 'eval_recall': 0.8359848849312912, 'eval_f1': 0.8282313095243514, 'eval_balanced accuracy': 0.8359848849312912, 'eval_runtime': 9.3372, 'eval_samples_per_second': 980.165, 'eval_steps_per_second': 15.315, 'epoch': 3.0}
                                                   
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1716/3432 [06:37<06:09,  4.65it/s]
                                                 Saving model checkpoint to model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1716
Configuration saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1716\config.json
Model weights saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1716\model.safetensors
tokenizer config file saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1716\tokenizer_config.json
Special tokens file saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1716\special_tokens_map.json
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1718/3432 [06:39<1:09:14,  2.42s/it][codecarbon INFO @ 15:40:15] Energy consumed for RAM : 0.001989 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:40:15] Energy consumed for all GPUs : 0.018756 kWh. Total GPU Power : 163.90051415176575 W
[codecarbon INFO @ 15:40:15] Energy consumed for all CPUs : 0.004786 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:40:15] 0.025531 kWh of electricity used since the beginning.
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1787/3432 [06:54<05:56,  4.62it/s][codecarbon INFO @ 15:40:30] Energy consumed for RAM : 0.002063 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:40:30] Energy consumed for all GPUs : 0.019456 kWh. Total GPU Power : 167.81239550086218 W
[codecarbon INFO @ 15:40:30] Energy consumed for all CPUs : 0.004963 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:40:30] 0.026481 kWh of electricity used since the beginning.
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1856/3432 [07:09<05:44,  4.58it/s][codecarbon INFO @ 15:40:45] Energy consumed for RAM : 0.002137 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:40:45] Energy consumed for all GPUs : 0.020162 kWh. Total GPU Power : 169.6087556828432 W
[codecarbon INFO @ 15:40:45] Energy consumed for all CPUs : 0.005140 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:40:45] 0.027439 kWh of electricity used since the beginning.
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1925/3432 [07:24<05:25,  4.63it/s][codecarbon INFO @ 15:41:00] Energy consumed for RAM : 0.002210 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:41:00] Energy consumed for all GPUs : 0.020869 kWh. Total GPU Power : 169.42498531643238 W
[codecarbon INFO @ 15:41:00] Energy consumed for all CPUs : 0.005317 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:41:00] 0.028396 kWh of electricity used since the beginning.
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1994/3432 [07:39<05:13,  4.58it/s][codecarbon INFO @ 15:41:15] Energy consumed for RAM : 0.002284 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:41:15] Energy consumed for all GPUs : 0.021577 kWh. Total GPU Power : 169.6979239547863 W
[codecarbon INFO @ 15:41:15] Energy consumed for all CPUs : 0.005494 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:41:15] 0.029355 kWh of electricity used since the beginning.
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2000/3432 [07:40<05:15,  4.54it/s]{'loss': 0.1644, 'grad_norm': 7.606659889221191, 'learning_rate': 8.344988344988347e-06, 'epoch': 3.5}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2063/3432 [07:54<04:57,  4.60it/s][codecarbon INFO @ 15:41:30] Energy consumed for RAM : 0.002358 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:41:30] Energy consumed for all GPUs : 0.022282 kWh. Total GPU Power : 169.2754138668 W
[codecarbon INFO @ 15:41:30] Energy consumed for all CPUs : 0.005671 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:41:30] 0.030311 kWh of electricity used since the beginning.
[codecarbon INFO @ 15:41:30] 0.015115 g.CO2eq/s mean an estimation of 476.680700044402 kg.CO2eq/year
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2132/3432 [08:09<04:40,  4.64it/s][codecarbon INFO @ 15:41:45] Energy consumed for RAM : 0.002431 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:41:45] Energy consumed for all GPUs : 0.022991 kWh. Total GPU Power : 169.90539600816402 W
[codecarbon INFO @ 15:41:45] Energy consumed for all CPUs : 0.005849 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:41:45] 0.031271 kWh of electricity used since the beginning.
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2202/3432 [08:24<04:16,  4.80it/s][codecarbon INFO @ 15:42:00] Energy consumed for RAM : 0.002505 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:42:00] Energy consumed for all GPUs : 0.023702 kWh. Total GPU Power : 170.7127554987254 W
[codecarbon INFO @ 15:42:00] Energy consumed for all CPUs : 0.006026 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:42:00] 0.032233 kWh of electricity used since the beginning.
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2272/3432 [08:39<04:07,  4.69it/s][codecarbon INFO @ 15:42:15] Energy consumed for RAM : 0.002579 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:42:15] Energy consumed for all GPUs : 0.024417 kWh. Total GPU Power : 171.54154535918255 W
[codecarbon INFO @ 15:42:15] Energy consumed for all CPUs : 0.006203 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:42:15] 0.033199 kWh of electricity used since the beginning.
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2288/3432 [08:43<04:06,  4.64it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 9152
  Batch size = 64

  0%|          | 0/143 [00:00<?, ?it/s]
  2%|â–         | 3/143 [00:00<00:05, 24.99it/s]
  4%|â–         | 6/143 [00:00<00:07, 18.41it/s]
  6%|â–Œ         | 8/143 [00:00<00:07, 17.67it/s]
  7%|â–‹         | 10/143 [00:00<00:07, 17.09it/s]
  8%|â–Š         | 12/143 [00:00<00:07, 16.88it/s]
 10%|â–‰         | 14/143 [00:00<00:07, 16.57it/s]
 11%|â–ˆ         | 16/143 [00:00<00:08, 15.61it/s]
 13%|â–ˆâ–Ž        | 18/143 [00:01<00:08, 14.29it/s]
 14%|â–ˆâ–        | 20/143 [00:01<00:09, 13.57it/s]
 15%|â–ˆâ–Œ        | 22/143 [00:01<00:09, 12.95it/s]
 17%|â–ˆâ–‹        | 24/143 [00:01<00:09, 12.63it/s]
 18%|â–ˆâ–Š        | 26/143 [00:01<00:09, 12.40it/s]
 20%|â–ˆâ–‰        | 28/143 [00:01<00:09, 12.32it/s]
 21%|â–ˆâ–ˆ        | 30/143 [00:02<00:09, 12.17it/s]
 22%|â–ˆâ–ˆâ–       | 32/143 [00:02<00:09, 12.24it/s]
 24%|â–ˆâ–ˆâ–       | 34/143 [00:02<00:08, 12.80it/s]
 25%|â–ˆâ–ˆâ–Œ       | 36/143 [00:02<00:07, 13.40it/s]
 27%|â–ˆâ–ˆâ–‹       | 38/143 [00:02<00:07, 13.75it/s]
 28%|â–ˆâ–ˆâ–Š       | 40/143 [00:02<00:07, 14.13it/s]
 29%|â–ˆâ–ˆâ–‰       | 42/143 [00:02<00:07, 14.29it/s]
 31%|â–ˆâ–ˆâ–ˆ       | 44/143 [00:03<00:06, 14.48it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 46/143 [00:03<00:06, 14.52it/s]
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 48/143 [00:03<00:06, 15.06it/s]
 35%|â–ˆâ–ˆâ–ˆâ–      | 50/143 [00:03<00:05, 15.70it/s]
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 52/143 [00:03<00:05, 16.32it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 54/143 [00:03<00:05, 16.75it/s]
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 56/143 [00:03<00:05, 17.09it/s]
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 58/143 [00:03<00:04, 17.21it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 60/143 [00:04<00:04, 17.39it/s]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 62/143 [00:04<00:04, 17.41it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 64/143 [00:04<00:04, 16.97it/s]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 66/143 [00:04<00:04, 16.48it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 68/143 [00:04<00:04, 16.23it/s]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 70/143 [00:04<00:04, 15.99it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 72/143 [00:04<00:04, 15.95it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 74/143 [00:04<00:04, 15.78it/s]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 76/143 [00:05<00:04, 15.76it/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78/143 [00:05<00:04, 15.65it/s]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 80/143 [00:05<00:04, 15.66it/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 82/143 [00:05<00:03, 15.54it/s]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 84/143 [00:05<00:03, 15.56it/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 86/143 [00:05<00:03, 15.44it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 88/143 [00:05<00:03, 15.52it/s]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 90/143 [00:05<00:03, 15.44it/s]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 92/143 [00:06<00:03, 15.50it/s]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 94/143 [00:06<00:03, 15.17it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 96/143 [00:06<00:03, 14.93it/s]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 98/143 [00:06<00:03, 14.63it/s]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 100/143 [00:06<00:02, 14.48it/s]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 102/143 [00:06<00:02, 14.29it/s]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 104/143 [00:06<00:02, 14.31it/s]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 106/143 [00:07<00:02, 14.17it/s]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 108/143 [00:07<00:02, 14.24it/s]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 110/143 [00:07<00:02, 14.17it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 112/143 [00:07<00:02, 15.19it/s]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 114/143 [00:07<00:01, 15.88it/s]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 116/143 [00:07<00:01, 16.49it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 118/143 [00:07<00:01, 16.92it/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 120/143 [00:07<00:01, 17.23it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 122/143 [00:08<00:01, 17.49it/s]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 124/143 [00:08<00:01, 17.68it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 126/143 [00:08<00:00, 17.86it/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 128/143 [00:08<00:00, 17.90it/s]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 130/143 [00:08<00:00, 17.95it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 132/143 [00:08<00:00, 18.02it/s]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 134/143 [00:08<00:00, 17.95it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 136/143 [00:08<00:00, 17.99it/s]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 138/143 [00:08<00:00, 17.92it/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 140/143 [00:09<00:00, 17.93it/s]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 142/143 [00:09<00:00, 18.23it/s]
                                                   
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2288/3432 [08:52<04:06,  4.64it/s]
                                                 Saving model checkpoint to model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2288
Configuration saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2288\config.json
{'eval_loss': 0.4487888813018799, 'eval_precision': 0.8258832017980287, 'eval_recall': 0.8382285279427586, 'eval_f1': 0.8311295178217987, 'eval_balanced accuracy': 0.8382285279427586, 'eval_runtime': 9.2437, 'eval_samples_per_second': 990.078, 'eval_steps_per_second': 15.47, 'epoch': 4.0}
Model weights saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2288\model.safetensors
tokenizer config file saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2288\tokenizer_config.json
Special tokens file saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2288\special_tokens_map.json
Deleting older checkpoint [model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1716] due to args.save_total_limit
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2293/3432 [08:54<18:22,  1.03it/s][codecarbon INFO @ 15:42:30] Energy consumed for RAM : 0.002652 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:42:30] Energy consumed for all GPUs : 0.025109 kWh. Total GPU Power : 165.8631251461071 W
[codecarbon INFO @ 15:42:30] Energy consumed for all CPUs : 0.006380 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:42:30] 0.034141 kWh of electricity used since the beginning.
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2363/3432 [09:09<03:49,  4.65it/s][codecarbon INFO @ 15:42:45] Energy consumed for RAM : 0.002726 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:42:45] Energy consumed for all GPUs : 0.025821 kWh. Total GPU Power : 170.9814000460929 W
[codecarbon INFO @ 15:42:45] Energy consumed for all CPUs : 0.006557 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:42:45] 0.035104 kWh of electricity used since the beginning.
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2433/3432 [09:24<03:34,  4.66it/s][codecarbon INFO @ 15:43:00] Energy consumed for RAM : 0.002799 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:43:00] Energy consumed for all GPUs : 0.026537 kWh. Total GPU Power : 171.5906051468252 W
[codecarbon INFO @ 15:43:00] Energy consumed for all CPUs : 0.006735 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:43:00] 0.036071 kWh of electricity used since the beginning.
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2500/3432 [09:38<03:22,  4.61it/s]{'loss': 0.1216, 'grad_norm': 6.944225788116455, 'learning_rate': 5.431235431235432e-06, 'epoch': 4.37}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2503/3432 [09:39<03:21,  4.61it/s][codecarbon INFO @ 15:43:15] Energy consumed for RAM : 0.002873 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:43:15] Energy consumed for all GPUs : 0.027253 kWh. Total GPU Power : 171.8190755569994 W
[codecarbon INFO @ 15:43:15] Energy consumed for all CPUs : 0.006912 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:43:15] 0.037038 kWh of electricity used since the beginning.
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2573/3432 [09:54<03:04,  4.66it/s][codecarbon INFO @ 15:43:30] Energy consumed for RAM : 0.002947 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:43:30] Energy consumed for all GPUs : 0.027969 kWh. Total GPU Power : 171.678648588049 W
[codecarbon INFO @ 15:43:30] Energy consumed for all CPUs : 0.007089 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:43:30] 0.038004 kWh of electricity used since the beginning.
[codecarbon INFO @ 15:43:30] 0.015223 g.CO2eq/s mean an estimation of 480.08001438878136 kg.CO2eq/year
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2643/3432 [10:09<02:47,  4.71it/s][codecarbon INFO @ 15:43:45] Energy consumed for RAM : 0.003020 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:43:45] Energy consumed for all GPUs : 0.028684 kWh. Total GPU Power : 171.69621325742048 W
[codecarbon INFO @ 15:43:45] Energy consumed for all CPUs : 0.007266 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:43:45] 0.038970 kWh of electricity used since the beginning.
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2713/3432 [10:24<02:34,  4.66it/s][codecarbon INFO @ 15:44:00] Energy consumed for RAM : 0.003094 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:44:00] Energy consumed for all GPUs : 0.029399 kWh. Total GPU Power : 171.59991792951843 W
[codecarbon INFO @ 15:44:00] Energy consumed for all CPUs : 0.007443 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:44:00] 0.039937 kWh of electricity used since the beginning.
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2783/3432 [10:39<02:19,  4.65it/s][codecarbon INFO @ 15:44:15] Energy consumed for RAM : 0.003168 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:44:15] Energy consumed for all GPUs : 0.030117 kWh. Total GPU Power : 172.113926377083 W
[codecarbon INFO @ 15:44:15] Energy consumed for all CPUs : 0.007620 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:44:15] 0.040905 kWh of electricity used since the beginning.
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2853/3432 [10:54<02:04,  4.66it/s][codecarbon INFO @ 15:44:30] Energy consumed for RAM : 0.003241 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:44:30] Energy consumed for all GPUs : 0.030833 kWh. Total GPU Power : 171.75812708681966 W
[codecarbon INFO @ 15:44:30] Energy consumed for all CPUs : 0.007798 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:44:30] 0.041872 kWh of electricity used since the beginning.
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2860/3432 [10:56<02:01,  4.72it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 9152
  Batch size = 64

  0%|          | 0/143 [00:00<?, ?it/s]
  2%|â–         | 3/143 [00:00<00:05, 25.11it/s]
  4%|â–         | 6/143 [00:00<00:07, 18.47it/s]
  6%|â–Œ         | 8/143 [00:00<00:07, 17.80it/s]
  7%|â–‹         | 10/143 [00:00<00:07, 17.14it/s]
  8%|â–Š         | 12/143 [00:00<00:07, 16.95it/s]
 10%|â–‰         | 14/143 [00:00<00:07, 16.60it/s]
 11%|â–ˆ         | 16/143 [00:00<00:08, 15.64it/s]
 13%|â–ˆâ–Ž        | 18/143 [00:01<00:08, 14.26it/s]
 14%|â–ˆâ–        | 20/143 [00:01<00:09, 13.56it/s]
 15%|â–ˆâ–Œ        | 22/143 [00:01<00:09, 13.05it/s]
 17%|â–ˆâ–‹        | 24/143 [00:01<00:09, 12.77it/s]
 18%|â–ˆâ–Š        | 26/143 [00:01<00:09, 12.49it/s]
 20%|â–ˆâ–‰        | 28/143 [00:01<00:09, 12.42it/s]
 21%|â–ˆâ–ˆ        | 30/143 [00:02<00:09, 12.22it/s]
 22%|â–ˆâ–ˆâ–       | 32/143 [00:02<00:09, 12.27it/s]
 24%|â–ˆâ–ˆâ–       | 34/143 [00:02<00:08, 12.93it/s]
 25%|â–ˆâ–ˆâ–Œ       | 36/143 [00:02<00:07, 13.50it/s]
 27%|â–ˆâ–ˆâ–‹       | 38/143 [00:02<00:07, 13.84it/s]
 28%|â–ˆâ–ˆâ–Š       | 40/143 [00:02<00:07, 14.20it/s]
 29%|â–ˆâ–ˆâ–‰       | 42/143 [00:02<00:07, 14.30it/s]
 31%|â–ˆâ–ˆâ–ˆ       | 44/143 [00:03<00:06, 14.51it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 46/143 [00:03<00:06, 14.47it/s]
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 48/143 [00:03<00:06, 15.00it/s]
 35%|â–ˆâ–ˆâ–ˆâ–      | 50/143 [00:03<00:05, 15.78it/s]
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 52/143 [00:03<00:05, 16.34it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 54/143 [00:03<00:05, 16.75it/s]
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 56/143 [00:03<00:05, 17.15it/s]
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 58/143 [00:03<00:04, 17.27it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 60/143 [00:04<00:04, 17.51it/s]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 62/143 [00:04<00:04, 17.52it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 64/143 [00:04<00:04, 16.95it/s]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 66/143 [00:04<00:04, 16.50it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 68/143 [00:04<00:04, 16.32it/s]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 70/143 [00:04<00:04, 15.94it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 72/143 [00:04<00:04, 15.95it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 74/143 [00:04<00:04, 15.77it/s]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 76/143 [00:05<00:04, 15.78it/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78/143 [00:05<00:04, 15.62it/s]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 80/143 [00:05<00:04, 15.67it/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 82/143 [00:05<00:03, 15.57it/s]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 84/143 [00:05<00:03, 15.63it/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 86/143 [00:05<00:03, 15.43it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 88/143 [00:05<00:03, 15.50it/s]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 90/143 [00:05<00:03, 15.37it/s]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 92/143 [00:06<00:03, 15.47it/s]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 94/143 [00:06<00:03, 15.15it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 96/143 [00:06<00:03, 14.90it/s]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 98/143 [00:06<00:03, 14.59it/s]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 100/143 [00:06<00:02, 14.48it/s]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 102/143 [00:06<00:02, 14.30it/s]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 104/143 [00:06<00:02, 14.32it/s]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 106/143 [00:07<00:02, 14.17it/s]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 108/143 [00:07<00:02, 14.21it/s]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 110/143 [00:07<00:02, 14.14it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 112/143 [00:07<00:02, 15.17it/s]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 114/143 [00:07<00:01, 16.02it/s]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 116/143 [00:07<00:01, 16.45it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 118/143 [00:07<00:01, 16.82it/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 120/143 [00:07<00:01, 17.21it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 122/143 [00:07<00:01, 17.43it/s]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 124/143 [00:08<00:01, 17.66it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 126/143 [00:08<00:00, 17.67it/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 128/143 [00:08<00:00, 17.82it/s]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 130/143 [00:08<00:00, 17.88it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 132/143 [00:08<00:00, 17.98it/s]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 134/143 [00:08<00:00, 17.97it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 136/143 [00:08<00:00, 17.95it/s]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 138/143 [00:08<00:00, 17.98it/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 140/143 [00:08<00:00, 18.03it/s]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 142/143 [00:09<00:00, 18.28it/s]
{'eval_loss': 0.5214909315109253, 'eval_precision': 0.8298026230960183, 'eval_recall': 0.8357358828348918, 'eval_f1': 0.8325773549939044, 'eval_balanced accuracy': 0.8357358828348918, 'eval_runtime': 9.2292, 'eval_samples_per_second': 991.632, 'eval_steps_per_second': 15.494, 'epoch': 5.0}
                                                   
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2860/3432 [11:05<02:01,  4.72it/s]
                                                 Saving model checkpoint to model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2860
Configuration saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2860\config.json
Model weights saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2860\model.safetensors
tokenizer config file saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2860\tokenizer_config.json
Special tokens file saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2860\special_tokens_map.json
Deleting older checkpoint [model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2288] due to args.save_total_limit
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2874/3432 [11:09<02:16,  4.08it/s][codecarbon INFO @ 15:44:45] Energy consumed for RAM : 0.003315 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:44:45] Energy consumed for all GPUs : 0.031520 kWh. Total GPU Power : 164.95629321536413 W
[codecarbon INFO @ 15:44:45] Energy consumed for all CPUs : 0.007975 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:44:45] 0.042810 kWh of electricity used since the beginning.
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2943/3432 [11:24<01:45,  4.65it/s][codecarbon INFO @ 15:45:00] Energy consumed for RAM : 0.003389 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:45:00] Energy consumed for all GPUs : 0.032233 kWh. Total GPU Power : 171.13053599467565 W
[codecarbon INFO @ 15:45:00] Energy consumed for all CPUs : 0.008152 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:45:00] 0.043774 kWh of electricity used since the beginning.
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3000/3432 [11:36<01:33,  4.63it/s]{'loss': 0.09, 'grad_norm': 6.259932518005371, 'learning_rate': 2.517482517482518e-06, 'epoch': 5.24}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3013/3432 [11:39<01:29,  4.69it/s][codecarbon INFO @ 15:45:15] Energy consumed for RAM : 0.003462 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:45:15] Energy consumed for all GPUs : 0.032950 kWh. Total GPU Power : 171.82506257531682 W
[codecarbon INFO @ 15:45:15] Energy consumed for all CPUs : 0.008329 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:45:15] 0.044741 kWh of electricity used since the beginning.
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3084/3432 [11:54<01:14,  4.67it/s][codecarbon INFO @ 15:45:30] Energy consumed for RAM : 0.003536 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:45:30] Energy consumed for all GPUs : 0.033672 kWh. Total GPU Power : 173.24494663357916 W
[codecarbon INFO @ 15:45:30] Energy consumed for all CPUs : 0.008506 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:45:30] 0.045714 kWh of electricity used since the beginning.
[codecarbon INFO @ 15:45:30] 0.015258 g.CO2eq/s mean an estimation of 481.17808878966713 kg.CO2eq/year
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3153/3432 [12:09<00:59,  4.65it/s][codecarbon INFO @ 15:45:45] Energy consumed for RAM : 0.003610 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:45:45] Energy consumed for all GPUs : 0.034390 kWh. Total GPU Power : 172.09701702666544 W
[codecarbon INFO @ 15:45:45] Energy consumed for all CPUs : 0.008683 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:45:45] 0.046683 kWh of electricity used since the beginning.
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3223/3432 [12:24<00:44,  4.68it/s][codecarbon INFO @ 15:46:00] Energy consumed for RAM : 0.003683 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:46:00] Energy consumed for all GPUs : 0.035108 kWh. Total GPU Power : 172.2043935348113 W
[codecarbon INFO @ 15:46:00] Energy consumed for all CPUs : 0.008861 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:46:00] 0.047652 kWh of electricity used since the beginning.
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3293/3432 [12:39<00:29,  4.65it/s][codecarbon INFO @ 15:46:15] Energy consumed for RAM : 0.003757 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:46:15] Energy consumed for all GPUs : 0.035826 kWh. Total GPU Power : 172.35970015399928 W
[codecarbon INFO @ 15:46:15] Energy consumed for all CPUs : 0.009038 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:46:15] 0.048621 kWh of electricity used since the beginning.
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3363/3432 [12:54<00:14,  4.69it/s][codecarbon INFO @ 15:46:30] Energy consumed for RAM : 0.003831 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:46:30] Energy consumed for all GPUs : 0.036544 kWh. Total GPU Power : 172.15373763440058 W
[codecarbon INFO @ 15:46:30] Energy consumed for all CPUs : 0.009215 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:46:30] 0.049590 kWh of electricity used since the beginning.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3432/3432 [13:09<00:00,  4.66it/s]Saving model checkpoint to model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432
Configuration saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432\config.json
Model weights saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432\model.safetensors
tokenizer config file saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432\tokenizer_config.json
Special tokens file saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432\special_tokens_map.json
[codecarbon INFO @ 15:46:45] Energy consumed for RAM : 0.003904 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:46:45] Energy consumed for all GPUs : 0.037251 kWh. Total GPU Power : 169.50820708154959 W
[codecarbon INFO @ 15:46:45] Energy consumed for all CPUs : 0.009392 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:46:45] 0.050548 kWh of electricity used since the beginning.
Deleting older checkpoint [model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-2860] due to args.save_total_limit
The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 9152
  Batch size = 64

  0%|          | 0/143 [00:00<?, ?it/s]
  2%|â–         | 3/143 [00:00<00:06, 22.48it/s]
  4%|â–         | 6/143 [00:00<00:07, 18.69it/s]
  6%|â–Œ         | 8/143 [00:00<00:07, 17.50it/s]
  7%|â–‹         | 10/143 [00:00<00:07, 17.19it/s]
  8%|â–Š         | 12/143 [00:00<00:07, 16.90it/s]
 10%|â–‰         | 14/143 [00:00<00:07, 16.75it/s]
 11%|â–ˆ         | 16/143 [00:00<00:08, 15.50it/s]
 13%|â–ˆâ–Ž        | 18/143 [00:01<00:08, 14.28it/s]
 14%|â–ˆâ–        | 20/143 [00:01<00:09, 13.48it/s]
 15%|â–ˆâ–Œ        | 22/143 [00:01<00:09, 13.06it/s]
 17%|â–ˆâ–‹        | 24/143 [00:01<00:09, 12.66it/s]
 18%|â–ˆâ–Š        | 26/143 [00:01<00:09, 12.52it/s]
 20%|â–ˆâ–‰        | 28/143 [00:01<00:09, 12.35it/s]
 21%|â–ˆâ–ˆ        | 30/143 [00:02<00:09, 12.33it/s]
 22%|â–ˆâ–ˆâ–       | 32/143 [00:02<00:09, 12.24it/s]
 24%|â–ˆâ–ˆâ–       | 34/143 [00:02<00:08, 12.98it/s]
 25%|â–ˆâ–ˆâ–Œ       | 36/143 [00:02<00:07, 13.46it/s]
 27%|â–ˆâ–ˆâ–‹       | 38/143 [00:02<00:07, 13.91it/s]
 28%|â–ˆâ–ˆâ–Š       | 40/143 [00:02<00:07, 13.79it/s]
 29%|â–ˆâ–ˆâ–‰       | 42/143 [00:02<00:07, 13.79it/s]
 31%|â–ˆâ–ˆâ–ˆ       | 44/143 [00:03<00:07, 13.78it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 46/143 [00:03<00:07, 13.67it/s]
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 48/143 [00:03<00:09,  9.86it/s]
 35%|â–ˆâ–ˆâ–ˆâ–      | 50/143 [00:03<00:08, 11.28it/s]
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 52/143 [00:03<00:07, 12.57it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 54/143 [00:03<00:06, 13.80it/s]
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 56/143 [00:04<00:05, 14.61it/s]
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 58/143 [00:04<00:05, 15.36it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 60/143 [00:04<00:05, 16.02it/s]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 62/143 [00:04<00:04, 16.55it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 64/143 [00:04<00:04, 16.20it/s]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 66/143 [00:04<00:04, 16.10it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 68/143 [00:04<00:04, 15.71it/s]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 70/143 [00:04<00:04, 15.65it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 72/143 [00:05<00:04, 15.56it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 74/143 [00:05<00:04, 15.66it/s]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 76/143 [00:05<00:04, 15.59it/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78/143 [00:05<00:04, 15.68it/s]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 80/143 [00:05<00:04, 15.49it/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 82/143 [00:05<00:03, 15.58it/s]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 84/143 [00:05<00:03, 15.39it/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 86/143 [00:05<00:03, 15.47it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 88/143 [00:06<00:03, 15.34it/s]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 90/143 [00:06<00:03, 15.37it/s]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 92/143 [00:06<00:03, 15.27it/s]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 94/143 [00:06<00:03, 15.16it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 96/143 [00:06<00:03, 14.75it/s]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 98/143 [00:06<00:03, 14.56it/s]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 100/143 [00:06<00:03, 14.32it/s]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 102/143 [00:07<00:02, 14.31it/s]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 104/143 [00:07<00:02, 14.18it/s]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 106/143 [00:07<00:02, 14.22it/s]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 108/143 [00:07<00:02, 14.15it/s]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 110/143 [00:07<00:02, 14.19it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 112/143 [00:07<00:02, 15.05it/s]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 114/143 [00:07<00:01, 15.90it/s]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 116/143 [00:07<00:01, 16.52it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 118/143 [00:08<00:01, 16.91it/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 120/143 [00:08<00:01, 17.12it/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 122/143 [00:08<00:01, 17.44it/s]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 124/143 [00:08<00:01, 17.57it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 126/143 [00:08<00:00, 17.74it/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 128/143 [00:08<00:00, 17.83it/s]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 130/143 [00:08<00:00, 17.91it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 132/143 [00:08<00:00, 17.89it/s]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 134/143 [00:08<00:00, 17.93it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 136/143 [00:09<00:00, 18.03it/s]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 138/143 [00:09<00:00, 18.08it/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 140/143 [00:09<00:00, 18.06it/s]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 142/143 [00:09<00:00, 18.27it/s]
{'eval_loss': 0.5629975199699402, 'eval_precision': 0.82975457283746, 'eval_recall': 0.835270826226277, 'eval_f1': 0.8323476476425964, 'eval_balanced accuracy': 0.835270826226277, 'eval_runtime': 9.5103, 'eval_samples_per_second': 962.327, 'eval_steps_per_second': 15.036, 'epoch': 6.0}
                                                   
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3432/3432 [13:20<00:00,  4.66it/s]
                                                 Saving model checkpoint to model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432
Configuration saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432\config.json
Model weights saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432\model.safetensors
tokenizer config file saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432\tokenizer_config.json
Special tokens file saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432\special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-1144 (score: 0.35139015316963196).
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3432/3432 [13:21<00:00,  4.66it/s]Deleting older checkpoint [model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\checkpoint-3432] due to args.save_total_limit
{'train_runtime': 801.6088, 'train_samples_per_second': 274.009, 'train_steps_per_second': 4.281, 'train_loss': 0.20914477568406326, 'epoch': 6.0}
[codecarbon WARNING @ 15:46:57] Another instance of codecarbon is already running. Exiting.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3432/3432 [13:21<00:00,  4.28it/s]
Saving model checkpoint to model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained
Configuration saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\config.json
Model weights saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\model.safetensors
tokenizer config file saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\tokenizer_config.json
Special tokens file saved in model_output_bert\merged_winoqueer_seegull_gpt_augmentation_trained\special_tokens_map.json
[codecarbon INFO @ 15:46:57] Energy consumed for RAM : 0.003965 kWh. RAM Power : 17.66854476928711 W
[codecarbon INFO @ 15:46:57] Energy consumed for all GPUs : 0.037740 kWh. Total GPU Power : 143.34556416879 W
[codecarbon INFO @ 15:46:57] Energy consumed for all CPUs : 0.009537 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 15:46:57] 0.051242 kWh of electricity used since the beginning.
loading configuration file model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained\config.json
Model config BertConfig {
  "_name_or_path": "model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained\model.safetensors
Estimated total emissions: 0.012174558010075722 kg CO2
Number of unique labels: 2
All model checkpoint weights were used when initializing BertForSequenceClassification.

All the weights of BertForSequenceClassification were initialized from the model checkpoint at model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 653/653 [00:00<00:00, 29978.99 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 653/653 [00:00<00:00, 23655.90 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3236, 'input_ids': [101, 2087, 2111, 2040, 2024, 5637, 2024, 20497, 2015, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
loading configuration file model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained\config.json
Model config BertConfig {
  "_name_or_path": "model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained\model.safetensors
Number of unique labels: 2
All model checkpoint weights were used when initializing BertForSequenceClassification.

All the weights of BertForSequenceClassification were initialized from the model checkpoint at model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 414/414 [00:00<00:00, 28434.56 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 414/414 [00:00<00:00, 20609.61 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 840, 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'labels': 0}
Number of unique labels: 2
loading configuration file model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained\config.json
Model config BertConfig {
  "_name_or_path": "model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained\model.safetensors
All model checkpoint weights were used when initializing BertForSequenceClassification.

All the weights of BertForSequenceClassification were initialized from the model checkpoint at model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10374/10374 [00:00<00:00, 21754.09 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10374/10374 [00:00<00:00, 18376.66 examples/s]
Sample tokenized input from test: {'group': 'race', 'text': 'When i see an arab, i think they are a happy.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 17851, 'input_ids': [101, 2043, 1045, 2156, 2019, 5424, 1010, 1045, 2228, 2027, 2024, 1037, 3407, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
loading configuration file model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained\config.json
Model config BertConfig {
  "_name_or_path": "model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained\model.safetensors
Number of unique labels: 2
All model checkpoint weights were used when initializing BertForSequenceClassification.

All the weights of BertForSequenceClassification were initialized from the model checkpoint at model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.
loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11441/11441 [00:00<00:00, 18914.13 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11441/11441 [00:00<00:00, 27475.11 examples/s]
E:\Anaconda\envs\cw2\Lib\site-packages\transformers\pipelines\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}

Process finished with exit code 0
